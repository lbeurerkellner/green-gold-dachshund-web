import{_ as e,o as a,c as t,Q as o}from"./chunks/framework.c2adf1ba.js";const g=JSON.parse('{"title":"Serving","description":"","frontmatter":{"order":1},"headers":[],"relativePath":"docs/latest/lib/chat/serving.md","filePath":"docs/latest/lib/chat/serving.md"}'),n={name:"docs/latest/lib/chat/serving.md"},s=o(`<h1 id="serving" tabindex="-1">Serving <a class="header-anchor" href="#serving" aria-label="Permalink to &quot;Serving&quot;">​</a></h1><p>After building your chat application, serving it is the next step before it can be used by others. To facilitate this and to offer a starting point for your custom implementation, LMQL includes a simple server implementation and corresponding client web UI, that communicate via the <a href="https://tools.ietf.org/html/rfc6455" target="_blank" rel="noreferrer"><code>websocket</code> protocol</a>.</p><p>To access and use this implementation we offer two routes: (1) Use the <code>lmql chat</code> command directly with your <code>.lmql</code> file, or (2) use the <code>lmql.chat.chatserver</code> from your own code.</p><h2 id="using-the-lmql-chat-command" tabindex="-1">Using the <code>lmql chat</code> command <a class="header-anchor" href="#using-the-lmql-chat-command" aria-label="Permalink to &quot;Using the \`lmql chat\` command&quot;">​</a></h2><p>To locally serve an LMQL chat endpoint and user interface, simply run the following command:</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="hljs"><code><span class="line">lmql chat &lt;path-to-lmql-file&gt;.lmql
</span></code></pre></div><p>This will serve a web interface on <code>http://localhost:8089</code>, and automatically open it in your default browser. You can now start chatting with your custom LMQL chat application. The internal trace on the right-hand side always displays the complete current prompt, reflecting the current state of your chat application.</p><p>Note that changing the <code>.lmql</code> file will <strong>not</strong> automatically reload the server, so you will have to restart the server manually to see the changes.</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="hljs"><code><span class="line">:hidden:

./chat/overview
</span></code></pre></div><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="hljs"><code><span class="line">:name: lmql-chat
:alt: A simple chatbot using the LMQL chat UI
:align: center

A simple chatbot using the LMQL Chat UI.
</span></code></pre></div><h2 id="using-chatserver" tabindex="-1">Using <code>chatserver</code> <a class="header-anchor" href="#using-chatserver" aria-label="Permalink to &quot;Using \`chatserver\`&quot;">​</a></h2><p>Alternatively, to launch the LMQL chat server from your own code, you can use <code>lmql.lib.chat.chatserver</code>. This c;ass takes the path of the query file or a query function as argument, and returns a chat server object, that can be launched using <code>run()</code>:</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="hljs"><code><span class="line"><span class="hljs-keyword">from</span> lmql.lib.chat <span class="hljs-keyword">import</span> chatserver

chatserver(<span class="hljs-string">&#39;path/to/my-query.lmql&#39;</span>).run()
</span></code></pre></div><p>Note that when passing a query function directly, you have to always provide a <code>async def</code> function, which enables concurrent client serving.</p><h2 id="message-streaming" tabindex="-1"><code>@message</code> Streaming <a class="header-anchor" href="#message-streaming" aria-label="Permalink to &quot;\`@message\` Streaming&quot;">​</a></h2><p>Chat relies on a <a href="./../../language/decorators.html">decorator-based</a> output streaming. More specifically, only model output variables that are annotated as <code>@message</code> are streamed and shown to the user in the chat interface. This allows for a clean separation of model output and chat output, and eneables hidden/internal reasoning.</p><p>To use <code>@message</code> with your <a href="../output.ipynb">custom output writer</a>, make sure to inherit from <code>lmql.lib.chat</code>&#39;s <code>ChatMessageOutputWriter</code>, which offers additional methods for specifically handling and streaming <code>@message</code> variables.</p><h2 id="more-advanced-usage" tabindex="-1">More Advanced Usage <a class="header-anchor" href="#more-advanced-usage" aria-label="Permalink to &quot;More Advanced Usage&quot;">​</a></h2><p>For more advanced serving scenarios, e.g. when integrating Chat into your own web applications, please refer to the very minimal implementation of <code>chatserver</code> in <a href="https://github.com/eth-sri/lmql/blob/main/src/lmql/ui/chat/__init__.py" target="_blank" rel="noreferrer"><code>src/lmql/ui/chat/__init__.py</code></a>. This implementation is very minimal and can be easily adapted to your own needs and infrastructure. The corresponding web UI is implemented in <a href="https://github.com/eth-sri/lmql/blob/main/src/lmql/ui/chat/assets/" target="_blank" rel="noreferrer"><code>src/lmql/ui/chat/assets/</code></a> and offers a good starting point for your own implementation and UI adaptations on the client side.</p><p>For other forms of output streaming e.g. via HTTP or SSE, see also the chapter on <a href="../output.ipynb">Output Streaming</a></p><p><strong>Disclaimer</strong>: The LMQL chat server is a simple code template that does not include any security features, authentication or cost control. It is intended for local development and testing only, and should not be used as-is in production environments. Before deploying your own chat application, make sure to implement the necessary security measures, cost control and authentication mechanisms.</p>`,21),r=[s];function i(c,l,h,d,p,m){return a(),t("div",null,r)}const f=e(n,[["render",i]]);export{g as __pageData,f as default};
