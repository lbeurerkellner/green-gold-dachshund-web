import{_ as e,o as a,c as t,Q as o}from"./chunks/framework.c2adf1ba.js";const g=JSON.parse('{"title":"LMQL in Docker","description":"","frontmatter":{},"headers":[],"relativePath":"docs/latest/development/docker-setup.md","filePath":"docs/latest/development/docker-setup.md"}'),n={name:"docs/latest/development/docker-setup.md"},s=o(`<h1 id="lmql-in-docker" tabindex="-1">LMQL in Docker <a class="header-anchor" href="#lmql-in-docker" aria-label="Permalink to &quot;LMQL in Docker&quot;">​</a></h1><div class="subtitle">Use LMQL in a Docker container.</div><p>Hereafter are the instructions to setup a <code>docker</code> container running the latest stable version of LMQL. The <code>Dockerfile</code> for the image building can be found in the <code>scripts/</code> folder.</p><div class="tip custom-block"><p class="custom-block-title">TIP</p><p><strong>Important note:</strong> the current version of LMQL requires you to map the <code>3000</code> and <code>3004</code> ports of the docker container to your host machine in order to access the playground IDE and the LMQL backend.</p></div><h2 id="building-the-image" tabindex="-1">Building the image <a class="header-anchor" href="#building-the-image" aria-label="Permalink to &quot;Building the image&quot;">​</a></h2><p>The following command lets you create an image with this Dockerfile:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="hljs"><code><span class="line">docker build -t lmql-docker:latest .
</span></code></pre></div><h3 id="using-gpu-and-local-models" tabindex="-1">Using GPU and local models <a class="header-anchor" href="#using-gpu-and-local-models" aria-label="Permalink to &quot;Using GPU and local models&quot;">​</a></h3><p>In order to use the local models, you first need to ensure that you have CUDA installed on your host machine and a supported Nvidia GPU. Be aware that this image has been tested with <code>CUDA 12.1</code> on the host machine and will install PyTorch stable with <code>CUDA 11.8</code> support. Before that, make sure that you have install docker gpu support. Then, finally, build the image using the following line:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="hljs"><code><span class="line">docker build --build-arg GPU_ENABLED=true -t lmql-docker:cuda11.8 .
</span></code></pre></div><p>Note that the <code>cuda11.8</code> tag is indicative and can be changed as you like.</p><h2 id="starting-a-container" tabindex="-1">Starting a container <a class="header-anchor" href="#starting-a-container" aria-label="Permalink to &quot;Starting a container&quot;">​</a></h2><p>To start a container using the image that you have built:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="hljs"><code><span class="line">docker run -d -p 3000:3000 -p 3004:3004 lmql-docker:latest
</span></code></pre></div><h3 id="using-environment-variables" tabindex="-1">Using environment variables <a class="header-anchor" href="#using-environment-variables" aria-label="Permalink to &quot;Using environment variables&quot;">​</a></h3><p>To override the default environment variables, you can do it through the docker run command like this example:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="hljs"><code><span class="line">docker run -d -e OPENAI_API_KEY=&lt;your openai api key&gt; -p 3000:3000 -p 3004:3004 lmql-docker:latest
</span></code></pre></div><p>Otherwise, if you want to use the <code>api.env</code> file you can also mount the file as follow:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="hljs"><code><span class="line">docker run -d -v $(PWD)/api.env:/lmql/.lmql/api.env -p 3000:3000 -p 3004:3004 lmql-docker:latest
</span></code></pre></div><h3 id="starting-a-container-with-gpu-and-local-models" tabindex="-1">Starting a container with GPU and local models <a class="header-anchor" href="#starting-a-container-with-gpu-and-local-models" aria-label="Permalink to &quot;Starting a container with GPU and local models&quot;">​</a></h3><p>Make sure you have followed the image building step from the section <code>Using GPU and local models</code>. To start the docker container with access to the GPUs consider using the following command:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="hljs"><code><span class="line">docker run --gpus <span class="hljs-built_in">all</span> -d -p <span class="hljs-number">3000</span>:<span class="hljs-number">3000</span> -p <span class="hljs-number">3004</span>:<span class="hljs-number">3004</span> lmql-docker:cuda11<span class="hljs-number">.8</span>
</span></code></pre></div><p>Where <code>all</code> means that you allocate all the GPUs to the docker container.</p><p>Note that here we expose the port <code>3000</code> and <code>3004</code> from the container to the port <code>3000</code> and <code>3004</code> from your machine. And we reuse the name <code>lmql-docker:cuda11.8</code> as it is the value we previously used to build the image.</p>`,24),l=[s];function i(c,d,r,p,h,u){return a(),t("div",null,l)}const v=e(n,[["render",i]]);export{g as __pageData,v as default};
