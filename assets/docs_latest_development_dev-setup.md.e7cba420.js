import{_ as e,o as t,c as a,Q as n}from"./chunks/framework.c2adf1ba.js";const v=JSON.parse('{"title":"Development Environment","description":"","frontmatter":{},"headers":[],"relativePath":"docs/latest/development/dev-setup.md","filePath":"docs/latest/development/dev-setup.md"}'),o={name:"docs/latest/development/dev-setup.md"},s=n(`<h1 id="development-environment" tabindex="-1">Development Environment <a class="header-anchor" href="#development-environment" aria-label="Permalink to &quot;Development Environment&quot;">​</a></h1><div class="subtitle">Getting started with core LMQL development.</div><h2 id="gpu-enabled-anaconda" tabindex="-1">GPU-Enabled Anaconda <a class="header-anchor" href="#gpu-enabled-anaconda" aria-label="Permalink to &quot;GPU-Enabled Anaconda&quot;">​</a></h2><p>To setup a <code>conda</code> environment for local LMQL development with GPU support, run the following commands:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="hljs"><code><span class="line"><span class="hljs-comment"># prepare conda environment</span>
conda env create -f scripts/conda/requirements.yml -n lmql
conda activate lmql

<span class="hljs-comment"># registers the \`lmql\` command in the current shell</span>
source scripts/activate-dev.sh
</span></code></pre></div><div class="info custom-block"><p class="custom-block-title">INFO</p><p><strong>Operating System</strong>: The GPU-enabled version of LMQL was tested to work on Ubuntu 22.04 with CUDA 12.0 and Windows 10 via WSL2 and CUDA 11.7. The no-GPU version (see below) was tested to work on Ubuntu 22.04 and macOS 13.2 Ventura or Windows 10 via WSL2.</p></div><h2 id="anaconda-development-without-gpu" tabindex="-1">Anaconda Development without GPU <a class="header-anchor" href="#anaconda-development-without-gpu" aria-label="Permalink to &quot;Anaconda Development without GPU&quot;">​</a></h2><p>This section outlines how to setup an LMQL development environment without local GPU support. Note that LMQL without local GPU support only supports the use of API-integrated models like <code>openai/gpt-3.5-turbo-instruct</code>.</p><p>To setup a <code>conda</code> environment for LMQL with GPU support, run the following commands:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="hljs"><code><span class="line"><span class="hljs-comment"># prepare conda environment</span>
conda env create -f scripts/conda/requirements-no-gpu.yml -n lmql-no-gpu
conda activate lmql-no-gpu

<span class="hljs-comment"># registers the \`lmql\` command in the current shell</span>
source scripts/activate-dev.sh
</span></code></pre></div><h2 id="with-nix" tabindex="-1">With Nix <a class="header-anchor" href="#with-nix" aria-label="Permalink to &quot;With Nix&quot;">​</a></h2><p>If you have <a href="https://nixos.org/" target="_blank" rel="noreferrer">Nix</a> installed, this can be used to invoke LMQL, even if you don&#39;t have any of its dependencies previously installed! We try to test Nix support on ARM-based MacOS and Intel-based Linux; bugfixes and contributions for other targets are welcome.</p><p>Most targets within the flake have several variants:</p><ul><li>default targets (<code>nix run github:eth-sri/lmql#playground</code>, <code>nix run github:eth-sri/lmql#python</code>, <code>nix run github:eth-sri/lmql#lmtp-server</code>, <code>nix develop github:eth-sri/lmql#lmql</code>) download all optional dependencies for maximum flexibility; these are also available with the suffix <code>-all</code> (<code>playground-all</code>, <code>python-all</code>, <code>lmtp-server-all</code>).</li><li><code>-basic</code> targets only support OpenAI (and any future models that require no optional dependencies).</li><li><code>-hf</code> targets only support OpenAI and Hugging Face models.</li><li><code>-replicate</code> targets are only guaranteed to support Hugging Face models remoted via Replicate. (In practice, at present, they may also support local Hugging Face models; but this is subject to change).</li><li><code>-llamaCpp</code> targets are only guaranteed to support llama.cpp. (In practice, again, Hugging Face may be available as well).</li></ul><p>In all of these cases, <code>github:eth-sri/lmql</code> may be replaced with a local filesystem path; so if you&#39;re inside a checked-out copy of the LMQL source tree, you can use <code>nix run .#playground</code> to run the playground/debugger from that tree.</p>`,15),l=[s];function i(c,r,d,p,u,h){return t(),a("div",null,l)}const g=e(o,[["render",i]]);export{v as __pageData,g as default};
