import{_ as e,o as s,c as a,Q as n}from"./chunks/framework.c2adf1ba.js";const g=JSON.parse('{"title":"LlamaIndex","description":"","frontmatter":{},"headers":[],"relativePath":"docs/lib/integrations/llama_index.md","filePath":"docs/lib/integrations/llama_index.md"}'),t={name:"docs/lib/integrations/llama_index.md"},l=n(`<h1 id="llamaindex" tabindex="-1">LlamaIndex <a class="header-anchor" href="#llamaindex" aria-label="Permalink to &quot;LlamaIndex&quot;">​</a></h1><div class="subtitle">Leverage LlamaIndex for retrieval-enhanced LMQL queries</div><p>LMQL can be used with the <a href="https://github.com/jerryjliu/llama_index" target="_blank" rel="noreferrer">LlamaIndex</a> python library. To illustrate, this notebook demonstrates how you can query a LlamaIndex data structure as part of an LMQL query.</p><p>This enables you to leverage <a href="https://gpt-index.readthedocs.io/en/latest/guides/primer/index_guide.html" target="_blank" rel="noreferrer">LlamaIndex&#39;s powerful index data structures</a>, to enrich the reasoning capabilities of an LMQL query with retrieved information from e.g. a text document that you provide.</p><h3 id="importing-libraries" tabindex="-1">Importing Libraries <a class="header-anchor" href="#importing-libraries" aria-label="Permalink to &quot;Importing Libraries&quot;">​</a></h3><p>First, we need to import the required LlamaIndex library. For this make sure llama_index is installed via <code>pip install llama_index</code>. Then, you can run the following commands to import the required <code>lmql</code> and <code>llama_index</code> components.</p><div class="language-lmql vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">lmql</span><pre class="hljs"><code><span class="line"><span class="hljs-keyword">import</span> lmql
<span class="hljs-keyword">from</span> llama_index <span class="hljs-keyword">import</span> GPTVectorStoreIndex, SimpleDirectoryReader, ServiceContext
</span></code></pre></div><h3 id="load-documents-and-build-index" tabindex="-1">Load Documents and Build Index <a class="header-anchor" href="#load-documents-and-build-index" aria-label="Permalink to &quot;Load Documents and Build Index&quot;">​</a></h3><p>In this example, we want to query the full text of the LMQL research paper for useful information during question answering. For this, we first load documents using LlamaIndex&#39;s <code>SimpleDirectoryReader</code>, and build a <code>GPTVectorStoreIndex</code> (an index that uses an in-memory embedding store).</p><div class="language-lmql vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">lmql</span><pre class="hljs"><code><span class="line"><span class="hljs-comment"># loads ./lmql.txt, the full text of the LMQL paper</span>
documents = SimpleDirectoryReader(<span class="hljs-string">&#39;.&#39;</span>).load_data() 
service_context = ServiceContext.from_defaults(chunk_size_limit=<span class="hljs-number">512</span>)
</span></code></pre></div><p>Next, we construct a retrieval index over the full text of the research paper.</p><div class="language-lmql vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">lmql</span><pre class="hljs"><code><span class="line">index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)
</span></code></pre></div><h3 id="question-answering-by-querying-with-llamaindex" tabindex="-1">Question Answering by Querying with LlamaIndex <a class="header-anchor" href="#question-answering-by-querying-with-llamaindex" aria-label="Permalink to &quot;Question Answering by Querying with LlamaIndex&quot;">​</a></h3><p>Now that we have an <code>index</code> to query, we can employ it during LMQL query execution. Since LMQL is fully integrated with the surrounding python program context, we can simply call <code>index.query(...)</code> during query execution to do so:</p><div class="language-lmql vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">lmql</span><pre class="hljs"><code><span class="line">similarity_top_k = <span class="hljs-number">2</span>

<span class="hljs-meta">@lmql.query(<span class="hljs-params">model=<span class="hljs-string">&quot;openai/gpt-3.5-turbo&quot;</span></span>)</span>
<span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">index_query</span>(<span class="hljs-params">question: <span class="hljs-built_in">str</span></span>):
    <span class="hljs-inline-lmql"><span class="inline-lmql-delim">&#39;&#39;&#39;lmql</span>
    <span class="hljs-string">&quot;You are a QA bot that helps users answer questions.\\n&quot;</span>
    
    <span class="hljs-comment"># ask the question</span>
    <span class="hljs-string">&quot;Question: <span class="hljs-subst">{question}</span>\\n&quot;</span>

    <span class="hljs-comment"># look up and insert relevant information into the context</span>
    response = index.query(question, response_mode=<span class="hljs-string">&quot;no_text&quot;</span>, similarity_top_k=similarity_top_k)
    information = <span class="hljs-string">&quot;\\n\\n&quot;</span>.join([s.node.get_text() <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> response.source_nodes])
    <span class="hljs-string">&quot;\\nRelevant Information: <span class="hljs-subst">{information}</span>\\n&quot;</span>
    
    <span class="hljs-comment"># generate a response</span>
    <span class="hljs-string">&quot;Your response based on relevant information:<span class="hljs-placeholder">[RESPONSE]</span>&quot;</span>
    <span class="inline-lmql-delim">&#39;&#39;&#39;</span></span>
</span></code></pre></div><p>Here, we first query the <code>index</code> using a given <code>question</code> and then process the retrieved document chunks, into an small summary answering <code>question</code>, by producing a corresponding <code>RESPONSE</code> output, using the ChatGPT, as specified in the <code>from</code>-clause of the query.</p><div class="language-lmql vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">lmql</span><pre class="hljs"><code><span class="line">result = <span class="hljs-keyword">await</span> index_query(<span class="hljs-string">&quot;What is scripted prompting in LMQL?&quot;</span>, 
                   output_writer=lmql.stream(variable=<span class="hljs-string">&quot;RESPONSE&quot;</span>))
</span></code></pre></div><div class="language-output vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">output</span><pre class="hljs"><code><span class="line">Scripted prompting <span class="hljs-keyword">in</span> LMQL refers to the ability to specify <span class="hljs-built_in">complex</span> interactions, control flow, <span class="hljs-keyword">and</span> constraints using lightweight scripting <span class="hljs-keyword">and</span> declarative SQL-like elements <span class="hljs-keyword">in</span> the Language Model Query Language (LMQL). This allows users to prompt language models <span class="hljs-keyword">with</span> precise constraints <span class="hljs-keyword">and</span> efficient decoding without requiring knowledge of the LM<span class="hljs-string">&#39;s internals. LMQL can be used to express a wide variety of existing prompting methods using simple, concise, and vendor-agnostic code. The underlying runtime is compatible with existing LMs and can be supported easily, requiring only a simple change in the decoder logic.
</span></span></code></pre></div>`,18),o=[l];function i(r,p,d,c,u,h){return s(),a("div",null,o)}const y=e(t,[["render",i]]);export{g as __pageData,y as default};
