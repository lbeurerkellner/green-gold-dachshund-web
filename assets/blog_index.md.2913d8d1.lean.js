import{_ as c,C as d,o as n,c as o,F as p,D as h,l as u,k as t,a as m,t as g,b as f,e as b}from"./chunks/framework.c2adf1ba.js";const y=JSON.parse('[{"src":"---\\ndate: 2023-09-23 10:10:00\\ntitle: LMQL 0.7 brings Procedural Prompt Programming\\n---\\n\\n# LMQL 0.7 brings Procedural Prompt Programming\\n\\n<span class=\\"date\\">September 23, 2023</span>\\n\\nToday, we are releasing LMQL 0.7. This series is the biggest update since the original release, including many community contributions. Next to many new main-line features like nested queries, the Generations API and the Chat API, it also includes several *experimental preview features*, allowing you to test drive the latest functionality before it is fully released.\\n\\nLMQL 0.7 has also moved to [semantic versioning](https://semver.org) with the direct predecessor being 0.0.6.6. This means that the next feature release will be 0.8, and the next bugfix release will be 0.7.1, on our way to the eventual 1.0 release.\\n\\n## Nested Queries for Procedural Prompt Programming\\n\\nIn 0.7, you can now use [Nested Queries](../../docs/language/nestedqueries.md) to call an LMQL query as a nested function in the context of another query. For this, LMQL implements procedural programming for prompting. To illustrate, consider the following example:\\n\\n```lmql\\n# chain of thought prompting strategy\\n@lmql.query\\ndef chain_of_thought():\\n    \'\'\'lmql\\n    \\"A: Let\'s think step by step.\\\\n [REASONING]\\"\\n    \\"Therefore the answer is[ANSWER]\\" where STOPS_AT(ANSWER, \\".\\")\\n    return ANSWER.strip()\\n    \'\'\'\\n\\n# top-level query\\n\\"Q: It is August 12th, 2020. What date was it \\\\\\n    100 days ago? [ANSWER: chain_of_thought]\\"\\n\\nANSWER # May 4th, 2020\\n```\\n\\nWe first define a simple LMQL function `chain_of_thought` to do *chain-of-thought prompting*. In our top-level query, we can then call this function to decode an answer using the `[ANSWER: chain_of_thought]` syntax. During execution, LMQL then inserts the instructions and constraints from `chain_of_thought` into the top-level query, generates a value for `ANSWER`, and then removes the instructions and constraints again, only returning the final result.\\n\\n**Nested queries are Prompt Function Calls.** This design of nested queries is inspired by the idea of *function or procedure calls* in traditional programming. Removing intermediate instructions and constraints also has parallels to the idea of *stack unwinding*, a technique to implement function calls in low-level languages. \\n\\nLMQL transfers these ideas to prompting, inheriting the general benefits of procedural programming:\\n\\n- **Encapsulation and Model Focus** Nested Queries encapsulate and hide the prompting logic used to generate `ANSWER`, which means our top-level query is much cleaner and more concise. Further, by hiding intermediate instructions from the model in the context of the top-level query, we can reduce noise in the overall prompt, allowing the model to focus on the currently relevant information only, and not get distracted by previous intermediate steps.\\n\\n- **Nesting and Reuse** LMQL queries can be nested arbitrarily deep, allowing you to reuse and combine queries modularly. For example, you could define a query `get_year` to extract a year from the response text, and then use this query in `chain_of_thought` to extract the date from the question. By achieving modularity for sub-prompts, nested queries also allow you to reuse prompts across different query programs.\\n\\nTo learn more about nested queries, please refer to the relevant chapter in our [documentation](../../docs/index.md).\\n\\n## Generations API\\n\\nLMQL 0.7 adds the *Generations API*, a lightweight high-level library for LMQL-based text generation and scoring. The API was designed to be easy to use and does not require users to write any LMQL themselves:\\n\\n```python\\n# obtain a model instance\\nm: lmql.LLM = lmql.model(\\"openai/gpt-3.5-turbo-instruct\\")\\n# simple generation\\nm.generate_sync(\\"Hello\\", max_tokens=10)\\n# -> Hello, I am a 23 year old female.\\n```\\n<br/>\\n\\nFunctions such as [`LLM.generate`](../../docs/lib/generations.html#llm-generate) and [`LLM.score`](../../docs/lib/generations.html#llm-score) allow you to generate and score text using any LMQL-support inference backend. The Generations API is also seamlessly compatible with standard LMQL, allowing you to switch and combine the two as needed. \\n\\nFor more information, please refer to the [documentation](../../docs/lib/generations.html).\\n\\n## Chat \\n\\nLMQL 0.7 adds a new [Chat API](../../docs/lib/chat.md), allowing you to easily deploy chatbots with just a couple lines of LMQL.\\n\\n<img style=\\"max-width: 80vw; width: 400pt; display: block; margin: auto;\\" src=\\"https://github.com/eth-sri/lmql/assets/17903049/3f24b964-b9b6-4c50-acaa-b38e54554506\\"/>\\n\\nLMQL Chat comes with custom output writers, that allow you to easily stream chatbot input and output over a variety of channels, including WebSockets, HTTP, and SSE. A simple `lmql chat` CLI tool was also added, that allows you to instantly launch your LMQL queries as fully interactive chatbots. \\n\\nWe also provide documentation resources on how to get started with chatbot development with LMQL, including chapters on Chatbot Serving, Internal Reasoning and Defending against Prompt Injection. For more information, please refer to the [documentation](../../docs/lib/chat.md).\\n\\n## Backends\\n\\nLMQL 0.7 ships with three new backends for inference and tokenization:\\n\\n* LMQL 0.7 adds support for OpenAI\'s newly released `gpt-3.5-turbo-instruct` model. In contrast to other 3.5 series models, this variant supports the *Completions API*, which means that LMQL constraints are compatible with it.\\n\\n* LMQL now supports hosting models on [replicate.com](https://replicate.com) infrastructure, allowing you to run LMQL models in the cloud. To learn more, please refer to the [documentation](../../docs/models/replicate.md). Thanks a lot to community member [@charles-dyfis-net](https://github.com/charles-dyfis-net) for contributing this!\\n\\n* LMQL added `sentencepiece` as an additional tokenization backend, specifically for `llama.cpp` models. This means, `llama.cpp` models can now be used without requiring `transformers` for tokenization. Thanks a lot to community member [@khushChopra](https://github.com/khushChopra) for contributing this.\\n\\n## Decorators\\n\\n[Variable Decorators](../../docs/language/decorators.md) offer a new and simple way to call custom Python functions as part of the core generation loop in LMQL:\\n\\n```lmql\\ndef screaming(value):\\n    \\"\\"\\"Decorator to convert a string to uppercase\\"\\"\\"\\n    return value.upper()\\n\\n\\"Say \'this is a test\':[@screaming TEST]\\"\\n```\\n```promptdown\\nSay \'this is a test\': [TEST| THIS IS A TEST]\\n```\\n\\nSimilar to Python decorators, LMQL decorators are functions that take a variable as input and can wrap and modify its value. \\n\\nIn the example above, we use the `@screaming` decorator to convert the value of `TEST` to uppercase. Decorators can be used to implement a wide range of custom functionality, including string normalization, datatype conversion, and more. LMQL also provides decorators that allow to stream or pre-process data during generation. For more information, please refer to the [documentation](../../docs/language/decorators.md).\\n\\n\\n## Documentation Update\\n\\nThe website and many chapters of the LMQL documentation have also been updated and extended and now include more examples and explanations. Further, we have updated the visual design to make it easier to read and navigate. \\n\\n## Preview Features\\n\\nApart from many new core features, LMQL 0.7 also ships with several *experimental preview features*, allowing you to test drive new functionality before it has fully stabilized and is released as main-line functionality.\\n\\nThese features are marked as *experimental* and are not yet fully supported. We are releasing them to gather feedback and to allow users to test them out early on. Note that these features are subject to change and may be removed/modified in future releases.\\n\\n### LMQL Actions <span class=\\"beta badge\\">Preview</span>\\n\\n*LMQL Actions* is the first version of LMQL\'s function calling layer. It allows you to expose arbitrary Python functions to the LLM reasoning loop and lets the model call them during generation. Function demonstration and the calling protocol can be both handled automatically by the LMQL runtime, allowing for simple use like this:\\n\\n```{lmql}\\ndef wiki(q): ...\\ndef calc(expr): ...\\n\\n\\"Q: What is the population of the US and Germany combined?\\"\\n\\"A: [REASONING]\\" where inline_use(REASONING, [wiki, calc])\\n```\\n\\nTo learn more about LMQL Actions, please refer to the [separate preview announcement here](https://lmql.ai/actions).\\n\\n### Regex Constraints <span class=\\"beta badge\\">Preview</span>\\n\\nLMQL now has support for regex constraints, allowing you to use regular expressions to constrain the output of a variable. For example, the following query will always generate a valid date of the form `DD/MM`:\\n\\n```{lmql}\\n\\"It\'s the last day of June so today is [RESPONSE]\\" where REGEX(RESPONSE, r\\"[0-9]{2}/[0-9]{2}\\")\\n```\\n\\n### Types / Datatype Constraints <span class=\\"beta badge\\">Preview</span>\\n\\nLMQL is moving towards fully typed LLM generation. On the way there, we have started to add support for *dataclass constraints*, allowing you to constrain the output of a variable to a specific structured output schema:\\n\\n```lmql\\nimport lmql\\nfrom dataclasses import dataclass\\n\\n@dataclass\\nclass Person:\\n    name: str\\n    age: int\\n    job: str\\n\\n\\"Alice is a 21 years old and works as an engineer at LMQL Inc in Zurich, Switzerland.\\\\n\\"\\n\\"Structured: [PERSON_DATA]\\\\n\\" where type(PERSON_DATA) is Person\\n\\nPERSON_DATA\\n# Person(name=\'Alice\', age=21, job=\'engineer\')\\n```\\n\\nTo achieve this, LMQL leverages constrained generation to make sure the LLM always produces all information required to populate a valid `Person` object. The resulting `PERSON_DATA` object can then be directly used like a regular Python object. Types are still in an early stage and we are working on adding more features and functionality. \\n\\n\\n## Other Changes\\n\\n* The LMQL playground can now be used from the Windows `cmd.exe`. Thanks a lot to community member [@mosheduminer](https://github.com/mosheduminer).\\n\\n* LMQL/LMTP model backends can now be accessed [as Langchain `LLM` objects](https://github.com/eth-sri/lmql/blob/main/src/lmql/models/lmtp/lmtp_langchain.py) to use them in your Langchain pipelines. Thanks to [@4onon](https://github.com/4onon) for contributing this. \\n\\n* LMQL can now be [installed as a NixOS package](https://github.com/eth-sri/lmql/tree/main/scripts/flake.d). Thanks to [@charles-dyfis-net](https://github.com/charles-dyfis-net) for contributing this.\\n\\n<br/>\\n\\n## 🎬 And that\'s a wrap!\\n\\nLMQL 0.7 is a big release and we are excited to see what you will build with it. As always, please let us know if you have any questions, suggestions or bug reports, on [GitHub](https://github.com/eth-sri/lmql), [Discord](https://discord.gg/7eJP4fcyNT), [Twitter](https://twitter.com/lmqllang) or via [hello@lmql.ai](mailto:hello@lmql.ai).","html":"<h1 id=\\"lmql-0-7-brings-procedural-prompt-programming\\" tabindex=\\"-1\\">LMQL 0.7 brings Procedural Prompt Programming <a class=\\"header-anchor\\" href=\\"#lmql-0-7-brings-procedural-prompt-programming\\" aria-label=\\"Permalink to &quot;LMQL 0.7 brings Procedural Prompt Programming&quot;\\">&ZeroWidthSpace;</a></h1>\\n<p><span class=\\"date\\">September 23, 2023</span></p>\\n<p>Today, we are releasing LMQL 0.7. This series is the biggest update since the original release, including many community contributions. Next to many new main-line features like nested queries, the Generations API and the Chat API, it also includes several <em>experimental preview features</em>, allowing you to test drive the latest functionality before it is fully released.</p>\\n<p>LMQL 0.7 has also moved to <a href=\\"https://semver.org\\" target=\\"_blank\\" rel=\\"noreferrer\\">semantic versioning</a> with the direct predecessor being 0.0.6.6. This means that the next feature release will be 0.8, and the next bugfix release will be 0.7.1, on our way to the eventual 1.0 release.</p>\\n<h2 id=\\"nested-queries-for-procedural-prompt-programming\\" tabindex=\\"-1\\">Nested Queries for Procedural Prompt Programming <a class=\\"header-anchor\\" href=\\"#nested-queries-for-procedural-prompt-programming\\" aria-label=\\"Permalink to &quot;Nested Queries for Procedural Prompt Programming&quot;\\">&ZeroWidthSpace;</a></h2>\\n<p>In 0.7, you can now use <a href=\\"./../../docs/language/nestedqueries.html\\">Nested Queries</a> to call an LMQL query as a nested function in the context of another query. For this, LMQL implements procedural programming for prompting. To illustrate, consider the following example:</p>\\n<div class=\\"language-lmql vp-adaptive-theme\\"><button title=\\"Copy Code\\" class=\\"copy\\"></button><span class=\\"lang\\">lmql</span><pre class=\\"hljs\\"><code><span class=\\"line\\"><span class=\\"hljs-comment\\"># chain of thought prompting strategy</span>\\n<span class=\\"hljs-meta\\">@lmql.query</span>\\n<span class=\\"hljs-keyword\\">def</span> <span class=\\"hljs-title function_\\">chain_of_thought</span>():\\n    <span class=\\"hljs-inline-lmql\\"><span class=\'inline-lmql-delim\'>&#x27;&#x27;&#x27;lmql</span>\\n    <span class=\\"hljs-string\\">&quot;A: Let&#x27;s think step by step.\\\\n <span class=\\"hljs-placeholder\\">[REASONING]</span>&quot;</span>\\n    <span class=\\"hljs-string\\">&quot;Therefore the answer is<span class=\\"hljs-placeholder\\">[ANSWER]</span>&quot;</span> <span class=\\"hljs-keyword\\">where</span> STOPS_AT(ANSWER, <span class=\\"hljs-string\\">&quot;.&quot;</span>)\\n    <span class=\\"hljs-keyword\\">return</span> ANSWER.strip()\\n    <span class=\'inline-lmql-delim\'>&#x27;&#x27;&#x27;</span></span>\\n\\n<span class=\\"hljs-comment\\"># top-level query</span>\\n<span class=\\"hljs-string\\">&quot;Q: It is August 12th, 2020. What date was it \\\\\\n    100 days ago? <span class=\\"hljs-placeholder\\">[ANSWER: chain_of_thought]</span>&quot;</span>\\n\\nANSWER <span class=\\"hljs-comment\\"># May 4th, 2020</span>\\n</span></code></pre>\\n</div><p>We first define a simple LMQL function <code>chain_of_thought</code> to do <em>chain-of-thought prompting</em>. In our top-level query, we can then call this function to decode an answer using the <code>[ANSWER: chain_of_thought]</code> syntax. During execution, LMQL then inserts the instructions and constraints from <code>chain_of_thought</code> into the top-level query, generates a value for <code>ANSWER</code>, and then removes the instructions and constraints again, only returning the final result.</p>\\n<p><strong>Nested queries are Prompt Function Calls.</strong> This design of nested queries is inspired by the idea of <em>function or procedure calls</em> in traditional programming. Removing intermediate instructions and constraints also has parallels to the idea of <em>stack unwinding</em>, a technique to implement function calls in low-level languages.</p>\\n<p>LMQL transfers these ideas to prompting, inheriting the general benefits of procedural programming:</p>\\n<ul>\\n<li>\\n<p><strong>Encapsulation and Model Focus</strong> Nested Queries encapsulate and hide the prompting logic used to generate <code>ANSWER</code>, which means our top-level query is much cleaner and more concise. Further, by hiding intermediate instructions from the model in the context of the top-level query, we can reduce noise in the overall prompt, allowing the model to focus on the currently relevant information only, and not get distracted by previous intermediate steps.</p>\\n</li>\\n<li>\\n<p><strong>Nesting and Reuse</strong> LMQL queries can be nested arbitrarily deep, allowing you to reuse and combine queries modularly. For example, you could define a query <code>get_year</code> to extract a year from the response text, and then use this query in <code>chain_of_thought</code> to extract the date from the question. By achieving modularity for sub-prompts, nested queries also allow you to reuse prompts across different query programs.</p>\\n</li>\\n</ul>\\n<p>To learn more about nested queries, please refer to the relevant chapter in our <a href=\\"./../../docs/\\">documentation</a>.</p>\\n<h2 id=\\"generations-api\\" tabindex=\\"-1\\">Generations API <a class=\\"header-anchor\\" href=\\"#generations-api\\" aria-label=\\"Permalink to &quot;Generations API&quot;\\">&ZeroWidthSpace;</a></h2>\\n<p>LMQL 0.7 adds the <em>Generations API</em>, a lightweight high-level library for LMQL-based text generation and scoring. The API was designed to be easy to use and does not require users to write any LMQL themselves:</p>\\n<div class=\\"language-python vp-adaptive-theme\\"><button title=\\"Copy Code\\" class=\\"copy\\"></button><span class=\\"lang\\">python</span><pre class=\\"hljs\\"><code><span class=\\"line\\"><span class=\\"hljs-comment\\"># obtain a model instance</span>\\nm: lmql.LLM = lmql.model(<span class=\\"hljs-string\\">&quot;openai/gpt-3.5-turbo-instruct&quot;</span>)\\n<span class=\\"hljs-comment\\"># simple generation</span>\\nm.generate_sync(<span class=\\"hljs-string\\">&quot;Hello&quot;</span>, max_tokens=<span class=\\"hljs-number\\">10</span>)\\n<span class=\\"hljs-comment\\"># -&gt; Hello, I am a 23 year old female.</span>\\n</span></code></pre>\\n</div><br/>\\n<p>Functions such as <a href=\\"./../../docs/lib/generations.html#llm-generate\\"><code>LLM.generate</code></a> and <a href=\\"./../../docs/lib/generations.html#llm-score\\"><code>LLM.score</code></a> allow you to generate and score text using any LMQL-support inference backend. The Generations API is also seamlessly compatible with standard LMQL, allowing you to switch and combine the two as needed.</p>\\n<p>For more information, please refer to the <a href=\\"./../../docs/lib/generations.html\\">documentation</a>.</p>\\n<h2 id=\\"chat\\" tabindex=\\"-1\\">Chat <a class=\\"header-anchor\\" href=\\"#chat\\" aria-label=\\"Permalink to &quot;Chat&quot;\\">&ZeroWidthSpace;</a></h2>\\n<p>LMQL 0.7 adds a new <a href=\\"./../../docs/lib/chat.html\\">Chat API</a>, allowing you to easily deploy chatbots with just a couple lines of LMQL.</p>\\n<img style=\\"max-width: 80vw; width: 400pt; display: block; margin: auto;\\" src=\\"https://github.com/eth-sri/lmql/assets/17903049/3f24b964-b9b6-4c50-acaa-b38e54554506\\"/>\\n<p>LMQL Chat comes with custom output writers, that allow you to easily stream chatbot input and output over a variety of channels, including WebSockets, HTTP, and SSE. A simple <code>lmql chat</code> CLI tool was also added, that allows you to instantly launch your LMQL queries as fully interactive chatbots.</p>\\n<p>We also provide documentation resources on how to get started with chatbot development with LMQL, including chapters on Chatbot Serving, Internal Reasoning and Defending against Prompt Injection. For more information, please refer to the <a href=\\"./../../docs/lib/chat.html\\">documentation</a>.</p>\\n<h2 id=\\"backends\\" tabindex=\\"-1\\">Backends <a class=\\"header-anchor\\" href=\\"#backends\\" aria-label=\\"Permalink to &quot;Backends&quot;\\">&ZeroWidthSpace;</a></h2>\\n<p>LMQL 0.7 ships with three new backends for inference and tokenization:</p>\\n<ul>\\n<li>\\n<p>LMQL 0.7 adds support for OpenAI\'s newly released <code>gpt-3.5-turbo-instruct</code> model. In contrast to other 3.5 series models, this variant supports the <em>Completions API</em>, which means that LMQL constraints are compatible with it.</p>\\n</li>\\n<li>\\n<p>LMQL now supports hosting models on <a href=\\"https://replicate.com\\" target=\\"_blank\\" rel=\\"noreferrer\\">replicate.com</a> infrastructure, allowing you to run LMQL models in the cloud. To learn more, please refer to the <a href=\\"./../../docs/models/replicate.html\\">documentation</a>. Thanks a lot to community member <a href=\\"https://github.com/charles-dyfis-net\\" target=\\"_blank\\" rel=\\"noreferrer\\">@charles-dyfis-net</a> for contributing this!</p>\\n</li>\\n<li>\\n<p>LMQL added <code>sentencepiece</code> as an additional tokenization backend, specifically for <code>llama.cpp</code> models. This means, <code>llama.cpp</code> models can now be used without requiring <code>transformers</code> for tokenization. Thanks a lot to community member <a href=\\"https://github.com/khushChopra\\" target=\\"_blank\\" rel=\\"noreferrer\\">@khushChopra</a> for contributing this.</p>\\n</li>\\n</ul>\\n<h2 id=\\"decorators\\" tabindex=\\"-1\\">Decorators <a class=\\"header-anchor\\" href=\\"#decorators\\" aria-label=\\"Permalink to &quot;Decorators&quot;\\">&ZeroWidthSpace;</a></h2>\\n<p><a href=\\"./../../docs/language/decorators.html\\">Variable Decorators</a> offer a new and simple way to call custom Python functions as part of the core generation loop in LMQL:</p>\\n<div class=\\"language-lmql vp-adaptive-theme\\"><button title=\\"Copy Code\\" class=\\"copy\\"></button><span class=\\"lang\\">lmql</span><pre class=\\"hljs\\"><code><span class=\\"line\\"><span class=\\"hljs-keyword\\">def</span> <span class=\\"hljs-title function_\\">screaming</span>(<span class=\\"hljs-params\\">value</span>):\\n    <span class=\\"hljs-string\\">&quot;&quot;&quot;Decorator to convert a string to uppercase&quot;&quot;&quot;</span>\\n    <span class=\\"hljs-keyword\\">return</span> value.upper()\\n\\n<span class=\\"hljs-string\\">&quot;Say &#x27;this is a test&#x27;:<span class=\\"hljs-placeholder\\">[@screaming TEST]</span>&quot;</span>\\n</span></code></pre>\\n</div><div class=\\"language-promptdown vp-adaptive-theme\\"><button title=\\"Copy Code\\" class=\\"copy\\"></button><span class=\\"lang\\">promptdown</span><pre pd-text=\\"Say \'this is a test\': [TEST| THIS IS A TEST]\\n\\" animate=\\"true\\" __animate=\\"true\\" animate-speed=\\"50\\" class=\\"promptdown promptdown-compiled\\" style=\\"opacity: 1;\\"><p pd-shadow-id=\\"2053\\" text=\\"S\\" pd-insertion-point=\\"true\\">Say \'this is a test\': <span pd-shadow-id=\\"2055\\" pd-instant=\\"false\\" text=\\"\\" class=\\"promptdown-var color-pink\\"><span pd-shadow-id=\\"2056\\" text=\\"T\\" class=\\"promptdown-var-name\\">TEST</span> THIS IS A TEST</span>\\n</p></pre>\\n</div><p>Similar to Python decorators, LMQL decorators are functions that take a variable as input and can wrap and modify its value.</p>\\n<p>In the example above, we use the <code>@screaming</code> decorator to convert the value of <code>TEST</code> to uppercase. Decorators can be used to implement a wide range of custom functionality, including string normalization, datatype conversion, and more. LMQL also provides decorators that allow to stream or pre-process data during generation. For more information, please refer to the <a href=\\"./../../docs/language/decorators.html\\">documentation</a>.</p>\\n<h2 id=\\"documentation-update\\" tabindex=\\"-1\\">Documentation Update <a class=\\"header-anchor\\" href=\\"#documentation-update\\" aria-label=\\"Permalink to &quot;Documentation Update&quot;\\">&ZeroWidthSpace;</a></h2>\\n<p>The website and many chapters of the LMQL documentation have also been updated and extended and now include more examples and explanations. Further, we have updated the visual design to make it easier to read and navigate.</p>\\n<h2 id=\\"preview-features\\" tabindex=\\"-1\\">Preview Features <a class=\\"header-anchor\\" href=\\"#preview-features\\" aria-label=\\"Permalink to &quot;Preview Features&quot;\\">&ZeroWidthSpace;</a></h2>\\n<p>Apart from many new core features, LMQL 0.7 also ships with several <em>experimental preview features</em>, allowing you to test drive new functionality before it has fully stabilized and is released as main-line functionality.</p>\\n<p>These features are marked as <em>experimental</em> and are not yet fully supported. We are releasing them to gather feedback and to allow users to test them out early on. Note that these features are subject to change and may be removed/modified in future releases.</p>\\n<h3 id=\\"lmql-actions-preview\\" tabindex=\\"-1\\">LMQL Actions <span class=\\"beta badge\\">Preview</span> <a class=\\"header-anchor\\" href=\\"#lmql-actions-preview\\" aria-label=\\"Permalink to &quot;LMQL Actions &lt;span class=&quot;beta badge&quot;&gt;Preview&lt;/span&gt;&quot;\\">&ZeroWidthSpace;</a></h3>\\n<p><em>LMQL Actions</em> is the first version of LMQL\'s function calling layer. It allows you to expose arbitrary Python functions to the LLM reasoning loop and lets the model call them during generation. Function demonstration and the calling protocol can be both handled automatically by the LMQL runtime, allowing for simple use like this:</p>\\n<div class=\\"language- vp-adaptive-theme\\"><button title=\\"Copy Code\\" class=\\"copy\\"></button><span class=\\"lang\\"></span><pre class=\\"hljs\\"><code><span class=\\"line\\"><span class=\\"hljs-keyword\\">def</span> <span class=\\"hljs-title function_\\">wiki</span>(<span class=\\"hljs-params\\">q</span>): ...\\n<span class=\\"hljs-keyword\\">def</span> <span class=\\"hljs-title function_\\">calc</span>(<span class=\\"hljs-params\\">expr</span>): ...\\n\\n<span class=\\"hljs-string\\">&quot;Q: What is the population of the US and Germany combined?&quot;</span>\\n<span class=\\"hljs-string\\">&quot;A: <span class=\\"hljs-placeholder\\">[REASONING]</span>&quot;</span> <span class=\\"hljs-keyword\\">where</span> inline_use(REASONING, [wiki, calc])\\n</span></code></pre>\\n</div><p>To learn more about LMQL Actions, please refer to the <a href=\\"https://lmql.ai/actions\\" target=\\"_blank\\" rel=\\"noreferrer\\">separate preview announcement here</a>.</p>\\n<h3 id=\\"regex-constraints-preview\\" tabindex=\\"-1\\">Regex Constraints <span class=\\"beta badge\\">Preview</span> <a class=\\"header-anchor\\" href=\\"#regex-constraints-preview\\" aria-label=\\"Permalink to &quot;Regex Constraints &lt;span class=&quot;beta badge&quot;&gt;Preview&lt;/span&gt;&quot;\\">&ZeroWidthSpace;</a></h3>\\n<p>LMQL now has support for regex constraints, allowing you to use regular expressions to constrain the output of a variable. For example, the following query will always generate a valid date of the form <code>DD/MM</code>:</p>\\n<div class=\\"language- vp-adaptive-theme\\"><button title=\\"Copy Code\\" class=\\"copy\\"></button><span class=\\"lang\\"></span><pre class=\\"hljs\\"><code><span class=\\"line\\"><span class=\\"hljs-string\\">&quot;It&#x27;s the last day of June so today is <span class=\\"hljs-placeholder\\">[RESPONSE]</span>&quot;</span> <span class=\\"hljs-keyword\\">where</span> REGEX(RESPONSE, r<span class=\\"hljs-string\\">&quot;<span class=\\"hljs-placeholder\\">[<span class=\\"hljs-number\\">0</span>-<span class=\\"hljs-number\\">9</span>]</span><span class=\\"hljs-subst\\">{<span class=\\"hljs-number\\">2</span>}</span>/<span class=\\"hljs-placeholder\\">[<span class=\\"hljs-number\\">0</span>-<span class=\\"hljs-number\\">9</span>]</span><span class=\\"hljs-subst\\">{<span class=\\"hljs-number\\">2</span>}</span>&quot;</span>)\\n</span></code></pre>\\n</div><h3 id=\\"types-datatype-constraints-preview\\" tabindex=\\"-1\\">Types / Datatype Constraints <span class=\\"beta badge\\">Preview</span> <a class=\\"header-anchor\\" href=\\"#types-datatype-constraints-preview\\" aria-label=\\"Permalink to &quot;Types / Datatype Constraints &lt;span class=&quot;beta badge&quot;&gt;Preview&lt;/span&gt;&quot;\\">&ZeroWidthSpace;</a></h3>\\n<p>LMQL is moving towards fully typed LLM generation. On the way there, we have started to add support for <em>dataclass constraints</em>, allowing you to constrain the output of a variable to a specific structured output schema:</p>\\n<div class=\\"language-lmql vp-adaptive-theme\\"><button title=\\"Copy Code\\" class=\\"copy\\"></button><span class=\\"lang\\">lmql</span><pre class=\\"hljs\\"><code><span class=\\"line\\"><span class=\\"hljs-keyword\\">import</span> lmql\\n<span class=\\"hljs-keyword\\">from</span> dataclasses <span class=\\"hljs-keyword\\">import</span> dataclass\\n\\n<span class=\\"hljs-meta\\">@dataclass</span>\\n<span class=\\"hljs-keyword\\">class</span> <span class=\\"hljs-title class_\\">Person</span>:\\n    name: <span class=\\"hljs-built_in\\">str</span>\\n    age: <span class=\\"hljs-built_in\\">int</span>\\n    job: <span class=\\"hljs-built_in\\">str</span>\\n\\n<span class=\\"hljs-string\\">&quot;Alice is a 21 years old and works as an engineer at LMQL Inc in Zurich, Switzerland.\\\\n&quot;</span>\\n<span class=\\"hljs-string\\">&quot;Structured: <span class=\\"hljs-placeholder\\">[PERSON_DATA]</span>\\\\n&quot;</span> <span class=\\"hljs-keyword\\">where</span> <span class=\\"hljs-built_in\\">type</span>(PERSON_DATA) <span class=\\"hljs-keyword\\">is</span> Person\\n\\nPERSON_DATA\\n<span class=\\"hljs-comment\\"># Person(name=&#x27;Alice&#x27;, age=21, job=&#x27;engineer&#x27;)</span>\\n</span></code></pre>\\n</div><p>To achieve this, LMQL leverages constrained generation to make sure the LLM always produces all information required to populate a valid <code>Person</code> object. The resulting <code>PERSON_DATA</code> object can then be directly used like a regular Python object. Types are still in an early stage and we are working on adding more features and functionality.</p>\\n<h2 id=\\"other-changes\\" tabindex=\\"-1\\">Other Changes <a class=\\"header-anchor\\" href=\\"#other-changes\\" aria-label=\\"Permalink to &quot;Other Changes&quot;\\">&ZeroWidthSpace;</a></h2>\\n<ul>\\n<li>\\n<p>The LMQL playground can now be used from the Windows <code>cmd.exe</code>. Thanks a lot to community member <a href=\\"https://github.com/mosheduminer\\" target=\\"_blank\\" rel=\\"noreferrer\\">@mosheduminer</a>.</p>\\n</li>\\n<li>\\n<p>LMQL/LMTP model backends can now be accessed <a href=\\"https://github.com/eth-sri/lmql/blob/main/src/lmql/models/lmtp/lmtp_langchain.py\\" target=\\"_blank\\" rel=\\"noreferrer\\">as Langchain <code>LLM</code> objects</a> to use them in your Langchain pipelines. Thanks to <a href=\\"https://github.com/4onon\\" target=\\"_blank\\" rel=\\"noreferrer\\">@4onon</a> for contributing this.</p>\\n</li>\\n<li>\\n<p>LMQL can now be <a href=\\"https://github.com/eth-sri/lmql/tree/main/scripts/flake.d\\" target=\\"_blank\\" rel=\\"noreferrer\\">installed as a NixOS package</a>. Thanks to <a href=\\"https://github.com/charles-dyfis-net\\" target=\\"_blank\\" rel=\\"noreferrer\\">@charles-dyfis-net</a> for contributing this.</p>\\n</li>\\n</ul>\\n<br/>\\n<h2 id=\\"🎬-and-that-s-a-wrap\\" tabindex=\\"-1\\">🎬 And that\'s a wrap! <a class=\\"header-anchor\\" href=\\"#🎬-and-that-s-a-wrap\\" aria-label=\\"Permalink to &quot;🎬 And that\'s a wrap!&quot;\\">&ZeroWidthSpace;</a></h2>\\n<p>LMQL 0.7 is a big release and we are excited to see what you will build with it. As always, please let us know if you have any questions, suggestions or bug reports, on <a href=\\"https://github.com/eth-sri/lmql\\" target=\\"_blank\\" rel=\\"noreferrer\\">GitHub</a>, <a href=\\"https://discord.gg/7eJP4fcyNT\\" target=\\"_blank\\" rel=\\"noreferrer\\">Discord</a>, <a href=\\"https://twitter.com/lmqllang\\" target=\\"_blank\\" rel=\\"noreferrer\\">Twitter</a> or via <a href=\\"mailto:hello@lmql.ai\\" target=\\"_blank\\" rel=\\"noreferrer\\">hello@lmql.ai</a>.</p>\\n","frontmatter":{"date":"2023-09-23T10:10:00.000Z","title":"LMQL 0.7 brings Procedural Prompt Programming"},"excerpt":"","url":"/blog/posts/release-0.7.html"},{"src":"---\\ndate: 2023-07-25\\ntitle: LMQL 0.0.6.6 v0.0.6\\n---\\nWe just released LMQL *0.0.6.6*. This is a minor update with a couple of smaller fixes and improvements.\\n\\n* `lmql.F` now supports positional arguments:\\n\\n```python\\ngreet = lmql.F(\\"Greet {a} and {b}: [GREETING]\\")\\n\\n# call with positional arguments\\ngreet(\\"Alice\\", \\"Bob\\") # Greet Alice and Bob: Hello!\\n# call with keyword arguments\\ngreet(a=\\"Alice\\", b=\\"Bob\\") # Greet Alice and Bob: Hello!\\n```\\n\\n* We improved the error handling of the `llama.cpp` backend and fixed a bug with model identifier parsing. \\n\\n* We also fixed a bug with the LMTP scheduler, where CPU load was high even when no tasks were present. Thanks to community member [@4onen](https://github.com/4onen) for reporting and fixing this!\\n\\n* Added backend support for `auto_gptq` quantized models, contributed by community member [@meditans](https://github.com/meditans).\\n\\n* We fixed an issue where for Azure OpenAI models, a dummy configuration `api.env` was needed. See our [documentation](../../docs/models/azure.md) for details. Thanks to community members Missing and [@hooman-bayer](https://github.com/hooman-bayer) for their feedback and contributions to this.\\n\\n> **Versioning Note**: 0.0.6.6 is the last release with two leading zeros. Starting with the next release, LMQL will adopt semantic versioning and use a single leading zero, i.e. 0.6.7.","html":"<p>We just released LMQL <em>0.0.6.6</em>. This is a minor update with a couple of smaller fixes and improvements.</p>\\n<ul>\\n<li><code>lmql.F</code> now supports positional arguments:</li>\\n</ul>\\n<div class=\\"language-python vp-adaptive-theme\\"><button title=\\"Copy Code\\" class=\\"copy\\"></button><span class=\\"lang\\">python</span><pre class=\\"hljs\\"><code><span class=\\"line\\">greet = lmql.F(<span class=\\"hljs-string\\">&quot;Greet <span class=\\"hljs-subst\\">{a}</span> and <span class=\\"hljs-subst\\">{b}</span>: <span class=\\"hljs-placeholder\\">[GREETING]</span>&quot;</span>)\\n\\n<span class=\\"hljs-comment\\"># call with positional arguments</span>\\ngreet(<span class=\\"hljs-string\\">&quot;Alice&quot;</span>, <span class=\\"hljs-string\\">&quot;Bob&quot;</span>) <span class=\\"hljs-comment\\"># Greet Alice and Bob: Hello!</span>\\n<span class=\\"hljs-comment\\"># call with keyword arguments</span>\\ngreet(a=<span class=\\"hljs-string\\">&quot;Alice&quot;</span>, b=<span class=\\"hljs-string\\">&quot;Bob&quot;</span>) <span class=\\"hljs-comment\\"># Greet Alice and Bob: Hello!</span>\\n</span></code></pre>\\n</div><ul>\\n<li>\\n<p>We improved the error handling of the <code>llama.cpp</code> backend and fixed a bug with model identifier parsing.</p>\\n</li>\\n<li>\\n<p>We also fixed a bug with the LMTP scheduler, where CPU load was high even when no tasks were present. Thanks to community member <a href=\\"https://github.com/4onen\\" target=\\"_blank\\" rel=\\"noreferrer\\">@4onen</a> for reporting and fixing this!</p>\\n</li>\\n<li>\\n<p>Added backend support for <code>auto_gptq</code> quantized models, contributed by community member <a href=\\"https://github.com/meditans\\" target=\\"_blank\\" rel=\\"noreferrer\\">@meditans</a>.</p>\\n</li>\\n<li>\\n<p>We fixed an issue where for Azure OpenAI models, a dummy configuration <code>api.env</code> was needed. See our <a href=\\"./../../docs/models/azure.html\\">documentation</a> for details. Thanks to community members Missing and <a href=\\"https://github.com/hooman-bayer\\" target=\\"_blank\\" rel=\\"noreferrer\\">@hooman-bayer</a> for their feedback and contributions to this.</p>\\n</li>\\n</ul>\\n<blockquote>\\n<p><strong>Versioning Note</strong>: 0.0.6.6 is the last release with two leading zeros. Starting with the next release, LMQL will adopt semantic versioning and use a single leading zero, i.e. 0.6.7.</p>\\n</blockquote>\\n","frontmatter":{"date":"2023-07-25T00:00:00.000Z","title":"LMQL 0.0.6.6 v0.0.6"},"excerpt":"","url":"/blog/posts/release-0.0.6.6.html"}]');const w={class:"posts"},L={class:"post"},v=["href"],q=["innerHTML"],T=JSON.parse('{"title":"Blog","description":"","frontmatter":{"title":"Blog","layout":"doc","aside":false,"outline":false},"headers":[],"relativePath":"blog/index.md","filePath":"blog/index.md"}'),k={name:"blog/index.md"},x=Object.assign(k,{setup(M){function r(e){const s=e.match(/v\d+\.\d+(\.\d+)?/);if(s)return s[0]}function i(e){return console.log(e),e.replace(/v\d+\.\d+(\.\d+)?/,"").trim()}return(e,s)=>{const l=d("Badge");return n(),o("div",null,[(n(!0),o(p,null,h(u(y),a=>(n(),o("div",w,[t("div",L,[t("a",{href:a.url},[t("h1",null,[m(g(i(a.frontmatter.title))+" ",1),r(a.frontmatter.title)?(n(),f(l,{key:0,type:"tip",text:r(a.frontmatter.title)},null,8,["text"])):b("",!0)])],8,v),t("div",{class:"body",innerHTML:a.html},null,8,q)])]))),256))])}}}),P=c(x,[["__scopeId","data-v-f9bfe28a"]]);export{T as __pageData,P as default};
