import{_ as o,o as a,c as t,F as r,D as i,l,k as e,t as c}from"./chunks/framework.c2adf1ba.js";const p=JSON.parse('[{"src":"---\\ndate: 2023-09-23 10:10:00\\ntitle: LMQL 0.7 brings Procedural Prompt Programming\\n---\\n\\n# LMQL 0.7 brings Procedural Prompt Programming\\n\\n<span class=\\"date\\">September 23, 2023</span>\\n\\nToday, we are releasing LMQL 0.7. This series is the biggest update since the original release, including many community contributions. Next to several new main-line features like nested queries, the Generations API and the Chat API, it also includes several *experimental preview features*, allowing you to experiment with new incoming functionality before it is fully released.\\n\\nLMQL 0.7 has also moved to [semantic versioning](https://semver.org) with the direct predecessor being 0.0.6.6. This means that the next feature release will be 0.8, and the next bugfix release will be 0.7.1.\\n\\n## Nested Queries for Procedural Prompt Programming\\n\\nIn 0.7, you can now use [Nested Queries](../../docs/language/nestedqueries.md) to call an LMQL query as a nested function in the context of another query. For this, LMQL implements procedural programming for prompting. To illustrate, consider the following example:\\n\\n```lmql\\n# chain of thought prompting strategy\\n@lmql.query\\ndef chain_of_thought():\\n    \'\'\'lmql\\n    \\"A: Let\'s think step by step.\\\\n [REASONING]\\"\\n    \\"Therefore the answer is[ANSWER]\\" where STOPS_AT(ANSWER, \\".\\")\\n    return ANSWER.strip()\\n    \'\'\'\\n\\n# top-level query\\n\\"Q: It is August 12th, 2020. What date was it \\\\\\n    100 days ago? [ANSWER: chain_of_thought]\\"\\n\\nANSWER # May 4th, 2020\\n```\\n\\nWe first define a simple LMQL function `chain_of_thought` to do *chain-of-thought prompting*. In our top-level query, we can then call this function to decode an answer using the `[ANSWER: chain_of_thought]` syntax. During execution, LMQL then inserts the instructions and constraints from `chain_of_thought` into the top-level query, generates a value for `ANSWER`, and then removes the instructions and constraints again, only returning the final result.\\n\\n**Nested queries are Prompt Function Calls.** This design of nested queries is inspired by the idea of *function or procedure calls* in traditional programming. Removing intermediate instructions and constraints also has parallels to the idea of *stack unwinding*, a technique to implement function calls in low-level languages. \\n\\nLMQL transfers these ideas to prompting, inheriting the general benefits of procedural programming:\\n\\n- **Encapsulation and Model Focus** Nested Queries encapsulate and hide the prompting logic used to generate `ANSWER`, which means our top-level query is much cleaner and more concise. Further, by hiding intermediate instructions from the model in the context of the top-level query, we can reduce noise in the overall prompt, allowing the model to focus on the currently relevant information only, and not get distracted by previous intermediate steps.\\n\\n- **Nesting and Reuse** LMQL queries can be nested arbitrarily deep, allowing you to reuse and combine queries modularly. For example, you could define a query `get_year` to extract a year from the response text, and then use this query in `chain_of_thought` to extract the date from the question. By achieving modularity for sub-prompts, nested queries also allow you to reuse prompts across different query programs.\\n\\nTo learn more about nested queries, please refer to the [relevant chapter in the documentation](../../docs/language/nestedqueries.md).\\n\\n## Generations API\\n\\nLMQL 0.7 adds the *Generations API*, a lightweight high-level library for LMQL-based text generation and scoring. The API was designed to be easy to use and does not require users to write any LMQL themselves:\\n\\n```python\\n# obtain a model instance\\nm: lmql.LLM = lmql.model(\\"openai/gpt-3.5-turbo-instruct\\")\\n# simple generation\\nm.generate_sync(\\"Hello\\", max_tokens=10)\\n# -> Hello, I am a 23 year old female.\\n```\\n<br/>\\n\\nFunctions such as [`LLM.generate`](../../docs/lib/generations.html#llm-generate) and [`LLM.score`](../../docs/lib/generations.html#llm-score) allow you to generate and score text using any LMQL-support inference backend. The Generations API is also seamlessly compatible with standard LMQL, allowing you to switch and combine the two as needed. \\n\\nFor more information, please refer to the [documentation](../../docs/lib/generations.html).\\n\\n## Chat \\n\\nLMQL 0.7 adds a new [Chat API](../../docs/lib/chat.md), allowing you to easily deploy chatbots with just a couple lines of LMQL.\\n\\n<img style=\\"max-width: 80vw; width: 400pt; display: block; margin: auto;\\" src=\\"https://github.com/eth-sri/lmql/assets/17903049/3f24b964-b9b6-4c50-acaa-b38e54554506\\"/>\\n\\nLMQL Chat comes with custom output writers, that allow you to easily stream chatbot input and output over a variety of channels, including WebSockets, HTTP, and SSE. A simple `lmql chat` CLI tool was also added, that allows you to instantly launch your LMQL queries as fully interactive chatbots. \\n\\nWe also provide documentation resources on how to get started with chatbot development with LMQL, including chapters on Chatbot Serving, Internal Reasoning and Defending against Prompt Injection. For more information, please refer to the [documentation](../../docs/lib/chat.md).\\n\\n## Backends\\n\\nLMQL 0.7 ships with three new backends for inference and tokenization:\\n\\n* LMQL 0.7 adds support for OpenAI\'s newly released `gpt-3.5-turbo-instruct` model. In contrast to other 3.5 series models, this variant supports the *Completions API*, which means that LMQL constraints are compatible with it.\\n\\n* LMQL now supports hosting models on [replicate.com](https://replicate.com) infrastructure, allowing you to run LMQL models in the cloud. To learn more, please refer to the [documentation](../../docs/models/replicate.md). Thanks a lot to community member [@charles-dyfis-net](https://github.com/charles-dyfis-net) for contributing this!\\n\\n* LMQL added `sentencepiece` as an additional tokenization backend, specifically for `llama.cpp` models. This means, `llama.cpp` models can now be used without requiring `transformers` for tokenization. Thanks a lot to community member [@khushChopra](https://github.com/khushChopra) for contributing this.\\n\\n\\n## Inference Certificates\\n\\nTo make LLM inference more transparent and re-producible, LMQL 0.7 also adds [*inference certificates*](../../docs/lib/inference-certificates.md). An inference certificate is a simple data structure that records essential information needed to reproduce an inference result. Certificates can be generated for any LLM call that happens in an LMQL context.\\n\\nTo produce an inference certificate, pass `certificate=True` or `certificate=<filename>` to your query or generate call:\\n\\n```truncated\\n# call and save certificate\\nsay_hello(certificate=\\"my-certificate.json\\")\\n```\\n\\nThe resulting certificate file provides a way to document, trace and reproduce LLM inference results by recording the *exact (tokenized) prompts* and information on the *environment and generation parameters*.\\n\\nThis can be helpful to better understand what is happening during inference, to debug issues, and to reproduce results. It also offers a way to document LLM failures, to better guide the discussion around the concrete capabilities and limitations of LLMs.\\n\\n## Decorators\\n\\n[Variable Decorators](../../docs/language/decorators.md) offer a new and simple way to call custom Python functions as part of the core generation loop in LMQL:\\n\\n```lmql\\ndef screaming(value):\\n    \\"\\"\\"Decorator to convert a string to uppercase\\"\\"\\"\\n    return value.upper()\\n\\n\\"Say \'this is a test\':[@screaming TEST]\\"\\n```\\n```promptdown\\nSay \'this is a test\': [TEST| THIS IS A TEST]\\n```\\n\\nSimilar to Python decorators, LMQL decorators are functions that take a variable as input and can wrap and modify its value. \\n\\nIn the example above, we use the `@screaming` decorator to convert the value of `TEST` to uppercase. Decorators can be used to implement a wide range of custom functionality, including string normalization, datatype conversion, and more. LMQL also provides decorators that allow to stream or pre-process data during generation. For more information, please refer to the [documentation](../../docs/language/decorators.md).\\n\\n\\n## Documentation Update\\n\\nThe website and many chapters of the LMQL documentation have also been updated and extended and now include more examples and explanations. We have updated the visual design to make it easier to read and navigate. \\n\\nThe documentation now also includes a *work-in-progress* [Language Reference](/docs/language/reference.md), which aims to provide a more comprehensive and formal description of LMQL\'s syntax and semantics, all in one place.\\n\\n## Preview Features\\n\\nApart from many new core features, LMQL 0.7 also ships with several *experimental preview features*, allowing you to test drive new functionality before it has fully stabilized and is released as main-line functionality.\\n\\nThese features are marked as *experimental* and are not yet fully supported. We are releasing them to gather feedback and to allow users to test them out early on. Note that these features are subject to change and may be removed/modified in future releases.\\n\\n### LMQL Actions <span class=\\"beta badge\\">Preview</span>\\n\\n*LMQL Actions* is the first version of LMQL\'s function calling layer. It allows you to expose arbitrary Python functions to the LLM reasoning loop and lets the model call them during generation. Function demonstration and the calling protocol can be both handled automatically by the LMQL runtime, allowing for simple use like this:\\n\\n```{lmql}\\ndef wiki(q): ...\\ndef calc(expr): ...\\n\\n\\"Q: What is the population of the US and Germany combined?\\"\\n\\"A: [REASONING]\\" where inline_use(REASONING, [wiki, calc])\\n```\\n\\nTo learn more about LMQL Actions, please refer to the [separate preview announcement here](https://lmql.ai/actions).\\n\\n### Regex Constraints <span class=\\"beta badge\\">Preview</span>\\n\\nLMQL now has support for regex constraints, allowing you to use regular expressions to constrain the output of a variable. For example, the following query will always generate a valid date of the form `DD/MM`:\\n\\n```{lmql}\\n\\"It\'s the last day of June so today is [RESPONSE]\\" where REGEX(RESPONSE, r\\"[0-9]{2}/[0-9]{2}\\")\\n```\\n\\n### Types / Datatype Constraints <span class=\\"beta badge\\">Preview</span>\\n\\nLMQL is moving towards fully typed LLM generation. On the way there, we have started to add support for *dataclass constraints*, allowing you to constrain the output of a variable to a specific structured output schema:\\n\\n```lmql\\nimport lmql\\nfrom dataclasses import dataclass\\n\\n@dataclass\\nclass Person:\\n    name: str\\n    age: int\\n    job: str\\n\\n\\"Alice is a 21 years old and works as an engineer at LMQL Inc in Zurich, Switzerland.\\\\n\\"\\n\\"Structured: [PERSON_DATA]\\\\n\\" where type(PERSON_DATA) is Person\\n\\nPERSON_DATA\\n# Person(name=\'Alice\', age=21, job=\'engineer\')\\n```\\n\\nTo achieve this, LMQL leverages constrained generation to make sure the LLM always produces all information required to populate a valid `Person` object. The resulting `PERSON_DATA` object can then be directly used like a regular Python object. Types are still in an early stage and we are working on adding more features and functionality. \\n\\n\\n## Other Changes\\n\\n* The LMQL playground can now be used from the Windows `cmd.exe`. Thanks a lot to community member [@mosheduminer](https://github.com/mosheduminer).\\n\\n* LMQL/LMTP model backends can now be accessed [as Langchain `LLM` objects](https://github.com/eth-sri/lmql/blob/main/src/lmql/models/lmtp/lmtp_langchain.py) to use them in your Langchain pipelines. Thanks to [@4onon](https://github.com/4onon) for contributing this. \\n\\n* LMQL can now be [installed as a NixOS package](https://github.com/eth-sri/lmql/tree/main/scripts/flake.d). Thanks to [@charles-dyfis-net](https://github.com/charles-dyfis-net) for contributing this.\\n\\n## 🎬 And that\'s a wrap!\\n\\nLMQL 0.7 is a big release and we are excited to see what you will build with it. As always, please let us know if you have any questions, suggestions or bug reports, on [GitHub](https://github.com/eth-sri/lmql), [Discord](https://discord.gg/7eJP4fcyNT), [Twitter](https://twitter.com/lmqllang) or via [hello@lmql.ai](mailto:hello@lmql.ai).","html":"<h1 id=\\"lmql-0-7-brings-procedural-prompt-programming\\" tabindex=\\"-1\\">LMQL 0.7 brings Procedural Prompt Programming <a class=\\"header-anchor\\" href=\\"#lmql-0-7-brings-procedural-prompt-programming\\" aria-label=\\"Permalink to &quot;LMQL 0.7 brings Procedural Prompt Programming&quot;\\">&ZeroWidthSpace;</a></h1>\\n<p><span class=\\"date\\">September 23, 2023</span></p>\\n<p>Today, we are releasing LMQL 0.7. This series is the biggest update since the original release, including many community contributions. Next to several new main-line features like nested queries, the Generations API and the Chat API, it also includes several <em>experimental preview features</em>, allowing you to experiment with new incoming functionality before it is fully released.</p>\\n<p>LMQL 0.7 has also moved to <a href=\\"https://semver.org\\" target=\\"_blank\\" rel=\\"noreferrer\\">semantic versioning</a> with the direct predecessor being 0.0.6.6. This means that the next feature release will be 0.8, and the next bugfix release will be 0.7.1.</p>\\n<h2 id=\\"nested-queries-for-procedural-prompt-programming\\" tabindex=\\"-1\\">Nested Queries for Procedural Prompt Programming <a class=\\"header-anchor\\" href=\\"#nested-queries-for-procedural-prompt-programming\\" aria-label=\\"Permalink to &quot;Nested Queries for Procedural Prompt Programming&quot;\\">&ZeroWidthSpace;</a></h2>\\n<p>In 0.7, you can now use <a href=\\"./../../docs/language/nestedqueries.html\\">Nested Queries</a> to call an LMQL query as a nested function in the context of another query. For this, LMQL implements procedural programming for prompting. To illustrate, consider the following example:</p>\\n<div class=\\"language-lmql vp-adaptive-theme\\"><button title=\\"Copy Code\\" class=\\"copy\\"></button><span class=\\"lang\\">lmql</span><pre class=\\"hljs\\"><code><span class=\\"line\\"><span class=\\"hljs-comment\\"># chain of thought prompting strategy</span>\\n<span class=\\"hljs-meta\\">@lmql.query</span>\\n<span class=\\"hljs-keyword\\">def</span> <span class=\\"hljs-title function_\\">chain_of_thought</span>():\\n    <span class=\\"hljs-inline-lmql\\"><span class=\'inline-lmql-delim\'>&#x27;&#x27;&#x27;lmql</span>\\n    <span class=\\"hljs-string\\">&quot;A: Let&#x27;s think step by step.\\\\n <span class=\\"hljs-placeholder\\">[REASONING]</span>&quot;</span>\\n    <span class=\\"hljs-string\\">&quot;Therefore the answer is<span class=\\"hljs-placeholder\\">[ANSWER]</span>&quot;</span> <span class=\\"hljs-keyword\\">where</span> STOPS_AT(ANSWER, <span class=\\"hljs-string\\">&quot;.&quot;</span>)\\n    <span class=\\"hljs-keyword\\">return</span> ANSWER.strip()\\n    <span class=\'inline-lmql-delim\'>&#x27;&#x27;&#x27;</span></span>\\n\\n<span class=\\"hljs-comment\\"># top-level query</span>\\n<span class=\\"hljs-string\\">&quot;Q: It is August 12th, 2020. What date was it \\\\\\n    100 days ago? <span class=\\"hljs-placeholder\\">[ANSWER: chain_of_thought]</span>&quot;</span>\\n\\nANSWER <span class=\\"hljs-comment\\"># May 4th, 2020</span>\\n</span></code></pre>\\n</div><p>We first define a simple LMQL function <code>chain_of_thought</code> to do <em>chain-of-thought prompting</em>. In our top-level query, we can then call this function to decode an answer using the <code>[ANSWER: chain_of_thought]</code> syntax. During execution, LMQL then inserts the instructions and constraints from <code>chain_of_thought</code> into the top-level query, generates a value for <code>ANSWER</code>, and then removes the instructions and constraints again, only returning the final result.</p>\\n<p><strong>Nested queries are Prompt Function Calls.</strong> This design of nested queries is inspired by the idea of <em>function or procedure calls</em> in traditional programming. Removing intermediate instructions and constraints also has parallels to the idea of <em>stack unwinding</em>, a technique to implement function calls in low-level languages.</p>\\n<p>LMQL transfers these ideas to prompting, inheriting the general benefits of procedural programming:</p>\\n<ul>\\n<li>\\n<p><strong>Encapsulation and Model Focus</strong> Nested Queries encapsulate and hide the prompting logic used to generate <code>ANSWER</code>, which means our top-level query is much cleaner and more concise. Further, by hiding intermediate instructions from the model in the context of the top-level query, we can reduce noise in the overall prompt, allowing the model to focus on the currently relevant information only, and not get distracted by previous intermediate steps.</p>\\n</li>\\n<li>\\n<p><strong>Nesting and Reuse</strong> LMQL queries can be nested arbitrarily deep, allowing you to reuse and combine queries modularly. For example, you could define a query <code>get_year</code> to extract a year from the response text, and then use this query in <code>chain_of_thought</code> to extract the date from the question. By achieving modularity for sub-prompts, nested queries also allow you to reuse prompts across different query programs.</p>\\n</li>\\n</ul>\\n<p>To learn more about nested queries, please refer to the <a href=\\"./../../docs/language/nestedqueries.html\\">relevant chapter in the documentation</a>.</p>\\n<h2 id=\\"generations-api\\" tabindex=\\"-1\\">Generations API <a class=\\"header-anchor\\" href=\\"#generations-api\\" aria-label=\\"Permalink to &quot;Generations API&quot;\\">&ZeroWidthSpace;</a></h2>\\n<p>LMQL 0.7 adds the <em>Generations API</em>, a lightweight high-level library for LMQL-based text generation and scoring. The API was designed to be easy to use and does not require users to write any LMQL themselves:</p>\\n<div class=\\"language-python vp-adaptive-theme\\"><button title=\\"Copy Code\\" class=\\"copy\\"></button><span class=\\"lang\\">python</span><pre class=\\"hljs\\"><code><span class=\\"line\\"><span class=\\"hljs-comment\\"># obtain a model instance</span>\\nm: lmql.LLM = lmql.model(<span class=\\"hljs-string\\">&quot;openai/gpt-3.5-turbo-instruct&quot;</span>)\\n<span class=\\"hljs-comment\\"># simple generation</span>\\nm.generate_sync(<span class=\\"hljs-string\\">&quot;Hello&quot;</span>, max_tokens=<span class=\\"hljs-number\\">10</span>)\\n<span class=\\"hljs-comment\\"># -&gt; Hello, I am a 23 year old female.</span>\\n</span></code></pre>\\n</div><br/>\\n<p>Functions such as <a href=\\"./../../docs/lib/generations.html#llm-generate\\"><code>LLM.generate</code></a> and <a href=\\"./../../docs/lib/generations.html#llm-score\\"><code>LLM.score</code></a> allow you to generate and score text using any LMQL-support inference backend. The Generations API is also seamlessly compatible with standard LMQL, allowing you to switch and combine the two as needed.</p>\\n<p>For more information, please refer to the <a href=\\"./../../docs/lib/generations.html\\">documentation</a>.</p>\\n<h2 id=\\"chat\\" tabindex=\\"-1\\">Chat <a class=\\"header-anchor\\" href=\\"#chat\\" aria-label=\\"Permalink to &quot;Chat&quot;\\">&ZeroWidthSpace;</a></h2>\\n<p>LMQL 0.7 adds a new <a href=\\"./../../docs/lib/chat.html\\">Chat API</a>, allowing you to easily deploy chatbots with just a couple lines of LMQL.</p>\\n<img style=\\"max-width: 80vw; width: 400pt; display: block; margin: auto;\\" src=\\"https://github.com/eth-sri/lmql/assets/17903049/3f24b964-b9b6-4c50-acaa-b38e54554506\\"/>\\n<p>LMQL Chat comes with custom output writers, that allow you to easily stream chatbot input and output over a variety of channels, including WebSockets, HTTP, and SSE. A simple <code>lmql chat</code> CLI tool was also added, that allows you to instantly launch your LMQL queries as fully interactive chatbots.</p>\\n<p>We also provide documentation resources on how to get started with chatbot development with LMQL, including chapters on Chatbot Serving, Internal Reasoning and Defending against Prompt Injection. For more information, please refer to the <a href=\\"./../../docs/lib/chat.html\\">documentation</a>.</p>\\n<h2 id=\\"backends\\" tabindex=\\"-1\\">Backends <a class=\\"header-anchor\\" href=\\"#backends\\" aria-label=\\"Permalink to &quot;Backends&quot;\\">&ZeroWidthSpace;</a></h2>\\n<p>LMQL 0.7 ships with three new backends for inference and tokenization:</p>\\n<ul>\\n<li>\\n<p>LMQL 0.7 adds support for OpenAI\'s newly released <code>gpt-3.5-turbo-instruct</code> model. In contrast to other 3.5 series models, this variant supports the <em>Completions API</em>, which means that LMQL constraints are compatible with it.</p>\\n</li>\\n<li>\\n<p>LMQL now supports hosting models on <a href=\\"https://replicate.com\\" target=\\"_blank\\" rel=\\"noreferrer\\">replicate.com</a> infrastructure, allowing you to run LMQL models in the cloud. To learn more, please refer to the <a href=\\"./../../docs/models/replicate.html\\">documentation</a>. Thanks a lot to community member <a href=\\"https://github.com/charles-dyfis-net\\" target=\\"_blank\\" rel=\\"noreferrer\\">@charles-dyfis-net</a> for contributing this!</p>\\n</li>\\n<li>\\n<p>LMQL added <code>sentencepiece</code> as an additional tokenization backend, specifically for <code>llama.cpp</code> models. This means, <code>llama.cpp</code> models can now be used without requiring <code>transformers</code> for tokenization. Thanks a lot to community member <a href=\\"https://github.com/khushChopra\\" target=\\"_blank\\" rel=\\"noreferrer\\">@khushChopra</a> for contributing this.</p>\\n</li>\\n</ul>\\n<h2 id=\\"inference-certificates\\" tabindex=\\"-1\\">Inference Certificates <a class=\\"header-anchor\\" href=\\"#inference-certificates\\" aria-label=\\"Permalink to &quot;Inference Certificates&quot;\\">&ZeroWidthSpace;</a></h2>\\n<p>To make LLM inference more transparent and re-producible, LMQL 0.7 also adds <a href=\\"./../../docs/lib/inference-certificates.html\\"><em>inference certificates</em></a>. An inference certificate is a simple data structure that records essential information needed to reproduce an inference result. Certificates can be generated for any LLM call that happens in an LMQL context.</p>\\n<p>To produce an inference certificate, pass <code>certificate=True</code> or <code>certificate=&lt;filename&gt;</code> to your query or generate call:</p>\\n<div class=\\"language-truncated vp-adaptive-theme\\"><button title=\\"Copy Code\\" class=\\"copy\\"></button><span class=\\"lang\\">truncated</span><pre class=\\"hljs\\"><code><span class=\\"line\\"><span class=\\"hljs-comment\\"># call and save certificate</span>\\nsay_hello(certificate=<span class=\\"hljs-string\\">&quot;my-certificate.json&quot;</span>)\\n</span></code></pre>\\n</div><p>The resulting certificate file provides a way to document, trace and reproduce LLM inference results by recording the <em>exact (tokenized) prompts</em> and information on the <em>environment and generation parameters</em>.</p>\\n<p>This can be helpful to better understand what is happening during inference, to debug issues, and to reproduce results. It also offers a way to document LLM failures, to better guide the discussion around the concrete capabilities and limitations of LLMs.</p>\\n<h2 id=\\"decorators\\" tabindex=\\"-1\\">Decorators <a class=\\"header-anchor\\" href=\\"#decorators\\" aria-label=\\"Permalink to &quot;Decorators&quot;\\">&ZeroWidthSpace;</a></h2>\\n<p><a href=\\"./../../docs/language/decorators.html\\">Variable Decorators</a> offer a new and simple way to call custom Python functions as part of the core generation loop in LMQL:</p>\\n<div class=\\"language-lmql vp-adaptive-theme\\"><button title=\\"Copy Code\\" class=\\"copy\\"></button><span class=\\"lang\\">lmql</span><pre class=\\"hljs\\"><code><span class=\\"line\\"><span class=\\"hljs-keyword\\">def</span> <span class=\\"hljs-title function_\\">screaming</span>(<span class=\\"hljs-params\\">value</span>):\\n    <span class=\\"hljs-string\\">&quot;&quot;&quot;Decorator to convert a string to uppercase&quot;&quot;&quot;</span>\\n    <span class=\\"hljs-keyword\\">return</span> value.upper()\\n\\n<span class=\\"hljs-string\\">&quot;Say &#x27;this is a test&#x27;:<span class=\\"hljs-placeholder\\">[@screaming TEST]</span>&quot;</span>\\n</span></code></pre>\\n</div><div class=\\"language-promptdown vp-adaptive-theme\\"><button title=\\"Copy Code\\" class=\\"copy\\"></button><span class=\\"lang\\">promptdown</span><pre pd-text=\\"Say \'this is a test\': [TEST| THIS IS A TEST]\\n\\" animate=\\"true\\" __animate=\\"true\\" animate-speed=\\"50\\" class=\\"promptdown promptdown-compiled\\" style=\\"opacity: 1;\\"><p pd-shadow-id=\\"2155\\" text=\\"S\\" pd-insertion-point=\\"true\\">Say \'this is a test\': <span pd-shadow-id=\\"2157\\" pd-instant=\\"false\\" text=\\"\\" class=\\"promptdown-var color-pink\\"><span pd-shadow-id=\\"2158\\" text=\\"T\\" class=\\"promptdown-var-name\\">TEST</span> THIS IS A TEST</span>\\n</p></pre>\\n</div><p>Similar to Python decorators, LMQL decorators are functions that take a variable as input and can wrap and modify its value.</p>\\n<p>In the example above, we use the <code>@screaming</code> decorator to convert the value of <code>TEST</code> to uppercase. Decorators can be used to implement a wide range of custom functionality, including string normalization, datatype conversion, and more. LMQL also provides decorators that allow to stream or pre-process data during generation. For more information, please refer to the <a href=\\"./../../docs/language/decorators.html\\">documentation</a>.</p>\\n<h2 id=\\"documentation-update\\" tabindex=\\"-1\\">Documentation Update <a class=\\"header-anchor\\" href=\\"#documentation-update\\" aria-label=\\"Permalink to &quot;Documentation Update&quot;\\">&ZeroWidthSpace;</a></h2>\\n<p>The website and many chapters of the LMQL documentation have also been updated and extended and now include more examples and explanations. We have updated the visual design to make it easier to read and navigate.</p>\\n<p>The documentation now also includes a <em>work-in-progress</em> <a href=\\"/docs/language/reference.html\\">Language Reference</a>, which aims to provide a more comprehensive and formal description of LMQL\'s syntax and semantics, all in one place.</p>\\n<h2 id=\\"preview-features\\" tabindex=\\"-1\\">Preview Features <a class=\\"header-anchor\\" href=\\"#preview-features\\" aria-label=\\"Permalink to &quot;Preview Features&quot;\\">&ZeroWidthSpace;</a></h2>\\n<p>Apart from many new core features, LMQL 0.7 also ships with several <em>experimental preview features</em>, allowing you to test drive new functionality before it has fully stabilized and is released as main-line functionality.</p>\\n<p>These features are marked as <em>experimental</em> and are not yet fully supported. We are releasing them to gather feedback and to allow users to test them out early on. Note that these features are subject to change and may be removed/modified in future releases.</p>\\n<h3 id=\\"lmql-actions-preview\\" tabindex=\\"-1\\">LMQL Actions <span class=\\"beta badge\\">Preview</span> <a class=\\"header-anchor\\" href=\\"#lmql-actions-preview\\" aria-label=\\"Permalink to &quot;LMQL Actions &lt;span class=&quot;beta badge&quot;&gt;Preview&lt;/span&gt;&quot;\\">&ZeroWidthSpace;</a></h3>\\n<p><em>LMQL Actions</em> is the first version of LMQL\'s function calling layer. It allows you to expose arbitrary Python functions to the LLM reasoning loop and lets the model call them during generation. Function demonstration and the calling protocol can be both handled automatically by the LMQL runtime, allowing for simple use like this:</p>\\n<div class=\\"language- vp-adaptive-theme\\"><button title=\\"Copy Code\\" class=\\"copy\\"></button><span class=\\"lang\\"></span><pre class=\\"hljs\\"><code><span class=\\"line\\"><span class=\\"hljs-keyword\\">def</span> <span class=\\"hljs-title function_\\">wiki</span>(<span class=\\"hljs-params\\">q</span>): ...\\n<span class=\\"hljs-keyword\\">def</span> <span class=\\"hljs-title function_\\">calc</span>(<span class=\\"hljs-params\\">expr</span>): ...\\n\\n<span class=\\"hljs-string\\">&quot;Q: What is the population of the US and Germany combined?&quot;</span>\\n<span class=\\"hljs-string\\">&quot;A: <span class=\\"hljs-placeholder\\">[REASONING]</span>&quot;</span> <span class=\\"hljs-keyword\\">where</span> inline_use(REASONING, [wiki, calc])\\n</span></code></pre>\\n</div><p>To learn more about LMQL Actions, please refer to the <a href=\\"https://lmql.ai/actions\\" target=\\"_blank\\" rel=\\"noreferrer\\">separate preview announcement here</a>.</p>\\n<h3 id=\\"regex-constraints-preview\\" tabindex=\\"-1\\">Regex Constraints <span class=\\"beta badge\\">Preview</span> <a class=\\"header-anchor\\" href=\\"#regex-constraints-preview\\" aria-label=\\"Permalink to &quot;Regex Constraints &lt;span class=&quot;beta badge&quot;&gt;Preview&lt;/span&gt;&quot;\\">&ZeroWidthSpace;</a></h3>\\n<p>LMQL now has support for regex constraints, allowing you to use regular expressions to constrain the output of a variable. For example, the following query will always generate a valid date of the form <code>DD/MM</code>:</p>\\n<div class=\\"language- vp-adaptive-theme\\"><button title=\\"Copy Code\\" class=\\"copy\\"></button><span class=\\"lang\\"></span><pre class=\\"hljs\\"><code><span class=\\"line\\"><span class=\\"hljs-string\\">&quot;It&#x27;s the last day of June so today is <span class=\\"hljs-placeholder\\">[RESPONSE]</span>&quot;</span> <span class=\\"hljs-keyword\\">where</span> REGEX(RESPONSE, r<span class=\\"hljs-string\\">&quot;<span class=\\"hljs-placeholder\\">[<span class=\\"hljs-number\\">0</span>-<span class=\\"hljs-number\\">9</span>]</span><span class=\\"hljs-subst\\">{<span class=\\"hljs-number\\">2</span>}</span>/<span class=\\"hljs-placeholder\\">[<span class=\\"hljs-number\\">0</span>-<span class=\\"hljs-number\\">9</span>]</span><span class=\\"hljs-subst\\">{<span class=\\"hljs-number\\">2</span>}</span>&quot;</span>)\\n</span></code></pre>\\n</div><h3 id=\\"types-datatype-constraints-preview\\" tabindex=\\"-1\\">Types / Datatype Constraints <span class=\\"beta badge\\">Preview</span> <a class=\\"header-anchor\\" href=\\"#types-datatype-constraints-preview\\" aria-label=\\"Permalink to &quot;Types / Datatype Constraints &lt;span class=&quot;beta badge&quot;&gt;Preview&lt;/span&gt;&quot;\\">&ZeroWidthSpace;</a></h3>\\n<p>LMQL is moving towards fully typed LLM generation. On the way there, we have started to add support for <em>dataclass constraints</em>, allowing you to constrain the output of a variable to a specific structured output schema:</p>\\n<div class=\\"language-lmql vp-adaptive-theme\\"><button title=\\"Copy Code\\" class=\\"copy\\"></button><span class=\\"lang\\">lmql</span><pre class=\\"hljs\\"><code><span class=\\"line\\"><span class=\\"hljs-keyword\\">import</span> lmql\\n<span class=\\"hljs-keyword\\">from</span> dataclasses <span class=\\"hljs-keyword\\">import</span> dataclass\\n\\n<span class=\\"hljs-meta\\">@dataclass</span>\\n<span class=\\"hljs-keyword\\">class</span> <span class=\\"hljs-title class_\\">Person</span>:\\n    name: <span class=\\"hljs-built_in\\">str</span>\\n    age: <span class=\\"hljs-built_in\\">int</span>\\n    job: <span class=\\"hljs-built_in\\">str</span>\\n\\n<span class=\\"hljs-string\\">&quot;Alice is a 21 years old and works as an engineer at LMQL Inc in Zurich, Switzerland.\\\\n&quot;</span>\\n<span class=\\"hljs-string\\">&quot;Structured: <span class=\\"hljs-placeholder\\">[PERSON_DATA]</span>\\\\n&quot;</span> <span class=\\"hljs-keyword\\">where</span> <span class=\\"hljs-built_in\\">type</span>(PERSON_DATA) <span class=\\"hljs-keyword\\">is</span> Person\\n\\nPERSON_DATA\\n<span class=\\"hljs-comment\\"># Person(name=&#x27;Alice&#x27;, age=21, job=&#x27;engineer&#x27;)</span>\\n</span></code></pre>\\n</div><p>To achieve this, LMQL leverages constrained generation to make sure the LLM always produces all information required to populate a valid <code>Person</code> object. The resulting <code>PERSON_DATA</code> object can then be directly used like a regular Python object. Types are still in an early stage and we are working on adding more features and functionality.</p>\\n<h2 id=\\"other-changes\\" tabindex=\\"-1\\">Other Changes <a class=\\"header-anchor\\" href=\\"#other-changes\\" aria-label=\\"Permalink to &quot;Other Changes&quot;\\">&ZeroWidthSpace;</a></h2>\\n<ul>\\n<li>\\n<p>The LMQL playground can now be used from the Windows <code>cmd.exe</code>. Thanks a lot to community member <a href=\\"https://github.com/mosheduminer\\" target=\\"_blank\\" rel=\\"noreferrer\\">@mosheduminer</a>.</p>\\n</li>\\n<li>\\n<p>LMQL/LMTP model backends can now be accessed <a href=\\"https://github.com/eth-sri/lmql/blob/main/src/lmql/models/lmtp/lmtp_langchain.py\\" target=\\"_blank\\" rel=\\"noreferrer\\">as Langchain <code>LLM</code> objects</a> to use them in your Langchain pipelines. Thanks to <a href=\\"https://github.com/4onon\\" target=\\"_blank\\" rel=\\"noreferrer\\">@4onon</a> for contributing this.</p>\\n</li>\\n<li>\\n<p>LMQL can now be <a href=\\"https://github.com/eth-sri/lmql/tree/main/scripts/flake.d\\" target=\\"_blank\\" rel=\\"noreferrer\\">installed as a NixOS package</a>. Thanks to <a href=\\"https://github.com/charles-dyfis-net\\" target=\\"_blank\\" rel=\\"noreferrer\\">@charles-dyfis-net</a> for contributing this.</p>\\n</li>\\n</ul>\\n<h2 id=\\"🎬-and-that-s-a-wrap\\" tabindex=\\"-1\\">🎬 And that\'s a wrap! <a class=\\"header-anchor\\" href=\\"#🎬-and-that-s-a-wrap\\" aria-label=\\"Permalink to &quot;🎬 And that\'s a wrap!&quot;\\">&ZeroWidthSpace;</a></h2>\\n<p>LMQL 0.7 is a big release and we are excited to see what you will build with it. As always, please let us know if you have any questions, suggestions or bug reports, on <a href=\\"https://github.com/eth-sri/lmql\\" target=\\"_blank\\" rel=\\"noreferrer\\">GitHub</a>, <a href=\\"https://discord.gg/7eJP4fcyNT\\" target=\\"_blank\\" rel=\\"noreferrer\\">Discord</a>, <a href=\\"https://twitter.com/lmqllang\\" target=\\"_blank\\" rel=\\"noreferrer\\">Twitter</a> or via <a href=\\"mailto:hello@lmql.ai\\" target=\\"_blank\\" rel=\\"noreferrer\\">hello@lmql.ai</a>.</p>\\n","frontmatter":{"date":"2023-09-23T10:10:00.000Z","title":"LMQL 0.7 brings Procedural Prompt Programming"},"excerpt":"","url":"/blog/posts/release-0.7.html"},{"src":"---\\ndate: 2023-07-25\\ntitle: LMQL v0.0.6.6\\n---\\n\\n<span class=\\"date\\">July 25, 2023</span>\\n\\nWe just released LMQL *0.0.6.6*. This is a minor update with a couple of smaller fixes and improvements.\\n\\n* `lmql.F` now supports positional arguments:\\n\\n```python\\ngreet = lmql.F(\\"Greet {a} and {b}: [GREETING]\\")\\n\\n# call with positional arguments\\ngreet(\\"Alice\\", \\"Bob\\") # Greet Alice and Bob: Hello!\\n# call with keyword arguments\\ngreet(a=\\"Alice\\", b=\\"Bob\\") # Greet Alice and Bob: Hello!\\n```\\n\\n* We improved the error handling of the `llama.cpp` backend and fixed a bug with model identifier parsing. \\n\\n* We also fixed a bug with the LMTP scheduler, where CPU load was high even when no tasks were present. Thanks to community member [@4onen](https://github.com/4onen) for reporting and fixing this!\\n\\n* Added backend support for `auto_gptq` quantized models, contributed by community member [@meditans](https://github.com/meditans).\\n\\n* We fixed an issue where for Azure OpenAI models, a dummy configuration `api.env` was needed. See our [documentation](../../docs/models/azure.md) for details. Thanks to community members Missing and [@hooman-bayer](https://github.com/hooman-bayer) for their feedback and contributions to this.\\n\\n> **Versioning Note**: 0.0.6.6 is the last release with two leading zeros. Starting with the next release, LMQL will adopt semantic versioning and use a single leading zero, i.e. 0.6.7.","html":"<p><span class=\\"date\\">July 25, 2023</span></p>\\n<p>We just released LMQL <em>0.0.6.6</em>. This is a minor update with a couple of smaller fixes and improvements.</p>\\n<ul>\\n<li><code>lmql.F</code> now supports positional arguments:</li>\\n</ul>\\n<div class=\\"language-python vp-adaptive-theme\\"><button title=\\"Copy Code\\" class=\\"copy\\"></button><span class=\\"lang\\">python</span><pre class=\\"hljs\\"><code><span class=\\"line\\">greet = lmql.F(<span class=\\"hljs-string\\">&quot;Greet <span class=\\"hljs-subst\\">{a}</span> and <span class=\\"hljs-subst\\">{b}</span>: <span class=\\"hljs-placeholder\\">[GREETING]</span>&quot;</span>)\\n\\n<span class=\\"hljs-comment\\"># call with positional arguments</span>\\ngreet(<span class=\\"hljs-string\\">&quot;Alice&quot;</span>, <span class=\\"hljs-string\\">&quot;Bob&quot;</span>) <span class=\\"hljs-comment\\"># Greet Alice and Bob: Hello!</span>\\n<span class=\\"hljs-comment\\"># call with keyword arguments</span>\\ngreet(a=<span class=\\"hljs-string\\">&quot;Alice&quot;</span>, b=<span class=\\"hljs-string\\">&quot;Bob&quot;</span>) <span class=\\"hljs-comment\\"># Greet Alice and Bob: Hello!</span>\\n</span></code></pre>\\n</div><ul>\\n<li>\\n<p>We improved the error handling of the <code>llama.cpp</code> backend and fixed a bug with model identifier parsing.</p>\\n</li>\\n<li>\\n<p>We also fixed a bug with the LMTP scheduler, where CPU load was high even when no tasks were present. Thanks to community member <a href=\\"https://github.com/4onen\\" target=\\"_blank\\" rel=\\"noreferrer\\">@4onen</a> for reporting and fixing this!</p>\\n</li>\\n<li>\\n<p>Added backend support for <code>auto_gptq</code> quantized models, contributed by community member <a href=\\"https://github.com/meditans\\" target=\\"_blank\\" rel=\\"noreferrer\\">@meditans</a>.</p>\\n</li>\\n<li>\\n<p>We fixed an issue where for Azure OpenAI models, a dummy configuration <code>api.env</code> was needed. See our <a href=\\"./../../docs/models/azure.html\\">documentation</a> for details. Thanks to community members Missing and <a href=\\"https://github.com/hooman-bayer\\" target=\\"_blank\\" rel=\\"noreferrer\\">@hooman-bayer</a> for their feedback and contributions to this.</p>\\n</li>\\n</ul>\\n<blockquote>\\n<p><strong>Versioning Note</strong>: 0.0.6.6 is the last release with two leading zeros. Starting with the next release, LMQL will adopt semantic versioning and use a single leading zero, i.e. 0.6.7.</p>\\n</blockquote>\\n","frontmatter":{"date":"2023-07-25T00:00:00.000Z","title":"LMQL v0.0.6.6"},"excerpt":"","url":"/blog/posts/release-0.0.6.6.html"},{"src":"---\\ndate: 2023-07-13\\ntitle: LMQL becomes simpler and adds llama.cpp\\n---\\n\\n# LMQL becomes simpler and adds llama.cpp\\n\\n<span class=\\"date\\">July 13, 2023</span>\\n\\nToday we are releasing LMQL 0.0.6.5. This update contains a major simplification of the LMQL syntax, moving it much closer to standard Python. It also includes a `llama.cpp` based inference backend, several bug fixes and other minor improvements.\\n\\nYou can try the latest version of LMQL in your browser at [lmql.ai/playground](https://lmql.ai/playground) or install it via `pip install lmql`.\\n\\n## One Line Is All It Takes\\n\\nMost notably, 0.0.6.5 comes with several simplifications of the core syntax of LMQL. Of course, all changes are backwards compatible, so you can continue to use your existing query code and move to the new version without any changes.\\n\\nWith this, we aim to minimize syntactic overhead, employing sensible defaults to enable more concise programs like the following:\\n\\n```{lmql}\\nname::simple-syntax\\n\\n\\"One line is all it takes [CONTINUATION]\\"\\n```\\n```promptdown\\nOne line is all it takes [CONTINUATION|Fallin\' in love with me.]\\n```\\n\\n**Sensible Defaults** This is possible because LMQL now automatically assumes `argmax` and `openai/text-davinci-003` as (configurable) default model. If you prefer to use \\na different model or custom decoder settings, you can still specify them explicitly, e.g. in the `@lmql.query` decorator function as demonstrated later in this post.\\n\\nWithout any additional configuration, the simple query code above translates to a full LMQL program like this:\\n\\n```{lmql}\\nname::simple-syntax-default\\n\\nargmax \\"One line is all it takes [CONTINUATION]\\" from \\"openai/text-davinci-003\\"\\n```\\n\\n<br/>\\n\\n### Inline Constraints\\n\\nLMQL now allows you to specify several inline `where` constraints. This enables constraints that refer to local program variables, which means constraints can now be dependent on previous model outputs.\\n\\n```{lmql}\\nname::list-with-array\\n\\n\\"A list of awesome Dua Lipa songs:\\\\n\\"\\nsongs = []\\n\\n\\"- New Rules\\\\n\\"\\nfor i in range(4):\\n    \\"-[SONG]\\\\n\\" where STOPS_BEFORE(SONG, \\"\\\\n\\")\\n    songs.append(SONG)\\n\\n\\"Out of these, my favorite is[FAVORITE]\\" where FAVORITE in songs\\n```\\n```promptdown\\nA list of awesome Dua Lipa songs:⏎\\n- New Rules\\n- [SONG|Don\'t Start Now]\\n- [SONG|IDGAF]\\n- [SONG|Be the One]\\n- [SONG|Blow Your Mind (Mwah)]\\nOut of these, my favorite is [FAVORITE|Don\'t Start Now]\\n```\\n\\nNote also how in this example LMQL code now reads much more like standard Python code, without any additional level of indentation. \\n\\n<br/>\\n\\n### `@lmql.query` functions\\n\\nThe overhauled syntax also makes LMQL much  easier on the eyes when used with the `@lmql.query` [function decorator in Python](/docs/lib/python.md):\\n\\n```python\\nimport lmql\\nimport json\\n\\n@lmql.query(model=\\"openai/text-curie-001\\", temperature=0.9)\\ndef summarize(): \\n    \'\'\'lmql\\n    \\"\\"\\"\\n    Provide a summary of Dua Lipa, the pop icon:\\n    {{\\n      \\"name\\": \\"[STRING_VALUE]\\",\\n      \\"chart_position\\": [INT_VALUE],\\n      \\"top_songs\\": [[\\n         \\"[STRING_VALUE]\\",\\n         \\"[STRING_VALUE]\\"\\n      ]]\\n    }}\\n    \\"\\"\\" where STOPS_BEFORE(STRING_VALUE, \'\\"\') and INT(INT_VALUE) and len(TOKENS(INT_VALUE)) < 3\\n    \\n    return json.loads(context.prompt.split(\\"pop icon:\\",1)[1])\\n    \'\'\'\\n\\nprint(summarize()) # {\'name\': \'Dua Lipa\', \'chart_position\': 3415, \'top_songs\': [\'New Rules\', \'Havana\']}\\n\\n```\\n\\n<br/>\\n\\n### `lmql.F` Lambda Functions\\n\\nBased on LMQL\'s new minimal syntax, we introduce a novel and concise way to write LLM-based lambda functions. This offers a lightweight entryway to get started with integrated small LLM-based utilities in your code, without having to write a full LMQL program.\\n\\n```python\\nimport lmql\\n\\nsummarize = lmql.F(\\"Summarize the following in a few words: {data}: [SUMMARY]\\")\\nmain_subject = lmql.F(\\"What is the main subject (noun) of the following text? {data}: [SUBJECT]\\", \\n                      \\"len(TOKENS(SUBJECT)) < 20\\")\\n\\ntext = \\"In LMQL, users can specify high-level, logical constraints ...\\"\\n\\nsummarize(data=text) # LMQL enables high-level constraints to be enforced during text \\n                     # generation, simplifying multi-part prompting and integration.\\nmain_subject(data=text) # Language Model Query Language (LMQL)\\n\\n```\\n\\n<br/>\\n<br/>\\n\\n## `llama.cpp` Inference Backend\\n\\nLMQL now also fully integrates with the excellent [llama.cpp](https://github.com/ggerganov/llama.cpp) C++ implementation of a number of Transformer-based language models. \\n\\nUsing `llama.cpp` from LMQL is as simple as specifying it in the `from` clause of a query:\\n\\n```{lmql}\\nname::llama-cpp-blog\\n\\nargmax \\"Say \'this is a test\':[RESPONSE]\\" from \\"llama.cpp:<PATH TO WEIGHTS>.bin\\"\\n```\\n\\nWe support, both, in-process loading of `llama.cpp`, as well as remote inference via `lmql serve-model`. To learn more about `llama.cpp` and how to use it with LMQL, check out the corresponding chapter in the LMQL [documentation](/docs/models/llama.cpp.md).\\n\\n<br/>\\n\\n## Other Changes\\n\\n* LMQL now includes a `random` model backend, which randomly samples tokens from the GPT-2 vocabulary. This is useful for debugging and testing purposes and can be used for data generation in the context of highly constrained query programs.\\n\\n* Two caching issues have been fixed, avoiding cache collisions which could lead to repeated model outputs.\\n\\n* More robust query string parsing, allowing for [robust escaping](/docs/language/scripted-prompting.md#escaping) of special characters `[`, `]`, `{` and `}`.\\n\\n* Added support for `transformers` based Llama models and the associated (fast) implementation of HF tokenizers.\\n\\n* Simplified Azure OpenAI support, see the relevant chapter in the [documentation](/docs/models/azure.md).\\n\\nWe thank community members [@minosvasilias](https://github.com/minosvasilias) and [@CircArgs](https://github.com/CircArgs) for their contribution to this release.","html":"<h1 id=\\"lmql-becomes-simpler-and-adds-llama-cpp\\" tabindex=\\"-1\\">LMQL becomes simpler and adds llama.cpp <a class=\\"header-anchor\\" href=\\"#lmql-becomes-simpler-and-adds-llama-cpp\\" aria-label=\\"Permalink to &quot;LMQL becomes simpler and adds llama.cpp&quot;\\">&ZeroWidthSpace;</a></h1>\\n<p><span class=\\"date\\">July 13, 2023</span></p>\\n<p>Today we are releasing LMQL 0.0.6.5. This update contains a major simplification of the LMQL syntax, moving it much closer to standard Python. It also includes a <code>llama.cpp</code> based inference backend, several bug fixes and other minor improvements.</p>\\n<p>You can try the latest version of LMQL in your browser at <a href=\\"https://lmql.ai/playground\\" target=\\"_blank\\" rel=\\"noreferrer\\">lmql.ai/playground</a> or install it via <code>pip install lmql</code>.</p>\\n<h2 id=\\"one-line-is-all-it-takes\\" tabindex=\\"-1\\">One Line Is All It Takes <a class=\\"header-anchor\\" href=\\"#one-line-is-all-it-takes\\" aria-label=\\"Permalink to &quot;One Line Is All It Takes&quot;\\">&ZeroWidthSpace;</a></h2>\\n<p>Most notably, 0.0.6.5 comes with several simplifications of the core syntax of LMQL. Of course, all changes are backwards compatible, so you can continue to use your existing query code and move to the new version without any changes.</p>\\n<p>With this, we aim to minimize syntactic overhead, employing sensible defaults to enable more concise programs like the following:</p>\\n<div class=\\"language- vp-adaptive-theme\\"><button title=\\"Copy Code\\" class=\\"copy\\"></button><span class=\\"lang\\"></span><pre class=\\"hljs\\"><code><span class=\\"line\\"><span class=\\"hljs-string\\">&quot;One line is all it takes <span class=\\"hljs-placeholder\\">[CONTINUATION]</span>&quot;</span>\\n</span></code></pre>\\n</div><div class=\\"language-promptdown vp-adaptive-theme\\"><button title=\\"Copy Code\\" class=\\"copy\\"></button><span class=\\"lang\\">promptdown</span><pre pd-text=\\"One line is all it takes [CONTINUATION|Fallin\' in love with me.]\\n\\" animate=\\"true\\" __animate=\\"true\\" animate-speed=\\"50\\" class=\\"promptdown promptdown-compiled\\" style=\\"opacity: 1;\\"><p pd-shadow-id=\\"2104\\" text=\\"O\\" pd-insertion-point=\\"true\\">One line is all it takes <span pd-shadow-id=\\"2106\\" pd-instant=\\"false\\" text=\\"\\" class=\\"promptdown-var color-pink\\"><span pd-shadow-id=\\"2107\\" text=\\"C\\" class=\\"promptdown-var-name\\">CONTINUATION</span>Fallin\' in love with me.</span>\\n</p></pre>\\n</div><p><strong>Sensible Defaults</strong> This is possible because LMQL now automatically assumes <code>argmax</code> and <code>openai/text-davinci-003</code> as (configurable) default model. If you prefer to use\\na different model or custom decoder settings, you can still specify them explicitly, e.g. in the <code>@lmql.query</code> decorator function as demonstrated later in this post.</p>\\n<p>Without any additional configuration, the simple query code above translates to a full LMQL program like this:</p>\\n<div class=\\"language- vp-adaptive-theme\\"><button title=\\"Copy Code\\" class=\\"copy\\"></button><span class=\\"lang\\"></span><pre class=\\"hljs\\"><code><span class=\\"line\\"><span class=\\"hljs-keyword\\">argmax</span> <span class=\\"hljs-string\\">&quot;One line is all it takes <span class=\\"hljs-placeholder\\">[CONTINUATION]</span>&quot;</span> <span class=\\"hljs-keyword\\">from</span> <span class=\\"hljs-string\\">&quot;openai/text-davinci-003&quot;</span>\\n</span></code></pre>\\n</div><br/>\\n<h3 id=\\"inline-constraints\\" tabindex=\\"-1\\">Inline Constraints <a class=\\"header-anchor\\" href=\\"#inline-constraints\\" aria-label=\\"Permalink to &quot;Inline Constraints&quot;\\">&ZeroWidthSpace;</a></h3>\\n<p>LMQL now allows you to specify several inline <code>where</code> constraints. This enables constraints that refer to local program variables, which means constraints can now be dependent on previous model outputs.</p>\\n<div class=\\"language- vp-adaptive-theme\\"><button title=\\"Copy Code\\" class=\\"copy\\"></button><span class=\\"lang\\"></span><pre class=\\"hljs\\"><code><span class=\\"line\\"><span class=\\"hljs-string\\">&quot;A list of awesome Dua Lipa songs:\\\\n&quot;</span>\\nsongs = []\\n\\n<span class=\\"hljs-string\\">&quot;- New Rules\\\\n&quot;</span>\\n<span class=\\"hljs-keyword\\">for</span> i <span class=\\"hljs-keyword\\">in</span> <span class=\\"hljs-built_in\\">range</span>(<span class=\\"hljs-number\\">4</span>):\\n    <span class=\\"hljs-string\\">&quot;-<span class=\\"hljs-placeholder\\">[SONG]</span>\\\\n&quot;</span> <span class=\\"hljs-keyword\\">where</span> STOPS_BEFORE(SONG, <span class=\\"hljs-string\\">&quot;\\\\n&quot;</span>)\\n    songs.append(SONG)\\n\\n<span class=\\"hljs-string\\">&quot;Out of these, my favorite is<span class=\\"hljs-placeholder\\">[FAVORITE]</span>&quot;</span> <span class=\\"hljs-keyword\\">where</span> FAVORITE <span class=\\"hljs-keyword\\">in</span> songs\\n</span></code></pre>\\n</div><div class=\\"language-promptdown vp-adaptive-theme\\"><button title=\\"Copy Code\\" class=\\"copy\\"></button><span class=\\"lang\\">promptdown</span><pre pd-text=\\"A list of awesome Dua Lipa songs:⏎\\n- New Rules\\n- [SONG|Don\'t Start Now]\\n- [SONG|IDGAF]\\n- [SONG|Be the One]\\n- [SONG|Blow Your Mind (Mwah)]\\nOut of these, my favorite is [FAVORITE|Don\'t Start Now]\\n\\" animate=\\"true\\" __animate=\\"true\\" animate-speed=\\"50\\" class=\\"promptdown promptdown-compiled\\" style=\\"opacity: 1;\\"><p pd-shadow-id=\\"2113\\" text=\\"A\\" pd-insertion-point=\\"true\\">A list of awesome Dua Lipa songs:⏎\\n- New Rules\\n- <span pd-shadow-id=\\"2115\\" pd-instant=\\"false\\" text=\\"\\" class=\\"promptdown-var color-pink\\"><span pd-shadow-id=\\"2116\\" text=\\"S\\" class=\\"promptdown-var-name\\">SONG</span>Don\'t Start Now</span>\\n- <span pd-shadow-id=\\"2121\\" pd-instant=\\"false\\" text=\\"\\" class=\\"promptdown-var color-pink\\"><span pd-shadow-id=\\"2122\\" text=\\"S\\" class=\\"promptdown-var-name\\">SONG</span>IDGAF</span>\\n- <span pd-shadow-id=\\"2127\\" pd-instant=\\"false\\" text=\\"\\" class=\\"promptdown-var color-pink\\"><span pd-shadow-id=\\"2128\\" text=\\"S\\" class=\\"promptdown-var-name\\">SONG</span>Be the One</span>\\n- <span pd-shadow-id=\\"2133\\" pd-instant=\\"false\\" text=\\"\\" class=\\"promptdown-var color-pink\\"><span pd-shadow-id=\\"2134\\" text=\\"S\\" class=\\"promptdown-var-name\\">SONG</span>Blow Your Mind (Mwah)</span>\\nOut of these, my favorite is <span pd-shadow-id=\\"2139\\" pd-instant=\\"false\\" text=\\"\\" class=\\"promptdown-var color-ochre\\"><span pd-shadow-id=\\"2140\\" text=\\"F\\" class=\\"promptdown-var-name\\">FAVORITE</span>Don\'t Start Now</span>\\n</p></pre>\\n</div><p>Note also how in this example LMQL code now reads much more like standard Python code, without any additional level of indentation.</p>\\n<br/>\\n<h3 id=\\"lmql-query-functions\\" tabindex=\\"-1\\"><code>@lmql.query</code> functions <a class=\\"header-anchor\\" href=\\"#lmql-query-functions\\" aria-label=\\"Permalink to &quot;`@lmql.query` functions&quot;\\">&ZeroWidthSpace;</a></h3>\\n<p>The overhauled syntax also makes LMQL much  easier on the eyes when used with the <code>@lmql.query</code> <a href=\\"/docs/lib/python.html\\">function decorator in Python</a>:</p>\\n<div class=\\"language-python vp-adaptive-theme\\"><button title=\\"Copy Code\\" class=\\"copy\\"></button><span class=\\"lang\\">python</span><pre class=\\"hljs\\"><code><span class=\\"line\\"><span class=\\"hljs-keyword\\">import</span> lmql\\n<span class=\\"hljs-keyword\\">import</span> json\\n\\n<span class=\\"hljs-meta\\">@lmql.query(<span class=\\"hljs-params\\">model=<span class=\\"hljs-string\\">&quot;openai/text-curie-001&quot;</span>, temperature=<span class=\\"hljs-number\\">0.9</span></span>)</span>\\n<span class=\\"hljs-keyword\\">def</span> <span class=\\"hljs-title function_\\">summarize</span>(): \\n    <span class=\\"hljs-inline-lmql\\"><span style=\'opacity: 0.4\'>&#x27;&#x27;&#x27;lmql</span>\\n    <span class=\\"hljs-string\\">&quot;&quot;&quot;\\n    Provide a summary of Dua Lipa, the pop icon:\\n    <span v-pre>{{</span>\\n      &quot;name&quot;: &quot;<span class=\\"hljs-placeholder\\">[STRING_VALUE]</span>&quot;,\\n      &quot;chart_position&quot;: <span class=\\"hljs-placeholder\\">[INT_VALUE]</span>,\\n      &quot;top_songs&quot;: <span class=\\"hljs-placeholder\\">[[\\n         <span class=\\"hljs-string\\">&quot;[STRING_VALUE]&quot;</span>,\\n         <span class=\\"hljs-string\\">&quot;[STRING_VALUE]&quot;</span>\\n      ]</span>]\\n    }}\\n    &quot;&quot;&quot;</span> <span class=\\"hljs-keyword\\">where</span> STOPS_BEFORE(STRING_VALUE, <span class=\\"hljs-string\\">&#x27;&quot;&#x27;</span>) <span class=\\"hljs-keyword\\">and</span> INT(INT_VALUE) <span class=\\"hljs-keyword\\">and</span> <span class=\\"hljs-built_in\\">len</span>(TOKENS(INT_VALUE)) &lt; 3\\n    \\n    <span class=\\"hljs-keyword\\">return</span> json.loads(context.prompt.split(<span class=\\"hljs-string\\">&quot;pop icon:&quot;</span>,1)[1])\\n    &#x27;&#x27;&#x27;</span>\\n\\n<span class=\\"hljs-built_in\\">print</span>(summarize()) <span class=\\"hljs-comment\\"># {&#x27;name&#x27;: &#x27;Dua Lipa&#x27;, &#x27;chart_position&#x27;: 3415, &#x27;top_songs&#x27;: [&#x27;New Rules&#x27;, &#x27;Havana&#x27;]}</span>\\n\\n</span></code></pre>\\n</div><br/>\\n<h3 id=\\"lmql-f-lambda-functions\\" tabindex=\\"-1\\"><code>lmql.F</code> Lambda Functions <a class=\\"header-anchor\\" href=\\"#lmql-f-lambda-functions\\" aria-label=\\"Permalink to &quot;`lmql.F` Lambda Functions&quot;\\">&ZeroWidthSpace;</a></h3>\\n<p>Based on LMQL\'s new minimal syntax, we introduce a novel and concise way to write LLM-based lambda functions. This offers a lightweight entryway to get started with integrated small LLM-based utilities in your code, without having to write a full LMQL program.</p>\\n<div class=\\"language-python vp-adaptive-theme\\"><button title=\\"Copy Code\\" class=\\"copy\\"></button><span class=\\"lang\\">python</span><pre class=\\"hljs\\"><code><span class=\\"line\\"><span class=\\"hljs-keyword\\">import</span> lmql\\n\\nsummarize = lmql.F(<span class=\\"hljs-string\\">&quot;Summarize the following in a few words: <span class=\\"hljs-subst\\">{data}</span>: <span class=\\"hljs-placeholder\\">[SUMMARY]</span>&quot;</span>)\\nmain_subject = lmql.F(<span class=\\"hljs-string\\">&quot;What is the main subject (noun) of the following text? <span class=\\"hljs-subst\\">{data}</span>: <span class=\\"hljs-placeholder\\">[SUBJECT]</span>&quot;</span>, \\n                      <span class=\\"hljs-string\\">&quot;len(TOKENS(SUBJECT)) &lt; 20&quot;</span>)\\n\\ntext = <span class=\\"hljs-string\\">&quot;In LMQL, users can specify high-level, logical constraints ...&quot;</span>\\n\\nsummarize(data=text) <span class=\\"hljs-comment\\"># LMQL enables high-level constraints to be enforced during text </span>\\n                     <span class=\\"hljs-comment\\"># generation, simplifying multi-part prompting and integration.</span>\\nmain_subject(data=text) <span class=\\"hljs-comment\\"># Language Model Query Language (LMQL)</span>\\n\\n</span></code></pre>\\n</div><br/>\\n<br/>\\n<h2 id=\\"llama-cpp-inference-backend\\" tabindex=\\"-1\\"><code>llama.cpp</code> Inference Backend <a class=\\"header-anchor\\" href=\\"#llama-cpp-inference-backend\\" aria-label=\\"Permalink to &quot;`llama.cpp` Inference Backend&quot;\\">&ZeroWidthSpace;</a></h2>\\n<p>LMQL now also fully integrates with the excellent <a href=\\"https://github.com/ggerganov/llama.cpp\\" target=\\"_blank\\" rel=\\"noreferrer\\">llama.cpp</a> C++ implementation of a number of Transformer-based language models.</p>\\n<p>Using <code>llama.cpp</code> from LMQL is as simple as specifying it in the <code>from</code> clause of a query:</p>\\n<div class=\\"language- vp-adaptive-theme\\"><button title=\\"Copy Code\\" class=\\"copy\\"></button><span class=\\"lang\\"></span><pre class=\\"hljs\\"><code><span class=\\"line\\"><span class=\\"hljs-keyword\\">argmax</span> <span class=\\"hljs-string\\">&quot;Say &#x27;this is a test&#x27;:<span class=\\"hljs-placeholder\\">[RESPONSE]</span>&quot;</span> <span class=\\"hljs-keyword\\">from</span> <span class=\\"hljs-string\\">&quot;llama.cpp:&lt;PATH TO WEIGHTS&gt;.bin&quot;</span>\\n</span></code></pre>\\n</div><p>We support, both, in-process loading of <code>llama.cpp</code>, as well as remote inference via <code>lmql serve-model</code>. To learn more about <code>llama.cpp</code> and how to use it with LMQL, check out the corresponding chapter in the LMQL <a href=\\"/docs/models/llama.cpp.html\\">documentation</a>.</p>\\n<br/>\\n<h2 id=\\"other-changes\\" tabindex=\\"-1\\">Other Changes <a class=\\"header-anchor\\" href=\\"#other-changes\\" aria-label=\\"Permalink to &quot;Other Changes&quot;\\">&ZeroWidthSpace;</a></h2>\\n<ul>\\n<li>\\n<p>LMQL now includes a <code>random</code> model backend, which randomly samples tokens from the GPT-2 vocabulary. This is useful for debugging and testing purposes and can be used for data generation in the context of highly constrained query programs.</p>\\n</li>\\n<li>\\n<p>Two caching issues have been fixed, avoiding cache collisions which could lead to repeated model outputs.</p>\\n</li>\\n<li>\\n<p>More robust query string parsing, allowing for <a href=\\"/docs/language/scripted-prompting.html#escaping\\">robust escaping</a> of special characters <code>[</code>, <code>]</code>, <code>{</code> and <code>}</code>.</p>\\n</li>\\n<li>\\n<p>Added support for <code>transformers</code> based Llama models and the associated (fast) implementation of HF tokenizers.</p>\\n</li>\\n<li>\\n<p>Simplified Azure OpenAI support, see the relevant chapter in the <a href=\\"/docs/models/azure.html\\">documentation</a>.</p>\\n</li>\\n</ul>\\n<p>We thank community members <a href=\\"https://github.com/minosvasilias\\" target=\\"_blank\\" rel=\\"noreferrer\\">@minosvasilias</a> and <a href=\\"https://github.com/CircArgs\\" target=\\"_blank\\" rel=\\"noreferrer\\">@CircArgs</a> for their contribution to this release.</p>\\n","frontmatter":{"date":"2023-07-13T00:00:00.000Z","title":"LMQL becomes simpler and adds llama.cpp"},"excerpt":"","url":"/blog/posts/release-0.0.6.5.html"},{"src":"---\\ndate: 2023-06-08\\ntitle: Releasing LMQL v0.0.6.4 LMTP, Azure, Synchronous API, and more\\n---\\n\\n# Releasing LMQL 0.0.6.4: LMTP, Azure, Synchronous API, and more\\n\\n<span class=\\"date\\">June 8, 2023</span>\\n\\nAmong many things, this update contains several bug fixes and improvements. The most notable changes are:\\n\\n* **Azure OpenAI support** LMQL now supports OpenAI models that are served via Azure. For more information on how to use Azure models, please see the corresponding chapter in the [documentation](/docs/models/azure.md). Many thanks to [@veqtor](https://github.com/veqtor) for contributing this feature.\\n\\n* **Local Models via the Language Model Transport Protocol** LMQL 0.0.6.4 implements a novel protocol to stream token output from local models, vastly improving performance. In our first benchmarks, we observed a 5-6x speedup for local model inference. For more information on how to use local models, please see the corresponding chapter in the [documentation](/docs/models/hf.md).\\n\\n   To learn more about the internals of the new streaming protocol, i.e. the language model transport protocol (LMTP), you can find more details in [this README file](https://github.com/eth-sri/lmql/blob/main/src/lmql/models/lmtp/README.md). In the future, we intend to implement more model backends using LMTP, streamlining communication between LMQL and models.\\n\\n   <div style=\\"text-align:center\\">\\n    <img src=\\"https://docs.lmql.ai/en/stable/_images/inference.svg\\" width=\\"80%\\" />\\n    <br>\\n    <i>LMQL\'s new streaming protocol (LMTP) allows for faster local model inference.</i>\\n   </div>\\n\\n* **Synchronous Python API** Next to an `async/await` based API, LMQL now also provides a synchronous API. This means you no longer need to use `asyncio` to use LMQL from Python. \\n\\n    To use the synchronous API, simply declare `@lmql.query` function without the `async` keyword, e.g.\\n\\n    ```python\\n    import lmql\\n\\n    @lmql.query\\n    def hello(s: str):\\n        \'\'\'lmql\\n        argmax \\n            \\"Hello {s} [RESPONSE]\\" \\n            return RESPONSE\\n        from \\n            \\"chatgpt\\"\\n        \'\'\'\\n\\n    print(hello(\\"world\\")) # [\'Hello! How can I assist you today?\']\\n    ```\\n\\n    If you instead want to use `lmql.run` in a synchronous context, you can now use `lmql.run_sync` instead. To learn more about how LMQL can be used from Python, check out our [documentation](/docs/lib/python.md).\\n\\n* **Improved Tokenizer Backends** LMQL can now use the excellent [`tiktoken` tokenizer](https://github.com/openai/tiktoken) as tokenization backend (for OpenAI models). Furthermore, all tokenization backends have been ported to operate on a byte-level, which improves support for multibyte characters and emojis. This is especially relevant for non-English languages and special characters.\\n\\n* **Docker Image** LMQL now provides a Docker image that can be used to run the LMQL playground in a containerized environment. For more information, please see the [documentation](/docs/development/docker-setup.md). Many thanks to [@SilacciA](https://github.com/SilacciA) for contributing this feature.\\n\\n* **Faster Startup Time** We optimized LMQL\'s import hierarchy, which results in faster module loading time.","html":"<h1 id=\\"releasing-lmql-0-0-6-4-lmtp-azure-synchronous-api-and-more\\" tabindex=\\"-1\\">Releasing LMQL 0.0.6.4: LMTP, Azure, Synchronous API, and more <a class=\\"header-anchor\\" href=\\"#releasing-lmql-0-0-6-4-lmtp-azure-synchronous-api-and-more\\" aria-label=\\"Permalink to &quot;Releasing LMQL 0.0.6.4: LMTP, Azure, Synchronous API, and more&quot;\\">&ZeroWidthSpace;</a></h1>\\n<p><span class=\\"date\\">June 8, 2023</span></p>\\n<p>Among many things, this update contains several bug fixes and improvements. The most notable changes are:</p>\\n<ul>\\n<li>\\n<p><strong>Azure OpenAI support</strong> LMQL now supports OpenAI models that are served via Azure. For more information on how to use Azure models, please see the corresponding chapter in the <a href=\\"/docs/models/azure.html\\">documentation</a>. Many thanks to <a href=\\"https://github.com/veqtor\\" target=\\"_blank\\" rel=\\"noreferrer\\">@veqtor</a> for contributing this feature.</p>\\n</li>\\n<li>\\n<p><strong>Local Models via the Language Model Transport Protocol</strong> LMQL 0.0.6.4 implements a novel protocol to stream token output from local models, vastly improving performance. In our first benchmarks, we observed a 5-6x speedup for local model inference. For more information on how to use local models, please see the corresponding chapter in the <a href=\\"/docs/models/hf.html\\">documentation</a>.</p>\\n<p>To learn more about the internals of the new streaming protocol, i.e. the language model transport protocol (LMTP), you can find more details in <a href=\\"https://github.com/eth-sri/lmql/blob/main/src/lmql/models/lmtp/README.md\\" target=\\"_blank\\" rel=\\"noreferrer\\">this README file</a>. In the future, we intend to implement more model backends using LMTP, streamlining communication between LMQL and models.</p>\\n <div style=\\"text-align:center\\">\\n  <img src=\\"https://docs.lmql.ai/en/stable/_images/inference.svg\\" width=\\"80%\\" />\\n  <br>\\n  <i>LMQL\'s new streaming protocol (LMTP) allows for faster local model inference.</i>\\n </div>\\n</li>\\n<li>\\n<p><strong>Synchronous Python API</strong> Next to an <code>async/await</code> based API, LMQL now also provides a synchronous API. This means you no longer need to use <code>asyncio</code> to use LMQL from Python.</p>\\n<p>To use the synchronous API, simply declare <code>@lmql.query</code> function without the <code>async</code> keyword, e.g.</p>\\n<div class=\\"language-python vp-adaptive-theme\\"><button title=\\"Copy Code\\" class=\\"copy\\"></button><span class=\\"lang\\">python</span><pre class=\\"hljs\\"><code><span class=\\"line\\"><span class=\\"hljs-keyword\\">import</span> lmql\\n\\n<span class=\\"hljs-meta\\">@lmql.query</span>\\n<span class=\\"hljs-keyword\\">def</span> <span class=\\"hljs-title function_\\">hello</span>(<span class=\\"hljs-params\\">s: <span class=\\"hljs-built_in\\">str</span></span>):\\n    <span class=\\"hljs-inline-lmql\\"><span style=\'opacity: 0.4\'>&#x27;&#x27;&#x27;lmql</span>\\n    <span class=\\"hljs-keyword\\">argmax</span> \\n        <span class=\\"hljs-string\\">&quot;Hello <span class=\\"hljs-subst\\">{s}</span> <span class=\\"hljs-placeholder\\">[RESPONSE]</span>&quot;</span> \\n        <span class=\\"hljs-keyword\\">return</span> RESPONSE\\n    <span class=\\"hljs-keyword\\">from</span> \\n        <span class=\\"hljs-string\\">&quot;chatgpt&quot;</span>\\n    &#x27;&#x27;&#x27;</span>\\n\\n<span class=\\"hljs-built_in\\">print</span>(hello(<span class=\\"hljs-string\\">&quot;world&quot;</span>)) <span class=\\"hljs-comment\\"># [&#x27;Hello! How can I assist you today?&#x27;]</span>\\n</span></code></pre>\\n</div><p>If you instead want to use <code>lmql.run</code> in a synchronous context, you can now use <code>lmql.run_sync</code> instead. To learn more about how LMQL can be used from Python, check out our <a href=\\"/docs/lib/python.html\\">documentation</a>.</p>\\n</li>\\n<li>\\n<p><strong>Improved Tokenizer Backends</strong> LMQL can now use the excellent <a href=\\"https://github.com/openai/tiktoken\\" target=\\"_blank\\" rel=\\"noreferrer\\"><code>tiktoken</code> tokenizer</a> as tokenization backend (for OpenAI models). Furthermore, all tokenization backends have been ported to operate on a byte-level, which improves support for multibyte characters and emojis. This is especially relevant for non-English languages and special characters.</p>\\n</li>\\n<li>\\n<p><strong>Docker Image</strong> LMQL now provides a Docker image that can be used to run the LMQL playground in a containerized environment. For more information, please see the <a href=\\"/docs/development/docker-setup.html\\">documentation</a>. Many thanks to <a href=\\"https://github.com/SilacciA\\" target=\\"_blank\\" rel=\\"noreferrer\\">@SilacciA</a> for contributing this feature.</p>\\n</li>\\n<li>\\n<p><strong>Faster Startup Time</strong> We optimized LMQL\'s import hierarchy, which results in faster module loading time.</p>\\n</li>\\n</ul>\\n","frontmatter":{"date":"2023-06-08T00:00:00.000Z","title":"Releasing LMQL v0.0.6.4 LMTP, Azure, Synchronous API, and more"},"excerpt":"","url":"/blog/posts/release-0.0.6.4.html"},{"src":"---\\ndate: 2023-05-11\\ntitle: LMQL Release v0.0.6.3\\n---\\n\\n# LMQL v0.0.6.3\\n\\n<span class=\\"date\\">May 11, 2023</span>\\n\\nToday, we are releasing LMQL v0.0.6.3. This update contains several bug fixes and improvements. The most notable changes are:\\n\\n* **Lighter Runtime** As part of our continued efforts, we made LMQL much lighter (no more mandatory `transformers` dependency). By default LMQL now no longer requires `transformers` or PyTorch. If you rely on local models, just install LMQL via `pip install lmql[hf]` to get full Transformers integration.\\n\\n* **Token Constraints** A new function `TOKENS(...)` was added to the LMQL constraint language, allowing you to specify lower and upper bounds or the exact number of tokens to generate for a given variable.\\n    \\n    ```{lmql}\\n    name::token_constraints\\n    argmax \\n        \\"A 10 token response[WHO]\\" \\n    from \\n        \\"openai/text-ada-001\\" \\n    where \\n        len(TOKENS(WHO)) == 10\\n    ```\\n\\n* **Conditional Stopping** `STOPS_AT` can now be combined with additional side conditions. This allows you to specify stopping phrases that are only enforced, once other conditions are met. \\n\\n    For example, below, we stop when the generated text hits a newline character, but only if the overall variable output is already at least 10 tokens long.\\n\\n    ```{lmql}\\n    name::conditional_stopping \\n    argmax \\n        \\"Hello[WHO]\\" \\n    from \\n        \\"openai/text-ada-001\\" \\n    where \\n        len(TOKENS(WHO)) > 10 and STOPS_AT(WHO, \\"\\\\n\\")\\n    ```\\n\\n* **lmql.run**: Improved input validation for `lmql.run` as contributed by <a href=\\"https://twitter.com/lfegray\\" target=\\"_blank\\">@lfegray</a>. More specifically, `lmql.run` wil now provide more helpful error messages when client logic does not specify input values for all required query parameters.\\n\\n* **Automatic Cache Invalidation**: LMQL\'s tokenizer cache at `~/.cache/lmql` is now invalidated automatically when upgrading to a new version. This should prevent issues with outdated cache files.\\n\\n> Note: Version 0.0.6.2 was skipped and yanked from pypi.org, as an invalid release was pushed accidentally.","html":"<h1 id=\\"lmql-v0-0-6-3\\" tabindex=\\"-1\\">LMQL v0.0.6.3 <a class=\\"header-anchor\\" href=\\"#lmql-v0-0-6-3\\" aria-label=\\"Permalink to &quot;LMQL v0.0.6.3&quot;\\">&ZeroWidthSpace;</a></h1>\\n<p><span class=\\"date\\">May 11, 2023</span></p>\\n<p>Today, we are releasing LMQL v0.0.6.3. This update contains several bug fixes and improvements. The most notable changes are:</p>\\n<ul>\\n<li>\\n<p><strong>Lighter Runtime</strong> As part of our continued efforts, we made LMQL much lighter (no more mandatory <code>transformers</code> dependency). By default LMQL now no longer requires <code>transformers</code> or PyTorch. If you rely on local models, just install LMQL via <code>pip install lmql[hf]</code> to get full Transformers integration.</p>\\n</li>\\n<li>\\n<p><strong>Token Constraints</strong> A new function <code>TOKENS(...)</code> was added to the LMQL constraint language, allowing you to specify lower and upper bounds or the exact number of tokens to generate for a given variable.</p>\\n<div class=\\"language- vp-adaptive-theme\\"><button title=\\"Copy Code\\" class=\\"copy\\"></button><span class=\\"lang\\"></span><pre class=\\"hljs\\"><code><span class=\\"line\\"><span class=\\"hljs-keyword\\">argmax</span> \\n    <span class=\\"hljs-string\\">&quot;A 10 token response<span class=\\"hljs-placeholder\\">[WHO]</span>&quot;</span> \\n<span class=\\"hljs-keyword\\">from</span> \\n    <span class=\\"hljs-string\\">&quot;openai/text-ada-001&quot;</span> \\n<span class=\\"hljs-keyword\\">where</span> \\n    <span class=\\"hljs-built_in\\">len</span>(TOKENS(WHO)) == <span class=\\"hljs-number\\">10</span>\\n</span></code></pre>\\n</div></li>\\n<li>\\n<p><strong>Conditional Stopping</strong> <code>STOPS_AT</code> can now be combined with additional side conditions. This allows you to specify stopping phrases that are only enforced, once other conditions are met.</p>\\n<p>For example, below, we stop when the generated text hits a newline character, but only if the overall variable output is already at least 10 tokens long.</p>\\n<div class=\\"language- vp-adaptive-theme\\"><button title=\\"Copy Code\\" class=\\"copy\\"></button><span class=\\"lang\\"></span><pre class=\\"hljs\\"><code><span class=\\"line\\"><span class=\\"hljs-keyword\\">argmax</span> \\n    <span class=\\"hljs-string\\">&quot;Hello<span class=\\"hljs-placeholder\\">[WHO]</span>&quot;</span> \\n<span class=\\"hljs-keyword\\">from</span> \\n    <span class=\\"hljs-string\\">&quot;openai/text-ada-001&quot;</span> \\n<span class=\\"hljs-keyword\\">where</span> \\n    <span class=\\"hljs-built_in\\">len</span>(TOKENS(WHO)) &gt; <span class=\\"hljs-number\\">10</span> <span class=\\"hljs-keyword\\">and</span> STOPS_AT(WHO, <span class=\\"hljs-string\\">&quot;\\\\n&quot;</span>)\\n</span></code></pre>\\n</div></li>\\n<li>\\n<p><strong>lmql.run</strong>: Improved input validation for <code>lmql.run</code> as contributed by <a href=\\"https://twitter.com/lfegray\\" target=\\"_blank\\">@lfegray</a>. More specifically, <code>lmql.run</code> wil now provide more helpful error messages when client logic does not specify input values for all required query parameters.</p>\\n</li>\\n<li>\\n<p><strong>Automatic Cache Invalidation</strong>: LMQL\'s tokenizer cache at <code>~/.cache/lmql</code> is now invalidated automatically when upgrading to a new version. This should prevent issues with outdated cache files.</p>\\n</li>\\n</ul>\\n<blockquote>\\n<p>Note: Version 0.0.6.2 was skipped and yanked from pypi.org, as an invalid release was pushed accidentally.</p>\\n</blockquote>\\n","frontmatter":{"date":"2023-05-11T00:00:00.000Z","title":"LMQL Release v0.0.6.3"},"excerpt":"","url":"/blog/posts/release-0.0.6.3.html"},{"src":"---\\ndate: 2023-05-03\\ntitle: LMQL Release v0.0.6.1\\n---\\n\\n# LMQL v0.0.6.1\\n\\n<span class=\\"date\\">May 3, 2023</span>\\n\\nWe released LMQL v0.0.6.1, which contains several bug fixes and improvements. The most notable changes are:\\n\\n* **Cache Layer Bug Fixes** This release contains several fixes and improvements to the recently introduced cache layer.\\n\\n* **Stopping Phrases** Stopping phrases specified via `STOPS_BEFORE` are now passed to the OpenAI API as `\\"stop\\"` parameter, decreasing the number of tokens used for the request. If you want to disable this (e.g. to allow speculative execution), you can specify the new decoder parameter `openai_nonstop=True`.\\n\\n* **Asynchronous Output Writers** All output writers have been refactored to use asynchronous I/O. This should simplify integration with other asynchronous frameworks, e.g. for HTTP or Websocket APIs. We also added a new chapter on [Output Streaming](/docs/lib/output.md) to the documentation.\\n\\n* **Output Writers for HTTP endpoints, WebSockets and Server-Sent Events** Based on the updated output writer interface, we added three new output writers for serving LMQL queries as HTTP endpoints, WebSockets and via Server-Sent Events (SSE). To learn more, check their relatively simple implementations in the new [lmql.output](https://github.com/eth-sri/lmql/tree/main/src/lmql/output) module. We will also provide more documentation on how to use them, e.g. with `aiohttp` in the future.","html":"<h1 id=\\"lmql-v0-0-6-1\\" tabindex=\\"-1\\">LMQL v0.0.6.1 <a class=\\"header-anchor\\" href=\\"#lmql-v0-0-6-1\\" aria-label=\\"Permalink to &quot;LMQL v0.0.6.1&quot;\\">&ZeroWidthSpace;</a></h1>\\n<p><span class=\\"date\\">May 3, 2023</span></p>\\n<p>We released LMQL v0.0.6.1, which contains several bug fixes and improvements. The most notable changes are:</p>\\n<ul>\\n<li>\\n<p><strong>Cache Layer Bug Fixes</strong> This release contains several fixes and improvements to the recently introduced cache layer.</p>\\n</li>\\n<li>\\n<p><strong>Stopping Phrases</strong> Stopping phrases specified via <code>STOPS_BEFORE</code> are now passed to the OpenAI API as <code>&quot;stop&quot;</code> parameter, decreasing the number of tokens used for the request. If you want to disable this (e.g. to allow speculative execution), you can specify the new decoder parameter <code>openai_nonstop=True</code>.</p>\\n</li>\\n<li>\\n<p><strong>Asynchronous Output Writers</strong> All output writers have been refactored to use asynchronous I/O. This should simplify integration with other asynchronous frameworks, e.g. for HTTP or Websocket APIs. We also added a new chapter on <a href=\\"/docs/lib/output.html\\">Output Streaming</a> to the documentation.</p>\\n</li>\\n<li>\\n<p><strong>Output Writers for HTTP endpoints, WebSockets and Server-Sent Events</strong> Based on the updated output writer interface, we added three new output writers for serving LMQL queries as HTTP endpoints, WebSockets and via Server-Sent Events (SSE). To learn more, check their relatively simple implementations in the new <a href=\\"https://github.com/eth-sri/lmql/tree/main/src/lmql/output\\" target=\\"_blank\\" rel=\\"noreferrer\\">lmql.output</a> module. We will also provide more documentation on how to use them, e.g. with <code>aiohttp</code> in the future.</p>\\n</li>\\n</ul>\\n","frontmatter":{"date":"2023-05-03T00:00:00.000Z","title":"LMQL Release v0.0.6.1"},"excerpt":"","url":"/blog/posts/release-0.0.6.1.html"},{"src":"---\\ndate: 2023-05-01\\ntitle: Releasing the LMQL Caching Layer (v0.0.6)\\n---\\n\\n# Releasing the LMQL Caching Layer (v0.0.6)\\n\\n<span class=\\"date\\">May 1, 2023</span>\\n\\nToday we are releasing LMQL 0.0.6, the first version of LMQL that integrates the *LMQL Caching Layer*. The caching layer can drastically reduce token use of LLM interaction, lowering both the cost and latency of running queries. In this blog post, we provide a quick overview of the caching layer and demonstrate how it can reduce token use, latency and the number of requests needed to run queries by up to 80%. We observe improvements across a wide range of different scenarios, including **template-based queries, long-form constraints and tool augmentation.**\\n\\nYou can experiment with LMQL in the browser-based [Playground IDE](http://lmql.ai/playground) or install the latest version locally, via `pip install lmql`.\\n\\n## Caching Layer\\n\\nThe caching layer is implemented as a **tree-based data structure** that caches all model output including logits, tokens, and metadata, allowing the runtime to more efficiently explore the token space of an LLM, even in the presence of multiple variables, constraints and tool augmentation. The cache can be considered an append-only tree, that is explored during query execution, expanding branches according to query code, constraints and speculative execution.\\n\\nTo illustrate the effect of a caching layer, we consider the following example scenarios, all of which now run in a fraction of the time and with a fraction of the tokens needed with traditional querying methods.\\n\\n### Template-Based Queries \\n\\nWhen specifying a prompt template with multiple variables to fill in, an LLM typically needs to be invoked once per variable. For instance, consider the following template that guides an LLM in generating a list of things:\\n```{lmql}\\nname::list-of-things-speculative\\nargmax\\n    \\"A list of things not to forget when going to the sea (not travelling): \\\\n\\"\\n    \\"- Sunglasses \\\\n\\"\\n    \\"-[THING]\\"\\n    \\"-[THING]\\"\\n    \\"-[THING]\\"\\n    \\"-[THING]\\"\\nfrom\\n    \'openai/text-ada-001\'\\nwhere\\n    STOPS_AT(THING, \\"\\\\n\\")\\n```\\n**Without Caching:** Tokens: 390, Requests: 4 | **With Caching Layer:** Tokens: 89 (<span style=\\"color: green\\">-77%</span>), Requests: 1 (<span style=\\"color: green\\">-75%</span>)\\n\\nHere, the LLM typically needs to be invoked 4 times, once per `[THING]` variable. On each call, this incurs a token and latency cost (both with OpenAI and local models). Separate calls are needed, because our template dictates the `-` token to be inserted before each `[THING]`. \\n\\nWith the caching layer, LMQL can now invoke the LLM only once, and fill in all variables with the resulting tokens, as long as the LLM output already aligns naturally with your template. In case the LLM result of the initial invocation at some point no longer aligns with the template, LMQL will automatically re-invoke the LLM from this point on, guaranteeing an overall consistent result that is already parsed into separate `[THING]` variables.\\n\\n### Short-Circuiting Long Constraints\\n\\nWhen you specify long constraints like `A in [\\"ABCDE\\", \\"FGHIJK\\"]`, the LMQL runtime guides the LLM to choose one of the provided options and then continues enforcing the sequence until the chosen values is fully decoded. To illustrate, consider the following query:\\n```{lmql}\\nname::long-form-constraints-speculative\\nargmax\\n    \\"If we have the choice we choose[OPTION]\\"\\nfrom \\n    \\"openai/text-ada-001\\"\\nwhere\\n    OPTION in [\\"Option A with a whole lot of extra context\\", \\n        \\"Option B with context\\", \\n        \\"Another Option, also with a lot of additional text\\"\\n    ]\\n```\\n```promptdown\\nIf we have the choice we choose [OPTION|Option A with a whole lot of extra context]\\n```\\n**Without Caching:** Tokens: 123, Requests: 9 | **With Caching Layer:** Tokens: 25 (<span style=\\"color: green\\">-80%</span>), Requests: 2 (<span style=\\"color: green\\">-78%</span>)\\n\\nHere, after the LLM has produced `\\"Option\\"` and then `\\" A\\"`, LMQL short-circuits further model calls and automatically completes the resulting sequence to `\\"Option A with a whole lot of extra context\\"`. This is possible because once `Option A` has been predicted, the remaining tokens are fully determined by the constraints.\\n\\n### Tool-Augmented Queries\\n\\nLastly, we consider tool augmented queries. LLM agents and tool augmentation are very powerful paradigms, that allow LLMs to incorporate external knowledge and reasoning into their predictions. However, this comes at a cost: On each tool invocation, the LLM needs to be re-invoked to continue decoding after the tool output has been inserted. This impacts both the token cost and latency of running queries, as many requests have to be send forth and back between the LLM and the tool.\\n\\nAs an example, consider the following query that augments an LLM with the ability to use a key-value storage, [also runnable in the browser-based LMQL Playground](http://lmql.ai/playground?snippet=kv).\\n\\n<center>\\n<a href=\\"http://lmql.ai/playground?snippet=kv\\">\\n    <img src=\\"https://user-images.githubusercontent.com/17903049/235436824-0150f73f-0ac6-4cd9-8cc9-d13343da54f0.png\\" alt=\\"Key-Storage Augmented LLM implemented in LMQL\\" style=\\"height:320pt;\\"/>\\n</a>\\n</center>\\n\\n**Without Caching:** Tokens: 5,162, Requests: 12 | **With Caching Layer:** Tokens: 3,481 (<span style=\\"color: green\\">-33%</span>), Requests: 8 (<span style=\\"color: green\\">-33%</span>)\\n\\nHere, whenever the LLM produces an action relating to our key-value storage, we invoke a tool that handles the storage and return the result (to `assign` and `get` stored values). The result of each tool invocation is then inserted into the LLM output, and the LLM is re-invoked to continue decoding.\\n\\nWe count 10 tool interactions which results in 12 requests if we run without caching. However, using the new caching layer, we can reduce this to 8 requests, even undercutting the number of tool interactions. This is possible because the caching layer will not abort LLM generation, if the LLM already correctly predicts the tool output. \\n\\nThis scenario demonstrates that the natural ability of LLMs to complete sequences can be leveraged to reduce the number of tool interactions, by relying on speculative execution.\\n\\n## Persisting the Cache\\n\\nOf course, the in-memory cache of the LMQL runtime can also be persisted to disk, allowing you to reuse the cache tree across multiple queries, automatically reducing token cost and latency. In some cases this can even be used to reduce the number of requests to the LLM to 0, e.g. if the cache already contains the desired result. \\n\\nTo do so, you can simply specify a `cache=\\"file.tokens\\"` parameter in your query code:\\n\\n```{lmql}\\nname::joke-with-cache\\nargmax(cache=\\"joke.tokens\\")\\n   \\"\\"\\"A good dad joke. A indicates the punchline\\n   Q:[JOKE]\\n   A:[PUNCHLINE]\\"\\"\\"\\nfrom\\n   \\"openai/text-davinci-003\\"\\nwhere\\n   len(JOKE) < 120 and \\n   STOPS_AT(JOKE, \\"?\\") and \\n   STOPS_AT(PUNCHLINE, \\"\\\\n\\") and \\n   len(PUNCHLINE) > 1\\n```\\n\\nThe first successful run of this query will persist the cache to `joke.tokens`. Subsequent runs will then automatically load the cache from disk, and only invoke the LLM if the cache does not contain a match. This also works for queries whose underlying LLM requests only partially overlap, as the tree-based cache data structure will automatically identify matching subtrees.\\n\\n**Caching During Query Development**: Persisting the cache can be particularly useful during query development, as it allows you to reuse the cache across multiple runs of the same query. A persistent cache will reduce token cost and latency of your query, even if you slightly change the query between runs.\\n\\n## Caveats and Disabling the Cache\\n\\nYou can disable the caching layer by specifying `cache=False` in your query code. This will cause the LMQL runtime to always invoke the LLM, and never use the cache. This is useful for debugging purposes, or if you want to ensure that the LLM is always invoked.\\n\\nFurther, as the cache currently is implemented as an append-only data structure, it will grow indefinitely. This may be problematic for long-running applications, as the cache will eventually grow to relatively large sizes. In the future, we plan to implement simple strategies to limit the cache size, such as a least-recently-used eviction policy.\\n\\n## Conclusion\\n\\nIn this post, we introduced the new caching layer of the LMQL runtime, which allows you to reduce the token cost and latency of your queries by reusing previously generated LLM outputs. We demonstrated how the caching layer can be used to reduce the number of LLM invocations in a variety of scenarios, including long constraints, short-circuiting, and tool-augmented queries. We also showed how the cache can be persisted to disk, allowing you to reuse the cache across multiple queries.\\n\\nTo learn more about LMQL please also check out our [documentation](/docs), or join our [Discord](https://discord.gg/2Y3Wz2Q) to chat with us directly. We are looking forward to hearing from you!","html":"<h1 id=\\"releasing-the-lmql-caching-layer-v0-0-6\\" tabindex=\\"-1\\">Releasing the LMQL Caching Layer (v0.0.6) <a class=\\"header-anchor\\" href=\\"#releasing-the-lmql-caching-layer-v0-0-6\\" aria-label=\\"Permalink to &quot;Releasing the LMQL Caching Layer (v0.0.6)&quot;\\">&ZeroWidthSpace;</a></h1>\\n<p><span class=\\"date\\">May 1, 2023</span></p>\\n<p>Today we are releasing LMQL 0.0.6, the first version of LMQL that integrates the <em>LMQL Caching Layer</em>. The caching layer can drastically reduce token use of LLM interaction, lowering both the cost and latency of running queries. In this blog post, we provide a quick overview of the caching layer and demonstrate how it can reduce token use, latency and the number of requests needed to run queries by up to 80%. We observe improvements across a wide range of different scenarios, including <strong>template-based queries, long-form constraints and tool augmentation.</strong></p>\\n<p>You can experiment with LMQL in the browser-based <a href=\\"http://lmql.ai/playground\\" target=\\"_blank\\" rel=\\"noreferrer\\">Playground IDE</a> or install the latest version locally, via <code>pip install lmql</code>.</p>\\n<h2 id=\\"caching-layer\\" tabindex=\\"-1\\">Caching Layer <a class=\\"header-anchor\\" href=\\"#caching-layer\\" aria-label=\\"Permalink to &quot;Caching Layer&quot;\\">&ZeroWidthSpace;</a></h2>\\n<p>The caching layer is implemented as a <strong>tree-based data structure</strong> that caches all model output including logits, tokens, and metadata, allowing the runtime to more efficiently explore the token space of an LLM, even in the presence of multiple variables, constraints and tool augmentation. The cache can be considered an append-only tree, that is explored during query execution, expanding branches according to query code, constraints and speculative execution.</p>\\n<p>To illustrate the effect of a caching layer, we consider the following example scenarios, all of which now run in a fraction of the time and with a fraction of the tokens needed with traditional querying methods.</p>\\n<h3 id=\\"template-based-queries\\" tabindex=\\"-1\\">Template-Based Queries <a class=\\"header-anchor\\" href=\\"#template-based-queries\\" aria-label=\\"Permalink to &quot;Template-Based Queries&quot;\\">&ZeroWidthSpace;</a></h3>\\n<p>When specifying a prompt template with multiple variables to fill in, an LLM typically needs to be invoked once per variable. For instance, consider the following template that guides an LLM in generating a list of things:</p>\\n<div class=\\"language- vp-adaptive-theme\\"><button title=\\"Copy Code\\" class=\\"copy\\"></button><span class=\\"lang\\"></span><pre class=\\"hljs\\"><code><span class=\\"line\\"><span class=\\"hljs-keyword\\">argmax</span>\\n    <span class=\\"hljs-string\\">&quot;A list of things not to forget when going to the sea (not travelling): \\\\n&quot;</span>\\n    <span class=\\"hljs-string\\">&quot;- Sunglasses \\\\n&quot;</span>\\n    <span class=\\"hljs-string\\">&quot;-<span class=\\"hljs-placeholder\\">[THING]</span>&quot;</span>\\n    <span class=\\"hljs-string\\">&quot;-<span class=\\"hljs-placeholder\\">[THING]</span>&quot;</span>\\n    <span class=\\"hljs-string\\">&quot;-<span class=\\"hljs-placeholder\\">[THING]</span>&quot;</span>\\n    <span class=\\"hljs-string\\">&quot;-<span class=\\"hljs-placeholder\\">[THING]</span>&quot;</span>\\n<span class=\\"hljs-keyword\\">from</span>\\n    <span class=\\"hljs-string\\">&#x27;openai/text-ada-001&#x27;</span>\\n<span class=\\"hljs-keyword\\">where</span>\\n    STOPS_AT(THING, <span class=\\"hljs-string\\">&quot;\\\\n&quot;</span>)\\n</span></code></pre>\\n</div><p><strong>Without Caching:</strong> Tokens: 390, Requests: 4 | <strong>With Caching Layer:</strong> Tokens: 89 (<span style=\\"color: green\\">-77%</span>), Requests: 1 (<span style=\\"color: green\\">-75%</span>)</p>\\n<p>Here, the LLM typically needs to be invoked 4 times, once per <code>[THING]</code> variable. On each call, this incurs a token and latency cost (both with OpenAI and local models). Separate calls are needed, because our template dictates the <code>-</code> token to be inserted before each <code>[THING]</code>.</p>\\n<p>With the caching layer, LMQL can now invoke the LLM only once, and fill in all variables with the resulting tokens, as long as the LLM output already aligns naturally with your template. In case the LLM result of the initial invocation at some point no longer aligns with the template, LMQL will automatically re-invoke the LLM from this point on, guaranteeing an overall consistent result that is already parsed into separate <code>[THING]</code> variables.</p>\\n<h3 id=\\"short-circuiting-long-constraints\\" tabindex=\\"-1\\">Short-Circuiting Long Constraints <a class=\\"header-anchor\\" href=\\"#short-circuiting-long-constraints\\" aria-label=\\"Permalink to &quot;Short-Circuiting Long Constraints&quot;\\">&ZeroWidthSpace;</a></h3>\\n<p>When you specify long constraints like <code>A in [&quot;ABCDE&quot;, &quot;FGHIJK&quot;]</code>, the LMQL runtime guides the LLM to choose one of the provided options and then continues enforcing the sequence until the chosen values is fully decoded. To illustrate, consider the following query:</p>\\n<div class=\\"language- vp-adaptive-theme\\"><button title=\\"Copy Code\\" class=\\"copy\\"></button><span class=\\"lang\\"></span><pre class=\\"hljs\\"><code><span class=\\"line\\"><span class=\\"hljs-keyword\\">argmax</span>\\n    <span class=\\"hljs-string\\">&quot;If we have the choice we choose<span class=\\"hljs-placeholder\\">[OPTION]</span>&quot;</span>\\n<span class=\\"hljs-keyword\\">from</span> \\n    <span class=\\"hljs-string\\">&quot;openai/text-ada-001&quot;</span>\\n<span class=\\"hljs-keyword\\">where</span>\\n    OPTION <span class=\\"hljs-keyword\\">in</span> [<span class=\\"hljs-string\\">&quot;Option A with a whole lot of extra context&quot;</span>, \\n        <span class=\\"hljs-string\\">&quot;Option B with context&quot;</span>, \\n        <span class=\\"hljs-string\\">&quot;Another Option, also with a lot of additional text&quot;</span>\\n    ]\\n</span></code></pre>\\n</div><div class=\\"language-promptdown vp-adaptive-theme\\"><button title=\\"Copy Code\\" class=\\"copy\\"></button><span class=\\"lang\\">promptdown</span><pre pd-text=\\"If we have the choice we choose [OPTION|Option A with a whole lot of extra context]\\n\\" animate=\\"true\\" __animate=\\"true\\" animate-speed=\\"50\\" class=\\"promptdown promptdown-compiled\\" style=\\"opacity: 1;\\"><p pd-shadow-id=\\"2146\\" text=\\"I\\" pd-insertion-point=\\"true\\">If we have the choice we choose <span pd-shadow-id=\\"2148\\" pd-instant=\\"false\\" text=\\"\\" class=\\"promptdown-var color-orange\\"><span pd-shadow-id=\\"2149\\" text=\\"O\\" class=\\"promptdown-var-name\\">OPTION</span>Option A with a whole lot of extra context</span>\\n</p></pre>\\n</div><p><strong>Without Caching:</strong> Tokens: 123, Requests: 9 | <strong>With Caching Layer:</strong> Tokens: 25 (<span style=\\"color: green\\">-80%</span>), Requests: 2 (<span style=\\"color: green\\">-78%</span>)</p>\\n<p>Here, after the LLM has produced <code>&quot;Option&quot;</code> and then <code>&quot; A&quot;</code>, LMQL short-circuits further model calls and automatically completes the resulting sequence to <code>&quot;Option A with a whole lot of extra context&quot;</code>. This is possible because once <code>Option A</code> has been predicted, the remaining tokens are fully determined by the constraints.</p>\\n<h3 id=\\"tool-augmented-queries\\" tabindex=\\"-1\\">Tool-Augmented Queries <a class=\\"header-anchor\\" href=\\"#tool-augmented-queries\\" aria-label=\\"Permalink to &quot;Tool-Augmented Queries&quot;\\">&ZeroWidthSpace;</a></h3>\\n<p>Lastly, we consider tool augmented queries. LLM agents and tool augmentation are very powerful paradigms, that allow LLMs to incorporate external knowledge and reasoning into their predictions. However, this comes at a cost: On each tool invocation, the LLM needs to be re-invoked to continue decoding after the tool output has been inserted. This impacts both the token cost and latency of running queries, as many requests have to be send forth and back between the LLM and the tool.</p>\\n<p>As an example, consider the following query that augments an LLM with the ability to use a key-value storage, <a href=\\"http://lmql.ai/playground?snippet=kv\\" target=\\"_blank\\" rel=\\"noreferrer\\">also runnable in the browser-based LMQL Playground</a>.</p>\\n<center>\\n<a href=\\"http://lmql.ai/playground?snippet=kv\\">\\n    <img src=\\"https://user-images.githubusercontent.com/17903049/235436824-0150f73f-0ac6-4cd9-8cc9-d13343da54f0.png\\" alt=\\"Key-Storage Augmented LLM implemented in LMQL\\" style=\\"height:320pt;\\"/>\\n</a>\\n</center>\\n<p><strong>Without Caching:</strong> Tokens: 5,162, Requests: 12 | <strong>With Caching Layer:</strong> Tokens: 3,481 (<span style=\\"color: green\\">-33%</span>), Requests: 8 (<span style=\\"color: green\\">-33%</span>)</p>\\n<p>Here, whenever the LLM produces an action relating to our key-value storage, we invoke a tool that handles the storage and return the result (to <code>assign</code> and <code>get</code> stored values). The result of each tool invocation is then inserted into the LLM output, and the LLM is re-invoked to continue decoding.</p>\\n<p>We count 10 tool interactions which results in 12 requests if we run without caching. However, using the new caching layer, we can reduce this to 8 requests, even undercutting the number of tool interactions. This is possible because the caching layer will not abort LLM generation, if the LLM already correctly predicts the tool output.</p>\\n<p>This scenario demonstrates that the natural ability of LLMs to complete sequences can be leveraged to reduce the number of tool interactions, by relying on speculative execution.</p>\\n<h2 id=\\"persisting-the-cache\\" tabindex=\\"-1\\">Persisting the Cache <a class=\\"header-anchor\\" href=\\"#persisting-the-cache\\" aria-label=\\"Permalink to &quot;Persisting the Cache&quot;\\">&ZeroWidthSpace;</a></h2>\\n<p>Of course, the in-memory cache of the LMQL runtime can also be persisted to disk, allowing you to reuse the cache tree across multiple queries, automatically reducing token cost and latency. In some cases this can even be used to reduce the number of requests to the LLM to 0, e.g. if the cache already contains the desired result.</p>\\n<p>To do so, you can simply specify a <code>cache=&quot;file.tokens&quot;</code> parameter in your query code:</p>\\n<div class=\\"language- vp-adaptive-theme\\"><button title=\\"Copy Code\\" class=\\"copy\\"></button><span class=\\"lang\\"></span><pre class=\\"hljs\\"><code><span class=\\"line\\"><span class=\\"hljs-keyword\\">argmax</span>(cache=<span class=\\"hljs-string\\">&quot;joke.tokens&quot;</span>)\\n   <span class=\\"hljs-string\\">&quot;&quot;&quot;A good dad joke. A indicates the punchline\\n   Q:<span class=\\"hljs-placeholder\\">[JOKE]</span>\\n   A:<span class=\\"hljs-placeholder\\">[PUNCHLINE]</span>&quot;&quot;&quot;</span>\\n<span class=\\"hljs-keyword\\">from</span>\\n   <span class=\\"hljs-string\\">&quot;openai/text-davinci-003&quot;</span>\\n<span class=\\"hljs-keyword\\">where</span>\\n   <span class=\\"hljs-built_in\\">len</span>(JOKE) &lt; <span class=\\"hljs-number\\">120</span> <span class=\\"hljs-keyword\\">and</span> \\n   STOPS_AT(JOKE, <span class=\\"hljs-string\\">&quot;?&quot;</span>) <span class=\\"hljs-keyword\\">and</span> \\n   STOPS_AT(PUNCHLINE, <span class=\\"hljs-string\\">&quot;\\\\n&quot;</span>) <span class=\\"hljs-keyword\\">and</span> \\n   <span class=\\"hljs-built_in\\">len</span>(PUNCHLINE) &gt; <span class=\\"hljs-number\\">1</span>\\n</span></code></pre>\\n</div><p>The first successful run of this query will persist the cache to <code>joke.tokens</code>. Subsequent runs will then automatically load the cache from disk, and only invoke the LLM if the cache does not contain a match. This also works for queries whose underlying LLM requests only partially overlap, as the tree-based cache data structure will automatically identify matching subtrees.</p>\\n<p><strong>Caching During Query Development</strong>: Persisting the cache can be particularly useful during query development, as it allows you to reuse the cache across multiple runs of the same query. A persistent cache will reduce token cost and latency of your query, even if you slightly change the query between runs.</p>\\n<h2 id=\\"caveats-and-disabling-the-cache\\" tabindex=\\"-1\\">Caveats and Disabling the Cache <a class=\\"header-anchor\\" href=\\"#caveats-and-disabling-the-cache\\" aria-label=\\"Permalink to &quot;Caveats and Disabling the Cache&quot;\\">&ZeroWidthSpace;</a></h2>\\n<p>You can disable the caching layer by specifying <code>cache=False</code> in your query code. This will cause the LMQL runtime to always invoke the LLM, and never use the cache. This is useful for debugging purposes, or if you want to ensure that the LLM is always invoked.</p>\\n<p>Further, as the cache currently is implemented as an append-only data structure, it will grow indefinitely. This may be problematic for long-running applications, as the cache will eventually grow to relatively large sizes. In the future, we plan to implement simple strategies to limit the cache size, such as a least-recently-used eviction policy.</p>\\n<h2 id=\\"conclusion\\" tabindex=\\"-1\\">Conclusion <a class=\\"header-anchor\\" href=\\"#conclusion\\" aria-label=\\"Permalink to &quot;Conclusion&quot;\\">&ZeroWidthSpace;</a></h2>\\n<p>In this post, we introduced the new caching layer of the LMQL runtime, which allows you to reduce the token cost and latency of your queries by reusing previously generated LLM outputs. We demonstrated how the caching layer can be used to reduce the number of LLM invocations in a variety of scenarios, including long constraints, short-circuiting, and tool-augmented queries. We also showed how the cache can be persisted to disk, allowing you to reuse the cache across multiple queries.</p>\\n<p>To learn more about LMQL please also check out our <a href=\\"/docs.html\\">documentation</a>, or join our <a href=\\"https://discord.gg/2Y3Wz2Q\\" target=\\"_blank\\" rel=\\"noreferrer\\">Discord</a> to chat with us directly. We are looking forward to hearing from you!</p>\\n","frontmatter":{"date":"2023-05-01T00:00:00.000Z","title":"Releasing the LMQL Caching Layer (v0.0.6)"},"excerpt":"","url":"/blog/posts/release-0.0.6.html"},{"src":"---\\ndate: 2023-04-17\\ntitle: LMQL Release 0.0.5\\n---\\n\\n# LMQL Release 0.0.5\\n\\n<span class=\\"date\\">April 17, 2023</span>\\n\\nToday we are releasing version 0.0.5 of LMQL. This release focuses on stability and performance improvements. For a detailed list of changes, please see below. We are particularly excited about the first community contributions that have been merged as part of this release, with many more in the works.\\n\\n`lmql==0.0.5` has been published on [PyPI](https://pypi.org/project/lmql/), based the current `main` branch of the [GitHub repository](https://github.com/eth-sri/lmql). The updated version has also been deployed to the browser-based [lmql.ai/playground](http://lmql.ai/playground).\\n\\n### Changelog\\n\\n* **Decoder Performance** The `argmax` and `sample` decoders have undergone some optimizations, allowing them to run faster. This results in a *20-30% speed-up* on common query workloads. [#24](https://github.com/eth-sri/lmql/pull/24).\\n\\n* **Postprocessing Semantics** Internally, LMQL now allows constraints to implement postprocessing semantics. This is used to convert variable values after they have been completed, to a more normalized form in the prompt, and to a semantically meaningful data type in the context of the query code. [#24](https://github.com/eth-sri/lmql/pull/24). \\n\\n   For example, when using an `INT(<var>)` constraint on a generated number, the model will be restricted to only generate valid integers, and now, the resulting `NUM` value will additionally be converted to an `int` value:\\n\\n   ```\\n   argmax\\n      \\"My favorite number is: [NUM]\\\\n\\"\\n      print(type(NUM), NUM * 2) # <class \'int\'> 4\\n      \\"Number times two is {NUM * 2}\\"\\n   from\\n      \'openai/text-ada-001\'\\n   where\\n      INT(NUM) \\n   ```\\n\\n* **Core Interpreter** A complete reimplementation of the LMQL core interpreter has been completed. This fixes a couple of minor issues and overall, improves reliability and performance when dealing with *branching* decoding algorithms. [#24](https://github.com/eth-sri/lmql/pull/24).\\n\\n\\n* **Playground** Locally and when used in-browser, the [LMQL Playground](http://lmql.ai/playground) now *streams debugger information* from the LMQL interpreter incrementally. This leads to speed-ups when running in the Playground, especially with longer outputs. [#27f9a8ad](https://github.com/eth-sri/lmql/commit/27f9a8adb819f732608ef61c9aca9dca579dc536).\\n\\n\\n* **Other Fixes**:\\n    - When used from within Python (as decorated function), LMQL code no longer has to be doubly-escaped, e.g. you can now write `STOPS_AT(VAR, \\"\\\\n\\")` instead of `STOPS_AT(VAR, \\"\\\\\\\\n\\")`\\n    - The LMQL inference API buffers requests that come in during startup, to avoid errors when the server is not yet ready. [#15](https://github.com/eth-sri/lmql/pull/15), thanks to [@chrispan](https://github.com/chrispan).\\n    - OpenAI request parallelization no longer leads to an error on Linux systems, with regards to worker processes [#6](https://github.com/eth-sri/lmql/issues/6).\\n\\n### Preview\\n\\nApart from the changes above, we are also working on a number of other features, including:\\n\\n* **llama.cpp support** as started in [this PR](https://github.com/eth-sri/lmql/pull/18), thanks to [@CircArgs](https://github.com/CircArgs).\\n* Support for **Type Constraints**, e.g.  `type(VAR) is DataClass`, that automatically force the model to produce a value that structurally conforms to the given type. See this [Twitter thread](https://twitter.com/lbeurerkellner/status/1646187597901733889) for more details.\\n* Support for using **Antlr parsers** during query execution, to force the model to produce a value that conforms to a given grammar. \\n\\n* **Extending Logit Masking to OpenAI Chat Models**. This will enable full support for LMQL constraints with e.g. `chatgpt` and `gpt-4` models. See [#25](https://github.com/eth-sri/lmql/pull/25), thanks to [@kharvd](https://github.com/kharvd).","html":"<h1 id=\\"lmql-release-0-0-5\\" tabindex=\\"-1\\">LMQL Release 0.0.5 <a class=\\"header-anchor\\" href=\\"#lmql-release-0-0-5\\" aria-label=\\"Permalink to &quot;LMQL Release 0.0.5&quot;\\">&ZeroWidthSpace;</a></h1>\\n<p><span class=\\"date\\">April 17, 2023</span></p>\\n<p>Today we are releasing version 0.0.5 of LMQL. This release focuses on stability and performance improvements. For a detailed list of changes, please see below. We are particularly excited about the first community contributions that have been merged as part of this release, with many more in the works.</p>\\n<p><code>lmql==0.0.5</code> has been published on <a href=\\"https://pypi.org/project/lmql/\\" target=\\"_blank\\" rel=\\"noreferrer\\">PyPI</a>, based the current <code>main</code> branch of the <a href=\\"https://github.com/eth-sri/lmql\\" target=\\"_blank\\" rel=\\"noreferrer\\">GitHub repository</a>. The updated version has also been deployed to the browser-based <a href=\\"http://lmql.ai/playground\\" target=\\"_blank\\" rel=\\"noreferrer\\">lmql.ai/playground</a>.</p>\\n<h3 id=\\"changelog\\" tabindex=\\"-1\\">Changelog <a class=\\"header-anchor\\" href=\\"#changelog\\" aria-label=\\"Permalink to &quot;Changelog&quot;\\">&ZeroWidthSpace;</a></h3>\\n<ul>\\n<li>\\n<p><strong>Decoder Performance</strong> The <code>argmax</code> and <code>sample</code> decoders have undergone some optimizations, allowing them to run faster. This results in a <em>20-30% speed-up</em> on common query workloads. <a href=\\"https://github.com/eth-sri/lmql/pull/24\\" target=\\"_blank\\" rel=\\"noreferrer\\">#24</a>.</p>\\n</li>\\n<li>\\n<p><strong>Postprocessing Semantics</strong> Internally, LMQL now allows constraints to implement postprocessing semantics. This is used to convert variable values after they have been completed, to a more normalized form in the prompt, and to a semantically meaningful data type in the context of the query code. <a href=\\"https://github.com/eth-sri/lmql/pull/24\\" target=\\"_blank\\" rel=\\"noreferrer\\">#24</a>.</p>\\n<p>For example, when using an <code>INT(&lt;var&gt;)</code> constraint on a generated number, the model will be restricted to only generate valid integers, and now, the resulting <code>NUM</code> value will additionally be converted to an <code>int</code> value:</p>\\n<div class=\\"language- vp-adaptive-theme\\"><button title=\\"Copy Code\\" class=\\"copy\\"></button><span class=\\"lang\\"></span><pre class=\\"hljs\\"><code><span class=\\"line\\"><span class=\\"hljs-keyword\\">argmax</span>\\n   <span class=\\"hljs-string\\">&quot;My favorite number is: <span class=\\"hljs-placeholder\\">[NUM]</span>\\\\n&quot;</span>\\n   <span class=\\"hljs-built_in\\">print</span>(<span class=\\"hljs-built_in\\">type</span>(NUM), NUM * <span class=\\"hljs-number\\">2</span>) <span class=\\"hljs-comment\\"># &lt;class &#x27;int&#x27;&gt; 4</span>\\n   <span class=\\"hljs-string\\">&quot;Number times two is <span class=\\"hljs-subst\\">{NUM * <span class=\\"hljs-number\\">2</span>}</span>&quot;</span>\\n<span class=\\"hljs-keyword\\">from</span>\\n   <span class=\\"hljs-string\\">&#x27;openai/text-ada-001&#x27;</span>\\n<span class=\\"hljs-keyword\\">where</span>\\n   INT(NUM) \\n</span></code></pre>\\n</div></li>\\n<li>\\n<p><strong>Core Interpreter</strong> A complete reimplementation of the LMQL core interpreter has been completed. This fixes a couple of minor issues and overall, improves reliability and performance when dealing with <em>branching</em> decoding algorithms. <a href=\\"https://github.com/eth-sri/lmql/pull/24\\" target=\\"_blank\\" rel=\\"noreferrer\\">#24</a>.</p>\\n</li>\\n<li>\\n<p><strong>Playground</strong> Locally and when used in-browser, the <a href=\\"http://lmql.ai/playground\\" target=\\"_blank\\" rel=\\"noreferrer\\">LMQL Playground</a> now <em>streams debugger information</em> from the LMQL interpreter incrementally. This leads to speed-ups when running in the Playground, especially with longer outputs. <a href=\\"https://github.com/eth-sri/lmql/commit/27f9a8adb819f732608ef61c9aca9dca579dc536\\" target=\\"_blank\\" rel=\\"noreferrer\\">#27f9a8ad</a>.</p>\\n</li>\\n<li>\\n<p><strong>Other Fixes</strong>:</p>\\n<ul>\\n<li>When used from within Python (as decorated function), LMQL code no longer has to be doubly-escaped, e.g. you can now write <code>STOPS_AT(VAR, &quot;\\\\n&quot;)</code> instead of <code>STOPS_AT(VAR, &quot;\\\\\\\\n&quot;)</code></li>\\n<li>The LMQL inference API buffers requests that come in during startup, to avoid errors when the server is not yet ready. <a href=\\"https://github.com/eth-sri/lmql/pull/15\\" target=\\"_blank\\" rel=\\"noreferrer\\">#15</a>, thanks to <a href=\\"https://github.com/chrispan\\" target=\\"_blank\\" rel=\\"noreferrer\\">@chrispan</a>.</li>\\n<li>OpenAI request parallelization no longer leads to an error on Linux systems, with regards to worker processes <a href=\\"https://github.com/eth-sri/lmql/issues/6\\" target=\\"_blank\\" rel=\\"noreferrer\\">#6</a>.</li>\\n</ul>\\n</li>\\n</ul>\\n<h3 id=\\"preview\\" tabindex=\\"-1\\">Preview <a class=\\"header-anchor\\" href=\\"#preview\\" aria-label=\\"Permalink to &quot;Preview&quot;\\">&ZeroWidthSpace;</a></h3>\\n<p>Apart from the changes above, we are also working on a number of other features, including:</p>\\n<ul>\\n<li>\\n<p><strong>llama.cpp support</strong> as started in <a href=\\"https://github.com/eth-sri/lmql/pull/18\\" target=\\"_blank\\" rel=\\"noreferrer\\">this PR</a>, thanks to <a href=\\"https://github.com/CircArgs\\" target=\\"_blank\\" rel=\\"noreferrer\\">@CircArgs</a>.</p>\\n</li>\\n<li>\\n<p>Support for <strong>Type Constraints</strong>, e.g.  <code>type(VAR) is DataClass</code>, that automatically force the model to produce a value that structurally conforms to the given type. See this <a href=\\"https://twitter.com/lbeurerkellner/status/1646187597901733889\\" target=\\"_blank\\" rel=\\"noreferrer\\">Twitter thread</a> for more details.</p>\\n</li>\\n<li>\\n<p>Support for using <strong>Antlr parsers</strong> during query execution, to force the model to produce a value that conforms to a given grammar.</p>\\n</li>\\n<li>\\n<p><strong>Extending Logit Masking to OpenAI Chat Models</strong>. This will enable full support for LMQL constraints with e.g. <code>chatgpt</code> and <code>gpt-4</code> models. See <a href=\\"https://github.com/eth-sri/lmql/pull/25\\" target=\\"_blank\\" rel=\\"noreferrer\\">#25</a>, thanks to <a href=\\"https://github.com/kharvd\\" target=\\"_blank\\" rel=\\"noreferrer\\">@kharvd</a>.</p>\\n</li>\\n</ul>\\n","frontmatter":{"date":"2023-04-17T00:00:00.000Z","title":"LMQL Release 0.0.5"},"excerpt":"","url":"/blog/posts/release-0.0.5.html"}]');const h={class:"posts"},d={class:"post"},u=["href"],m=["innerHTML"],v=JSON.parse('{"title":"Blog","description":"","frontmatter":{"title":"Blog","layout":"doc","aside":false,"outline":false},"headers":[],"relativePath":"blog/index.md","filePath":"blog/index.md"}'),g={name:"blog/index.md"},f=Object.assign(g,{setup(y){function b(s){return s}return(s,w)=>(a(),t("div",null,[(a(!0),t(r,null,i(l(p),n=>(a(),t("div",h,[e("div",d,[e("a",{href:n.url},[e("h1",null,c(n.frontmatter.title),1)],8,u),e("div",{class:"body",innerHTML:n.html},null,8,m)])]))),256))]))}}),q=o(f,[["__scopeId","data-v-61c06c99"]]);export{v as __pageData,q as default};
