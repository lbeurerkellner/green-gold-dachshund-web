import{_ as e,o,c as a,Q as t}from"./chunks/framework.980cae92.js";const s="/assets/inference.f47a6f3e.svg",f=JSON.parse('{"title":"Local Models / Transformers","description":"","frontmatter":{"order":1},"headers":[],"relativePath":"docs/latest/models/hf.md","filePath":"docs/latest/models/hf.md"}'),n={name:"docs/latest/models/hf.md"},r=t('<h1 id="local-models-transformers" tabindex="-1">Local Models / Transformers <a class="header-anchor" href="#local-models-transformers" aria-label="Permalink to &quot;Local Models / Transformers&quot;">​</a></h1><p>LMQL relies on a two-process architecture: The inference process (long-running) loads the model and provides an inference API, and the interpreter process (short-lived) executes your LMQL program.</p><p>This architecture is advantageous for locally-hosted models, as the model loading time can be quite long or the required GPU hardware might not even be available on the client machine.</p><p>This chapter first discusses how to use of the two-process inference API, and then presents an alternative on how to leverage <a href="#in-process-model-loading">In-Process Model Loading</a>, which avoids the need for a separate server process within the same architecture.</p><p><img src="'+s+`" alt="Inference Architecture"></p><p><strong>Prerequisites</strong> Before using a local model, make sure you installed LMQL via <code>pip install lmql[hf]</code>. This ensures the dependencies for running local models are installed. This requirement also applies to <a href="./llama.cpp.html">llama.cpp</a>, as LMQL still relies on HuggingFace <code>transformers</code> for tokenization.</p><p>Then, to start an LMQL inference server, e.g. for the <code>gpt2-medium</code> model, you can run the following command:</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="hljs"><code><span class="line">lmql serve-model gpt2-medium --cuda
</span></code></pre></div><blockquote><p><code>--cuda</code> will load the model on the GPU, if available. If multiple GPUs are available, the model will be distributed across all GPUs. To run with CPU inference, omit the <code>--cuda</code> flag. If you only want to use a specific GPU, you can specify the <code>CUDA_VISIBLE_DEVICES</code> environment variable, e.g. <code>CUDA_VISIBLE_DEVICES=0 lmql serve-model gpt2-medium</code>.</p></blockquote><p>By default, this exposes an <a href="https://github.com/eth-sri/lmql/blob/main/src/lmql/models/lmtp/README.md" target="_blank" rel="noreferrer">LMQL/LMTP inference API</a> on port 8080. When serving a model remotely, make sure to tunnel/forward the port to your client machine. Now, when executing an LMQL query in the playground or via the CLI, you can simply specify e.g. <code>gpt2-medium</code>, and the runtime will automatically connect to the model server running on port 8080 to obtain model-generated text.</p><h3 id="configuration" tabindex="-1">Configuration <a class="header-anchor" href="#configuration" aria-label="Permalink to &quot;Configuration&quot;">​</a></h3><p><strong>Endpoint and Port</strong> By default, models will be served via port <code>8080</code>. To change this, you can specify the port via the <code>--port</code> option of the <code>lmql serve-model</code> command. On the client side, to connect to a model server running on a different port, you can specify the port when constructing an <a href="./../lib/generations.html#lmql-llm-objects"><code>lmql.model</code></a> object:</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="hljs"><code><span class="line">lmql.model(<span class="hljs-string">&quot;gpt2&quot;</span>, endpoint=<span class="hljs-string">&quot;localhost:9999&quot;</span>)
</span></code></pre></div><p><strong>Model Configuration</strong> To load a model with custom quantization preferences or other Transformers arguments, you can specify additional arguments when running the <code>lmql serve-model</code> command. For this, you can provide arbitrary arguments that will directly be passed to the underyling <code>AutoModelForCausalLM.from_pretrained(...)</code> function, as documented in the <a href="https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#transformers.AutoConfig.from_pretrained" target="_blank" rel="noreferrer">Transformers documentation</a>.</p><p>For example, to set <code>trust_remote_code</code> to <code>True</code> with the <code>from_pretrained</code> function, run the following:</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="hljs"><code><span class="line">lmql serve-model gpt2-medium --cuda --port <span class="hljs-number">9999</span> --trust_remote_code <span class="hljs-literal">True</span>
</span></code></pre></div><p>Alternatively, you can also start to serve a model directly from within a Python environment, by running <code>lmql.serve(&quot;gpt2-medium&quot;, cuda=True, port=9999, trust_remote_code=True)</code>. Just as with the CLI, standard <code>transformers</code> arguments are passed through, to the <code>AutoModel.from_pretrained</code> function.</p><h2 id="in-process-models" tabindex="-1">In-Process Models <a class="header-anchor" href="#in-process-models" aria-label="Permalink to &quot;In-Process Models&quot;">​</a></h2><p>If you would like to load the model in-process, without having to execute a separate <code>lmql serve-model</code> command, you can do so by instantiating a custom <code>lmql.model</code> object with <code>local:</code> as part of the model name. For example, to load the <code>gpt2-medium</code> model in-process, run the following command:</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="hljs"><code><span class="line"><span class="hljs-keyword">argmax</span> <span class="hljs-string">&quot;Hello<span class="hljs-placeholder">[WHO]</span>&quot;</span> <span class="hljs-keyword">from</span> lmql.model(<span class="hljs-string">&quot;local:gpt2&quot;</span>)
</span></code></pre></div><p>Note however, that this will load the model on each restart of the LMQL process, which can incur a significant overhead.</p><p>If you want more control over model loading and configuration, you can pass additional arguments to <code>lmql.model(...)</code>, as demonstrated below.</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="hljs"><code><span class="line">lmql.model(<span class="hljs-string">&quot;local:gpt2&quot;</span>, cuda=<span class="hljs-literal">True</span>)
</span></code></pre></div><h2 id="quantization" tabindex="-1">Quantization <a class="header-anchor" href="#quantization" aria-label="Permalink to &quot;Quantization&quot;">​</a></h2><p>Quantization reduces the precision of model parameters to shrink model size and boost inference speed with minimal accuracy loss. LMQL supports two quantization formats: AWQ (using AutoAWQ) and GPTQ (using AutoGPTQ).</p><h3 id="autoawq" tabindex="-1">AutoAWQ <a class="header-anchor" href="#autoawq" aria-label="Permalink to &quot;AutoAWQ&quot;">​</a></h3><p>AWQ minimizes quantization error by protecting crucial weights, promoting model efficiency without sacrificing accuracy. It&#39;s ideal for scenarios requiring both compression and acceleration of LLMs.</p><p>Install AutoAWQ following the repo instructions. To use AWQ-quantized models, run:</p><p>To use <code>AWQ</code>-quantized models, first install <a href="https://github.com/casper-hansen/AutoAWQ" target="_blank" rel="noreferrer">AutoAWQ</a> using the instructions in the repo.</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="hljs"><code><span class="line">lmql serve-model TheBloke/Mistral-7B-OpenOrca-AWQ --loader awq
</span></code></pre></div><h3 id="autogptq" tabindex="-1">AutoGPTQ <a class="header-anchor" href="#autogptq" aria-label="Permalink to &quot;AutoGPTQ&quot;">​</a></h3><p>AutoGPTQ reduces model size while retaining performance by lowering the precision of model weights to 4 or 3 bits. It&#39;s suitable for efficient deployment and operation of LLMs on consumer-grade hardware.</p><p>Install <a href="https://github.com/PanQiWei/AutoGPTQ" target="_blank" rel="noreferrer">AutoGPTQ</a> following the repo instructions. To use GPTQ-quantized models, run:</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="hljs"><code><span class="line">lmql serve-model TheBloke/Arithmo-Mistral-7B-GPTQ --loader gptq
</span></code></pre></div>`,34),l=[r];function i(c,d,p,u,h,m){return o(),a("div",null,l)}const b=e(n,[["render",i]]);export{f as __pageData,b as default};
