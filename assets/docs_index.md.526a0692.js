import{_ as s,o,c as r,Q as a,k as e,a as t}from"./chunks/framework.980cae92.js";const b=JSON.parse('{"title":"Getting Started","description":"","frontmatter":{},"headers":[],"relativePath":"docs/index.md","filePath":"docs/index.md"}'),n={name:"docs/index.md"},l=a(`<h1 id="getting-started" tabindex="-1">Getting Started <a class="header-anchor" href="#getting-started" aria-label="Permalink to &quot;Getting Started&quot;">â€‹</a></h1><div class="subtitle">Learn how to get started with LMQL and write your first program.</div><h2 id="_1-installation" tabindex="-1">1. Installation <a class="header-anchor" href="#_1-installation" aria-label="Permalink to &quot;1. Installation&quot;">â€‹</a></h2><p>You can <a href="./installation.html">install LMQL locally</a> or use the web-based <a href="https://lmql.ai/playground" target="_blank" rel="noreferrer">Playground IDE</a>. For the use of self-hosted models via <a href="./models/hf.html">ðŸ¤— Transformers</a> or <a href="./models/llama.cpp.html">llama.cpp</a>, you have to install LMQL locally.</p><h2 id="_2-write-your-first-query" tabindex="-1">2. Write Your First Query <a class="header-anchor" href="#_2-write-your-first-query" aria-label="Permalink to &quot;2. Write Your First Query&quot;">â€‹</a></h2><p>A very simple <em>Hello World</em> LMQL query looks like this:</p><div class="language-lmql vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">lmql</span><pre class="hljs"><code><span class="line"><span class="hljs-string">&quot;Say &#39;this is a test&#39;:<span class="hljs-placeholder">[RESPONSE]</span>&quot;</span> <span class="hljs-keyword">where</span> <span class="hljs-built_in">len</span>(TOKENS(RESPONSE)) &lt; <span class="hljs-number">25</span>
</span></code></pre></div>`,7),i=e("div",{class:"language-promptdown vp-adaptive-theme"},[e("button",{title:"Copy Code",class:"copy"}),e("span",{class:"lang"},"promptdown"),e("pre",{"pd-text":`# Model Output
Say this is a test: [RESPONSE| This is a test]
`,animate:"true",__animate:"true","animate-speed":"50",class:"promptdown promptdown-compiled",style:{opacity:"1"}},[e("h1",{"pd-shadow-id":"62",text:" "}," Model Output"),e("p",{"pd-shadow-id":"64",text:"S","pd-insertion-point":"true"},[t("Say this is a test: "),e("span",{"pd-shadow-id":"66","pd-instant":"false",text:"",class:"promptdown-var color-pink"},[e("span",{"pd-shadow-id":"67",text:"R",class:"promptdown-var-name"},"RESPONSE"),t(" This is a test")]),t(`
`)])])],-1),p=a(`<p><strong>Note</strong>: <em>You can click Open In Playground to run and experiment with this query.</em></p><p>This simple LMQL program consists of a single prompt statement and an associated <code>where</code> clause:</p><ul><li><p><strong>Prompt Statement</strong> <code>&quot;Say &#39;this is a test&#39;[RESPONSE]&quot;</code>: Prompts are constructed using so-called prompt statements that look like top-level strings in Python. Template variables like <code>[RESPONSE]</code> are automatically completed by the model. Apart from single-line textual prompts, LMQL also support multi-part and scripted prompts, e.g. by allowing control flow and branching behavior to control prompt construction. To learn more, see <a href="./language/scripted-prompting.html">Scripted Prompting</a>.</p></li><li><p><strong>Constraint Clause</strong> <code>where len(RESPONSE) &lt; 10</code>: In this second part of the statement, users can specify logical, high-level constraints on the output. LMQL uses novel evaluation semantics for these constraints, to automatically translate character-level constraints like <code>len(RESPONSE) &lt; 25</code> to (sub)token masks, that can be eagerly enforced during text generation. To learn more, see <a href="./language/constraints.html">Constraints</a>.</p></li></ul><h2 id="_3-going-further" tabindex="-1">3. Going Further <a class="header-anchor" href="#_3-going-further" aria-label="Permalink to &quot;3. Going Further&quot;">â€‹</a></h2><p>Extending on your first query above, you may want to add more complex logic, e.g. by adding a second part to the prompt. Further, you may want to employ a different decoding algorithm, e.g. to sample multiple trajectories of your program or use a different model.</p><p>Let&#39;s extend our initial query, to allow for these changes:</p><div class="language-lmql vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">lmql</span><pre class="hljs"><code><span class="line"><span class="hljs-keyword">sample</span>(temperature=<span class="hljs-number">1.2</span>)

<span class="hljs-string">&quot;Say &#39;this is a test&#39;<span class="hljs-placeholder">[RESPONSE]</span>&quot;</span> <span class="hljs-keyword">where</span> <span class="hljs-built_in">len</span>(TOKENS(RESPONSE)) &lt; <span class="hljs-number">25</span>

<span class="hljs-keyword">if</span> <span class="hljs-string">&quot;test&quot;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> RESPONSE:
    <span class="hljs-string">&quot;You did not say &#39;test&#39;, try again:<span class="hljs-placeholder">[RESPONSE]</span>&quot;</span> <span class="hljs-keyword">where</span> \\
        <span class="hljs-built_in">len</span>(TOKENS(RESPONSE)) &lt; <span class="hljs-number">25</span>
<span class="hljs-keyword">else</span>:
    <span class="hljs-string">&quot;Good job&quot;</span>
</span></code></pre></div><p>Going beyond what we have seen so far, this LMQL program extends on the above in a few ways:</p><ul><li><p><strong>Decoder Declaration</strong> <code>sample(temperature=1.2)</code>: Here, we specify the decoding algorithm to use for text generation. In this case we use <code>sample</code> decoding with slightly increased temperature (&gt;1.0). Above, we implicitly relied on deterministic <code>argmax</code> decoding, which is the default in LMQL. To learn more about the different supported decoding algorithms in LMQL (e.g. <code>beam</code> or <code>best_k</code>), please see <a href="./language/decoding.html">Decoders</a>.</p></li><li><p><strong>Prompt Program</strong>: The main body of the program remains the prompt. As before, we use prompt statements here, however, now we also make use of control-flow and branching behavior.</p><p>On each LLM call, the concatenation of all prompt statements so far, form the prompt used to generate a value for the currently active template variable like <code>RESPONSE</code>. This means the LLM is always aware of the full prompt context so far, when generating a value for a template variable.</p><p>After a prompt statement has been executed, the contained template variables are automatically exposed to the surrounding program context. This allows you to react to model output and incorporate the results in your program logic. To learn more about this form of interactive prompting, please see <a href="./language/scripted-prompting.html">Scripted Prompting</a>.</p></li></ul><h2 id="_3-enjoy" tabindex="-1">3. Enjoy <a class="header-anchor" href="#_3-enjoy" aria-label="Permalink to &quot;3. Enjoy&quot;">â€‹</a></h2><p>These basic steps should get you started with LMQL. If you need more inspiration before writing your own queries, you can explore the examples included with the <a href="https://lmql.ai/playground" target="_blank" rel="noreferrer">Playground IDE</a> or showcased on the <a href="https://lmql.ai/" target="_blank" rel="noreferrer">LMQL Website</a>.</p><p>If you have any questions and or requests for documentation, please feel free to reach out to us via our <a href="https://discord.com/invite/7eJP4fcyNT" target="_blank" rel="noreferrer">Community Discord</a>, <a href="https://github.com/eth-sri/lmql/issues" target="_blank" rel="noreferrer">GitHub Issues</a>, or <a href="https://twitter.com/lmqllang" target="_blank" rel="noreferrer">Twitter</a>.</p>`,12),d=[l,i,p];function c(h,u,m,g,f,y){return o(),r("div",null,d)}const w=s(n,[["render",c]]);export{b as __pageData,w as default};
