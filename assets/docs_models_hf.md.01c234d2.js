import{_ as e,o,c as a,Q as t}from"./chunks/framework.c2adf1ba.js";const s="/assets/inference.f47a6f3e.svg",f=JSON.parse('{"title":"Local Models / Transformers","description":"","frontmatter":{"order":1},"headers":[],"relativePath":"docs/models/hf.md","filePath":"docs/models/hf.md"}'),n={name:"docs/models/hf.md"},l=t('<h1 id="local-models-transformers" tabindex="-1">Local Models / Transformers <a class="header-anchor" href="#local-models-transformers" aria-label="Permalink to &quot;Local Models / Transformers&quot;">​</a></h1><p>LMQL relies on a two-process architecture: The inference process (long-running) loads the model and provides an inference API, and the interpreter process (short-lived) executes your LMQL program.</p><p>This architecture is advantageous for locally-hosted models, as the model loading time can be quite long or the required GPU hardware might not even be available on the client machine.</p><p>This chapter first discusses how to use of the two-process inference API, and then presents an alternative on how to leverage <a href="#in-process-model-loading">In-Process Model Loading</a>, which avoids the need for a separate server process within the same architecture.</p><p><img src="'+s+`" alt="Inference Architecture"></p><p><strong>Prerequisites</strong> Before using a local model, make sure you installed LMQL via <code>pip install lmql[hf]</code>. This ensures the dependencies for running local models are installed. This requirement also applies to <a href="./llama.cpp.html">llama.cpp</a>, as LMQL still relies on HuggingFace <code>transformers</code> for tokenization.</p><p>Then, to start an LMQL inference server, e.g. for the <code>gpt2-medium</code> model, you can run the following command:</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="hljs"><code><span class="line">lmql serve-model gpt2-medium --cuda
</span></code></pre></div><blockquote><p><code>--cuda</code> will load the model on the GPU, if available. If multiple GPUs are available, the model will be distributed across all GPUs. To run with CPU inference, omit the <code>--cuda</code> flag. If you only want to use a specific GPU, you can specify the <code>CUDA_VISIBLE_DEVICES</code> environment variable, e.g. <code>CUDA_VISIBLE_DEVICES=0 lmql serve-model gpt2-medium</code>.</p></blockquote><p>By default, this exposes an <a href="https://github.com/eth-sri/lmql/blob/main/src/lmql/models/lmtp/README.md" target="_blank" rel="noreferrer">LMQL/LMTP inference API</a> on port 8080. When serving a model remotely, make sure to tunnel/forward the port to your client machine. Now, when executing an LMQL query in the playground or via the CLI, you can simply specify e.g. <code>gpt2-medium</code>, and the runtime will automatically connect to the model server running on port 8080 to obtain model-generated text.</p><h3 id="configuration" tabindex="-1">Configuration <a class="header-anchor" href="#configuration" aria-label="Permalink to &quot;Configuration&quot;">​</a></h3><p><strong>Endpoint and Port</strong> By default, models will be served via port <code>8080</code>. To change this, you can specify the port via the <code>--port</code> option of the <code>lmql serve-model</code> command. On the client side, to connect to a model server running on a different port, you can specify the port when constructing an <a href="./../lib/generations.html#lmql-llm-objects"><code>lmql.model</code></a> object:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="hljs"><code><span class="line">lmql.model(<span class="hljs-string">&quot;gpt2&quot;</span>, endpoint=<span class="hljs-string">&quot;localhost:9999&quot;</span>)
</span></code></pre></div><p><strong>Model Configuration</strong> To load a model with custom quantization preferences or other Transformers arguments, you can specify additional arguments when running the <code>lmql serve-model</code> command. For this, you can provide arbitrary arguments that will directly be passed to the underyling <code>AutoModelForCausalLM.from_pretrained(...)</code> function, as documented in the <a href="https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#transformers.AutoConfig.from_pretrained" target="_blank" rel="noreferrer">Transformers documentation</a>.</p><p>For example, to set <code>trust_remote_code</code> to <code>True</code> with the <code>from_pretrained</code> function, run the following:</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="hljs"><code><span class="line">lmql serve-model gpt2-medium --cuda --port <span class="hljs-number">9999</span> --trust_remote_code <span class="hljs-literal">True</span>
</span></code></pre></div><p>Alternatively, you can also start to serve a model directly from within a Python environment, by running <code>lmql.serve(&quot;gpt2-medium&quot;, cuda=True, port=9999, trust_remote_code=True)</code>. Just as with the CLI, standard <code>transformers</code> arguments are passed through, to the <code>AutoModel.from_pretrained</code> function.</p><h2 id="in-process-models" tabindex="-1">In-Process Models <a class="header-anchor" href="#in-process-models" aria-label="Permalink to &quot;In-Process Models&quot;">​</a></h2><p>If you would like to load the model in-process, without having to execute a separate <code>lmql serve-model</code> command, you can do so by instantiating a custom <code>lmql.model</code> object with <code>local:</code> as part of the model name. For example, to load the <code>gpt2-medium</code> model in-process, run the following command:</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="hljs"><code><span class="line"><span class="hljs-keyword">argmax</span> <span class="hljs-string">&quot;Hello<span class="hljs-placeholder">[WHO]</span>&quot;</span> <span class="hljs-keyword">from</span> lmql.model(<span class="hljs-string">&quot;local:gpt2&quot;</span>)
</span></code></pre></div><p>Note however, that this will load the model on each restart of the LMQL process, which can incur a significant overhead.</p><p>If you want more control over model loading and configuration, you can pass additional arguments to <code>lmql.model(...)</code>, as demonstrated below.</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="hljs"><code><span class="line">lmql.model(<span class="hljs-string">&quot;local:gpt2&quot;</span>, cuda=<span class="hljs-literal">True</span>)
</span></code></pre></div>`,23),r=[l];function c(d,i,p,m,h,u){return o(),a("div",null,r)}const v=e(n,[["render",c]]);export{f as __pageData,v as default};
