import{_ as a,o as e,c as s,Q as l}from"./chunks/framework.c2adf1ba.js";const g=JSON.parse('{"title":"llama.cpp","description":"","frontmatter":{"order":2},"headers":[],"relativePath":"docs/models/llama.cpp.md","filePath":"docs/models/llama.cpp.md"}'),n={name:"docs/models/llama.cpp.md"},o=l(`<h1 id="llama-cpp" tabindex="-1">llama.cpp <a class="header-anchor" href="#llama-cpp" aria-label="Permalink to &quot;llama.cpp&quot;">​</a></h1><p>LMQL also supports <a href="https://github.com/ggerganov/llama.cpp" target="_blank" rel="noreferrer">llama.cpp</a> as an inference backend, which can run efficiently on CPU-only and mixed CPU/GPU systems.</p><p>Before using llama.cpp models, make sure you installed its Python bindings via <code>pip install llama-cpp-python</code> in the same environment where you installed LMQL. You also need the <code>sentencepiece</code> or <code>transformers</code> package installed, for tokenization.</p><p>Also make sure you first convert your model weights according to the <a href="https://github.com/ggerganov/llama.cpp#prepare-data--run" target="_blank" rel="noreferrer">llama.cpp documentation</a>, to the <code>.bin</code> format.</p><p>Just like <a href="./hf.html">Transformers models</a>, you can load llama.cpp models either locally or via a long-lived <code>lmql serve-model</code> inference server.</p><h3 id="model-server" tabindex="-1">Model Server <a class="header-anchor" href="#model-server" aria-label="Permalink to &quot;Model Server&quot;">​</a></h3><p>To start a llama.cpp model server, you can run the following command:</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="hljs"><code><span class="line">lmql serve-model llama.cpp:&lt;PATH TO WEIGHTS&gt;.<span class="hljs-built_in">bin</span>
</span></code></pre></div><p>This will launch an <a href="https://github.com/eth-sri/lmql/tree/main/src/lmql/models/lmtp" target="_blank" rel="noreferrer">LMTP inference endpoint</a> on <code>localhost:8080</code>, which can be used from LMQL with a query program like this:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="hljs"><code><span class="line"><span class="hljs-keyword">argmax</span> 
    <span class="hljs-string">&quot;Say &#39;this is a test&#39;:<span class="hljs-placeholder">[RESPONSE]</span>&quot;</span> 
<span class="hljs-keyword">from</span> 
    <span class="hljs-string">&quot;llama.cpp:&lt;PATH TO WEIGHTS&gt;.bin&quot;</span>
<span class="hljs-keyword">where</span> 
    <span class="hljs-built_in">len</span>(TOKENS(RESPONSE)) &lt; <span class="hljs-number">10</span>
</span></code></pre></div><h3 id="running-without-a-model-server" tabindex="-1">Running Without a Model Server <a class="header-anchor" href="#running-without-a-model-server" aria-label="Permalink to &quot;Running Without a Model Server&quot;">​</a></h3><p>To load the llama.cpp from the Python process that executes your LMQL query, you can use the following syntax:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="hljs"><code><span class="line"><span class="hljs-keyword">argmax</span> 
    <span class="hljs-string">&quot;Say &#39;this is a test&#39;:<span class="hljs-placeholder">[RESPONSE]</span>&quot;</span>
<span class="hljs-keyword">from</span>
    <span class="hljs-string">&quot;local:llama.cpp:&lt;PATH TO WEIGHTS&gt;.bin&quot;</span>
<span class="hljs-keyword">where</span> 
    <span class="hljs-built_in">len</span>(TOKENS(RESPONSE)) &lt; <span class="hljs-number">10</span>
</span></code></pre></div><h3 id="configuring-the-llama-instance" tabindex="-1">Configuring the Llama(...) instance <a class="header-anchor" href="#configuring-the-llama-instance" aria-label="Permalink to &quot;Configuring the Llama(...) instance&quot;">​</a></h3><p>Any parameters passed to <code>lmql serve-model</code> and, when running locally, to <code>lmql.model(...)</code> will be passed to the <code>Llama(...)</code> constructor.</p><p>For example, to configure the <code>Llama(...)</code> instance to use an <code>n_ctx</code> value of 1024, run:</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="hljs"><code><span class="line">lmql serve-model llama.cpp:&lt;PATH TO WEIGHTS&gt;.<span class="hljs-built_in">bin</span> --n_ctx <span class="hljs-number">1024</span>
</span></code></pre></div><p>Or, when running locally, you can use <code>lmql.model(&quot;local:llama.cpp:&lt;PATH TO WEIGHTS&gt;.bin&quot;, n_ctx=1024)</code>.</p>`,18),t=[o];function c(p,r,i,d,h,m){return e(),s("div",null,t)}const b=a(n,[["render",c]]);export{g as __pageData,b as default};
