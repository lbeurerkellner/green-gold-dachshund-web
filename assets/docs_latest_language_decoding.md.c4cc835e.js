import{_ as e,o as a,c as t,Q as o}from"./chunks/framework.c2adf1ba.js";const g=JSON.parse('{"title":"Decoders","description":"","frontmatter":{},"headers":[],"relativePath":"docs/latest/language/decoding.md","filePath":"docs/latest/language/decoding.md"}'),n={name:"docs/latest/language/decoding.md"},s=o(`<h1 id="decoders" tabindex="-1">Decoders <a class="header-anchor" href="#decoders" aria-label="Permalink to &quot;Decoders&quot;">​</a></h1><p>LMQL support various decoding algorithms, which are used to generate text from the token distribution of a language model. For this, decoding algorithm in use, can be specified right at the beginning of a query, e.g. using a decoder keyword like <code>argmax</code>.</p><p>All supported decoding algorithms are model-agnostic and can be used with any LMQL-supported inference backend. For more information on the supported inference backends, see the <a href="./../models/index.html">Models</a> chapter.</p><h2 id="setting-the-decoding-algorithm" tabindex="-1">Setting The Decoding Algorithm <a class="header-anchor" href="#setting-the-decoding-algorithm" aria-label="Permalink to &quot;Setting The Decoding Algorithm&quot;">​</a></h2><p>Depending on context, LMQL offers two ways to specify the decoding algorithm to use.</p><br><p><strong>Decoder Configuration as part of the query</strong>: The first option is to simply specify the decoding algorithm and its parameters as part of the query itself. This can be particularly useful, if your choice of decoder is relevant to the concrete program you are writing.</p><div class="language-lmql vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">lmql</span><pre class="hljs"><code><span class="line"><span class="hljs-comment"># use beam search with beam width 2 for</span>
<span class="hljs-comment"># the entire program</span>
<span class="hljs-keyword">beam</span>(n=<span class="hljs-number">2</span>)

<span class="hljs-comment"># uses beam search to generate RESPONSE </span>
<span class="hljs-string">&quot;This is a query with a specified decoder: <span class="hljs-placeholder">[RESPONSE]</span>&quot;</span>
</span></code></pre></div><p>Decoding algorithms are always specified for the entire query program, and cannot change within a program. To use different decoders for different parts of your program, you have to split your program into multiple queries.</p><br><p><strong>Specifying the Decoding Algorithm Externally</strong>: The second option is to specify the decoding algorithm and parameters externally, i.e. separatly from the actual program code:</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="hljs"><code><span class="line"><span class="hljs-keyword">import</span> lmql

<span class="hljs-meta">@lmql.query(<span class="hljs-params">model=<span class="hljs-string">&quot;openai/text-davinci-003&quot;</span>, decoder=<span class="hljs-string">&quot;sample&quot;</span>, temperature=<span class="hljs-number">1.8</span></span>)</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">tell_a_joke</span>():
    <span class="hljs-inline-lmql"><span style="opacity:0.4;">&#39;&#39;&#39;lmql</span>
    <span class="hljs-string">&quot;&quot;&quot;A list good dad joke. A indicates the punchline:
    Q:<span class="hljs-placeholder">[JOKE]</span>
    A:<span class="hljs-placeholder">[PUNCHLINE]</span>&quot;&quot;&quot;</span> <span class="hljs-keyword">where</span> STOPS_AT(JOKE, <span class="hljs-string">&quot;?&quot;</span>) <span class="hljs-keyword">and</span>  STOPS_AT(PUNCHLINE, <span class="hljs-string">&quot;\\n&quot;</span>)
    &#39;&#39;&#39;</span>

tell_a_joke() <span class="hljs-comment"># uses the decoder specified in @lmql.query(...)</span>
tell_a_joke(decoder=<span class="hljs-string">&quot;beam&quot;</span>, n=<span class="hljs-number">2</span>) <span class="hljs-comment"># uses a beam search decoder with n=2</span>
</span></code></pre></div><p>This is only possible when using LMQL from a Python context.</p><h2 id="decoding-algorithms" tabindex="-1">Decoding Algorithms <a class="header-anchor" href="#decoding-algorithms" aria-label="Permalink to &quot;Decoding Algorithms&quot;">​</a></h2><p>In general, the very first keyword of an LMQL query, specifies the decoding algorithm to use. For this, the following decoder keywords are available:</p><h3 id="argmax" tabindex="-1"><code>argmax</code> <a class="header-anchor" href="#argmax" aria-label="Permalink to &quot;\`argmax\`&quot;">​</a></h3><p>The <code>argmax</code> decoder is the simplest decoder available in LMQL. It greedily selects the most likely token at each step of the decoding process. It has no additional parameters. Since <code>argmax</code> decoding is deterministic, one can only generate a single sequence at a time.</p><h3 id="sample-n-int-temperature-float" tabindex="-1"><code>sample(n: int, temperature: float)</code> <a class="header-anchor" href="#sample-n-int-temperature-float" aria-label="Permalink to &quot;\`sample(n: int, temperature: float)\`&quot;">​</a></h3><p>The <code>sample</code> decoder samples <code>n</code> sequences in parallel from the model. The <code>temperature</code> parameter controls the randomness of the sampling process. Higher values of <code>temperature</code> lead to more random samples, while lower values lead to more likely samples. A temperature value of <code>0.0</code> is equivalent to the <code>argmax</code> decoder.</p><h3 id="beam-n-int" tabindex="-1"><code>beam(n: int)</code> <a class="header-anchor" href="#beam-n-int" aria-label="Permalink to &quot;\`beam(n: int)\`&quot;">​</a></h3><p>A simple beam search decoder. The <code>n</code> parameter controls the beam size. The beam search decoder is deterministic, so it will generate the same <code>n</code> sequences every time. The result of a <code>beam</code> query is a list of <code>n</code> sequences, sorted by their likelihood.</p><h3 id="beam-sample-n-int-temperature-float" tabindex="-1"><code>beam_sample(n: int, temperature: float)</code> <a class="header-anchor" href="#beam-sample-n-int-temperature-float" aria-label="Permalink to &quot;\`beam_sample(n: int, temperature: float)\`&quot;">​</a></h3><p>A beam search decoder that samples from the beam at each step. The <code>n</code> parameter controls the beam size, while the <code>temperature</code> parameter controls the randomness of the sampling process. The result of a <code>beam_sample</code> query is a list of <code>n</code> sequences, sorted by their likelihood.</p><h2 id="novel-decoders" tabindex="-1">Novel Decoders <a class="header-anchor" href="#novel-decoders" aria-label="Permalink to &quot;Novel Decoders&quot;">​</a></h2><p>LMQL also implements a number of novel decoders. These decoders are experimental and may not work as expected. They are also not guaranteed to be stable across different LMQL versions. More documentation on these decoders will be provided in the future.</p><h3 id="var-b-int-n-int" tabindex="-1"><code>var(b: int, n: int)</code> <a class="header-anchor" href="#var-b-int-n-int" aria-label="Permalink to &quot;\`var(b: int, n: int)\`&quot;">​</a></h3><p>An experimental implementation of variable-level beam search.</p><h3 id="beam-var-n-int" tabindex="-1"><code>beam_var(n: int)</code> <a class="header-anchor" href="#beam-var-n-int" aria-label="Permalink to &quot;\`beam_var(n: int)\`&quot;">​</a></h3><p>An experimental implementation of a beam search procedure that groups by currently-decoded variable and applies adjusted length penalties.</p><h2 id="inspecting-decoding-trees" tabindex="-1">Inspecting Decoding Trees <a class="header-anchor" href="#inspecting-decoding-trees" aria-label="Permalink to &quot;Inspecting Decoding Trees&quot;">​</a></h2><p>LMQL also provides a way to inspect the decoding trees generated by the decoders. For this, make sure to execute the query in the Playground IDE and click on the <code>Advanced Mode</code> button, in the top right corner of the Playground. This will open a new pane, where you can navigate and inspect the LMQL decoding tree.</p><p>Among other things, this view allows you to track the decoding process, active hypotheses and interpreter state, including the current evaluation result of the <code>where</code> clause. For an example, consider the <a href="https://lmql.ai/playground/#translation" target="_blank" rel="noreferrer">translation example</a> as included in the Playground IDE (make sure to enable <code>Advanced Mode</code>).</p><h2 id="writing-custom-decoders" tabindex="-1">Writing Custom Decoders <a class="header-anchor" href="#writing-custom-decoders" aria-label="Permalink to &quot;Writing Custom Decoders&quot;">​</a></h2><p>LMQL also includes a library for array-based decoding <code>dclib</code>, which can be used to implement custom decoders. More information on this, will be provided in the future. The implementation of the available decoding procedures is located in <code>src/lmql/runtime/dclib/decoders.py</code> of the LMQL repository.</p><h2 id="other-decoding-parameters" tabindex="-1">Other Decoding Parameters <a class="header-anchor" href="#other-decoding-parameters" aria-label="Permalink to &quot;Other Decoding Parameters&quot;">​</a></h2><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td><code>max_len: int</code></td><td>The maximum length of the generated sequence. If not specified, the default value of <code>max_len</code> is <code>2048</code>. Note if the maximum length is reached, the LMQL runtime will throw an error if the query has not yet come to a valid result, according to the provided <code>where</code> clause.</td></tr><tr><td><code>chunksize: int</code></td><td>The chunksize parameter used for <code>max_tokens</code> in OpenAI API requests or in speculative inference with local models. If not specified, the default value of <code>chunksize</code> is <code>32</code>. See also the description of this parameter in the <a href="./../models/openai.html#monitoring-openai-api-use">Models</a> chapter.</td></tr><tr><td><code>verbose: bool</code></td><td>Enables verbose console logging for individual LLM inference calls (local generation calls or OpenAI API request payloads).</td></tr><tr><td><code>cache: Union[bool,str]</code></td><td><code>True</code> or <code>False</code> to enable in-memory token caching. If not specified, the default value of <code>cache</code> is <code>True</code>, indicating in-memory caching is enabled. <br><br> Setting <code>cache</code> to a string value, specifies a local file to use for disk-based caching, enabling caching across multiple query executions and sessions.</td></tr><tr><td><code>openai_nonstop</code></td><td>Experimental option for OpenAI-specific non-stop generation, which can further improve the effectiveness of caching in some scenarios.</td></tr><tr><td><code>chunk_timeout</code></td><td>OpenAI-specific maximum time in seconds to wait for the next chunk of tokens to arrive. If exceeded, the current API request will be retried with an approriate backoff. <br><br> If not specified, the default value of <code>chunk_timeout</code> is <code>2.5</code>. Adjust this parameter, if you are seeing a high number of timeouts in the console output of the LMQL runtime.</td></tr><tr><td></td><td></td></tr></tbody></table>`,36),r=[s];function i(d,c,l,h,p,m){return a(),t("div",null,r)}const f=e(n,[["render",i]]);export{g as __pageData,f as default};
