import{_ as e,o as t,c as a,Q as s}from"./chunks/framework.980cae92.js";const g=JSON.parse('{"title":"Your First Chatbot","description":"","frontmatter":{"order":0},"headers":[],"relativePath":"docs/latest/lib/chat/overview.md","filePath":"docs/latest/lib/chat/overview.md"}'),o={name:"docs/latest/lib/chat/overview.md"},n=s(`<h1 id="your-first-chatbot" tabindex="-1">Your First Chatbot <a class="header-anchor" href="#your-first-chatbot" aria-label="Permalink to &quot;Your First Chatbot&quot;">​</a></h1><p>This chapter will walk you through the process of implementing a simple LMQL chat applications. For this, we will implement a chatbot that responds to user messages while also considering a system prompt, that you will provide during development.</p><h2 id="_1-the-core-chat-loop" tabindex="-1">1. The Core Chat Loop <a class="header-anchor" href="#_1-the-core-chat-loop" aria-label="Permalink to &quot;1. The Core Chat Loop&quot;">​</a></h2><p>A chatbot is an interactive applications that continously responds to user input. To implement this in LMQL, we can simply use a <code>while</code> loop that repeatedly calls the <code>input()</code> function to wait for and process user input.</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="hljs"><code><span class="line"><span class="hljs-keyword">from</span> lmql.lib.chat <span class="hljs-keyword">import</span> message

<span class="hljs-keyword">argmax</span> 
    <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:
        <span class="hljs-string">&quot;<span class="hljs-subst">{:user}</span> <span class="hljs-subst">{<span class="hljs-keyword">await</span> <span class="hljs-built_in">input</span>()}</span>&quot;</span>
        <span class="hljs-string">&quot;<span class="hljs-subst">{:assistant}</span> <span class="hljs-placeholder">[@message ANSWER]</span>&quot;</span>
<span class="hljs-keyword">from</span>
   <span class="hljs-string">&quot;chatgpt&quot;</span>
</span></code></pre></div><blockquote><p><strong>Note:</strong> You can run this query for yourself in the <a href="./../../">LMQL playground</a>. When executed, the playground will provide a simple chat interface in the model output panel.</p></blockquote><p><strong>Tags</strong> As part of the prompt statements in our program, we use designated <code>{:user}</code> and <code>{:assistant}</code> tags, to annotate our prompt in a way that allows the model to distinguish between user and assistant messages.</p><p><strong>Input</strong> Note also, that in LMQL you have to use <code>await input()</code> instead of the traditional Python <code>input()</code> function, to make sure that the program does not block while waiting for user input.</p><p><strong>@message</strong> To differentiate between internal output and user-facing chat message output, you can use the <code>@message</code> decorator function. This way, only the output of the decorated variables will be displayed to the user, whereas intermediate reasoning is kept internal. To learn more about chat serving, make sure to also read the chapter on <a href="./serving.html">Chat Serving</a>.</p><p>With respect to model choice, we use the <code>chatgpt</code> model here. Nonetheless, based on LMQL&#39;s broad support for different <a href="./../../models/">inference backends</a>, the model can be easily replaced with any other supported model. For ChatGPT, tags like <code>{:user}</code> directly translate to <a href="https://platform.openai.com/docs/guides/gpt/chat-completions-api" target="_blank" rel="noreferrer">OpenAI roles</a>. For other models, LMQL inserts equivalent raw text annotations, e.g. <code>((user)):</code>.</p><h2 id="_2-adding-a-system-prompt" tabindex="-1">2. Adding a System Prompt <a class="header-anchor" href="#_2-adding-a-system-prompt" aria-label="Permalink to &quot;2. Adding a System Prompt&quot;">​</a></h2><p>While the above program already works, it is not very personalized. To make the chatbot more engaging, we can add a <em>system prompt</em> that instructs the model to respond in a specific way. The system prompt is an additional instruction that we include at the beginning of our program and will not be directly visible to the user.</p><p>To add a system prompt, we can simply include an additional annotated <code>{:system}</code> prompt statement:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="hljs"><code><span class="line"><span class="hljs-keyword">from</span> lmql.lib.chat <span class="hljs-keyword">import</span> message

<span class="hljs-keyword">argmax</span> 
    <span class="hljs-string">&quot;<span class="hljs-subst">{:system}</span> You are a marketing chatbot for the\\
     language model query language (LMQL).&quot;</span>
    <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:
        <span class="hljs-string">&quot;<span class="hljs-subst">{:user}</span> <span class="hljs-subst">{<span class="hljs-keyword">await</span> <span class="hljs-built_in">input</span>()}</span>&quot;</span>
        <span class="hljs-string">&quot;<span class="hljs-subst">{:assistant}</span> <span class="hljs-placeholder">[ANSWER]</span>&quot;</span>
<span class="hljs-keyword">from</span>
   <span class="hljs-string">&quot;chatgpt&quot;</span>
</span></code></pre></div><p>To resulting chat application will now respond in a more personalized way, as it will consider the system prompt before responding to user input. In this case, we instruct it to respond as LMQL marketing agent.</p><h2 id="_3-serving-the-chatbot" tabindex="-1">3. Serving the Chatbot <a class="header-anchor" href="#_3-serving-the-chatbot" aria-label="Permalink to &quot;3. Serving the Chatbot&quot;">​</a></h2><p>Lastly, to move beyond the playground, we can use the <code>lmql chat</code> command to serve our chatbot as a local web application. To do so, we just save the above program as <code>chat.lmql</code> and run the following command:</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="hljs"><code><span class="line">lmql chat chat.lmql
</span></code></pre></div><p>Once the server is running, you can access the chatbot at the provided local URL.</p><figure align="center" style="width:100%;margin:auto;" alt="Screenshot of the model dropdown in the playground"><img style="min-height:100pt;" src="https://github.com/eth-sri/lmql/assets/17903049/334e9ab4-aab8-448d-9dc0-c53be8351e27" alt="Screenshot of the model dropdown in the playground"><figcaption>A simple chatbot using the LMQL Chat UI.</figcaption></figure><p>In this interface, you can interact with your chatbot by typing into the input field at the bottom of the screen. The chatbot will then respond to your input, while also considering the system prompt that you provide in your program. On the right, you can inspect the full internal prompt of your program, including the generated prompt statements and the model output. This allows you at all times, to understand what exact input the model received and how it responded to it.</p><h2 id="learn-more" tabindex="-1">Learn More <a class="header-anchor" href="#learn-more" aria-label="Permalink to &quot;Learn More&quot;">​</a></h2><p>To learn more, return to the <a href="./../chat.html">Chat overview page</a> and pick one of the provided topics.</p>`,23),r=[n];function l(i,p,h,c,d,u){return t(),a("div",null,r)}const b=e(o,[["render",l]]);export{g as __pageData,b as default};
