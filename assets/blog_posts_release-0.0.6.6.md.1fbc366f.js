import{_ as e,o as s,c as a,Q as t}from"./chunks/framework.c2adf1ba.js";const g=JSON.parse('{"title":"LMQL v0.0.6.6","description":"","frontmatter":{"date":"2023-07-25T00:00:00.000Z","title":"LMQL v0.0.6.6"},"headers":[],"relativePath":"blog/posts/release-0.0.6.6.md","filePath":"blog/posts/release-0.0.6.6.md"}'),n={name:"blog/posts/release-0.0.6.6.md"},o=t(`<p><span class="date">July 25, 2023</span></p><p>We just released LMQL <em>0.0.6.6</em>. This is a minor update with a couple of smaller fixes and improvements.</p><ul><li><code>lmql.F</code> now supports positional arguments:</li></ul><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="hljs"><code><span class="line">greet = lmql.F(<span class="hljs-string">&quot;Greet <span class="hljs-subst">{a}</span> and <span class="hljs-subst">{b}</span>: <span class="hljs-placeholder">[GREETING]</span>&quot;</span>)

<span class="hljs-comment"># call with positional arguments</span>
greet(<span class="hljs-string">&quot;Alice&quot;</span>, <span class="hljs-string">&quot;Bob&quot;</span>) <span class="hljs-comment"># Greet Alice and Bob: Hello!</span>
<span class="hljs-comment"># call with keyword arguments</span>
greet(a=<span class="hljs-string">&quot;Alice&quot;</span>, b=<span class="hljs-string">&quot;Bob&quot;</span>) <span class="hljs-comment"># Greet Alice and Bob: Hello!</span>
</span></code></pre></div><ul><li><p>We improved the error handling of the <code>llama.cpp</code> backend and fixed a bug with model identifier parsing.</p></li><li><p>We also fixed a bug with the LMTP scheduler, where CPU load was high even when no tasks were present. Thanks to community member <a href="https://github.com/4onen" target="_blank" rel="noreferrer">@4onen</a> for reporting and fixing this!</p></li><li><p>Added backend support for <code>auto_gptq</code> quantized models, contributed by community member <a href="https://github.com/meditans" target="_blank" rel="noreferrer">@meditans</a>.</p></li><li><p>We fixed an issue where for Azure OpenAI models, a dummy configuration <code>api.env</code> was needed. See our <a href="./../../docs/models/azure.html">documentation</a> for details. Thanks to community members Missing and <a href="https://github.com/hooman-bayer" target="_blank" rel="noreferrer">@hooman-bayer</a> for their feedback and contributions to this.</p></li></ul><blockquote><p><strong>Versioning Note</strong>: 0.0.6.6 is the last release with two leading zeros. Starting with the next release, LMQL will adopt semantic versioning and use a single leading zero, i.e. 0.6.7.</p></blockquote>`,6),l=[o];function r(i,p,c,d,h,m){return s(),a("div",null,l)}const b=e(n,[["render",r]]);export{g as __pageData,b as default};
