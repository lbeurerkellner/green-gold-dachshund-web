import{_ as e,o as a,c as s,Q as l}from"./chunks/framework.c2adf1ba.js";const f=JSON.parse('{"title":"Overview","description":"","frontmatter":{"order":0},"headers":[],"relativePath":"docs/latest/models/index.md","filePath":"docs/latest/models/index.md"}'),o={name:"docs/latest/models/index.md"},n=l(`<h1 id="overview" tabindex="-1">Overview <a class="header-anchor" href="#overview" aria-label="Permalink to &quot;Overview&quot;">​</a></h1><p>LMQL is a high-level, front-end language for text generation. This means that LMQL is not specific to any particular text generation model. Instead, we support a wide range of text generation models on the backend, including <a href="./openai.html">OpenAI model</a>, <a href="./llama.cpp.html">llama.cpp</a> and <a href="./hf.html">HuggingFace Transformers</a>.</p><h2 id="loading-models" tabindex="-1">Loading Models <a class="header-anchor" href="#loading-models" aria-label="Permalink to &quot;Loading Models&quot;">​</a></h2><p>To load models in LMQL, you can use the <code>lmql.model(...)</code> function which gives you an <a href="./../lib/generations.html#lmql-llm-objects"><code>lmql.LLM</code></a> object:</p><div class="language-lmql vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">lmql</span><pre class="hljs"><code><span class="line">lmql.model(<span class="hljs-string">&quot;openai/gpt-3.5-turbo-instruct&quot;</span>) <span class="hljs-comment"># OpenAI API model</span>
lmql.model(<span class="hljs-string">&quot;random&quot;</span>, seed=<span class="hljs-number">123</span>) <span class="hljs-comment"># randomly sampling model</span>
lmql.model(<span class="hljs-string">&quot;llama.cpp:&lt;YOUR_WEIGHTS&gt;.bin&quot;</span>) <span class="hljs-comment"># llama.cpp model</span>

lmql.model(<span class="hljs-string">&quot;local:gpt2&quot;</span>) <span class="hljs-comment"># load a \`transformers\` model in-process</span>
lmql.model(<span class="hljs-string">&quot;local:gpt2&quot;</span>, cuda=<span class="hljs-literal">True</span>, load_in_4bit=<span class="hljs-literal">True</span>) <span class="hljs-comment"># load a \`transformers\` model in process with additional arguments</span>
lmql.model(<span class="hljs-string">&quot;gpt2&quot;</span>) <span class="hljs-comment"># access a \`transformers\` model hosted via \`lmql serve-model\`</span>
</span></code></pre></div><p>LMQL supports multiple inference backends, each of which has its own set of parameters. For more details on how to use and configure the different backends, please refer to one of the following sections:</p><ul><li><a href="./hf.html">Transformers</a></li><li><a href="./llama.cpp.html">llama.cpp</a></li><li><a href="./openai.html">OpenAI</a></li><li><a href="./azure.html">Azure OpenAI</a></li><li><a href="./replicate.html">Replicate</a></li></ul><h2 id="specifying-the-model" tabindex="-1">Specifying The Model <a class="header-anchor" href="#specifying-the-model" aria-label="Permalink to &quot;Specifying The Model&quot;">​</a></h2><p>After creating an <code>lmql.LLM</code> object, you can pass it to a query program to specify the model to use during execution. There are two ways to do this:</p><h3 id="option-a-specifying-the-model-externally" tabindex="-1">Option A: Specifying the Model Externally <a class="header-anchor" href="#option-a-specifying-the-model-externally" aria-label="Permalink to &quot;Option A: Specifying the Model Externally&quot;">​</a></h3><p>You can specify the model and its parameters externally, i.e. separately from the actual program code:</p><div class="language-lmql vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">lmql</span><pre class="hljs"><code><span class="line"><span class="hljs-keyword">import</span> lmql

<span class="hljs-comment"># uses &#39;chatgpt&#39; by default</span>
<span class="hljs-meta">@lmql.query(<span class="hljs-params">model=<span class="hljs-string">&quot;chatgpt&quot;</span></span>)</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">tell_a_joke</span>():
    <span class="hljs-inline-lmql"><span class="inline-lmql-delim">&#39;&#39;&#39;lmql</span>
    <span class="hljs-string">&quot;&quot;&quot;A great good dad joke. A indicates the punchline
    Q:<span class="hljs-placeholder">[JOKE]</span>
    A:<span class="hljs-placeholder">[PUNCHLINE]</span>&quot;&quot;&quot;</span> <span class="hljs-keyword">where</span> STOPS_AT(JOKE, <span class="hljs-string">&quot;?&quot;</span>) <span class="hljs-keyword">and</span> \\
                           STOPS_AT(PUNCHLINE, <span class="hljs-string">&quot;\\n&quot;</span>)
    <span class="inline-lmql-delim">&#39;&#39;&#39;</span></span>

tell_a_joke() <span class="hljs-comment"># uses chatgpt</span>
tell_a_joke(model=lmql.model(<span class="hljs-string">&quot;openai/text-davinci-003&quot;</span>)) <span class="hljs-comment"># uses text-davinci-003</span>
</span></code></pre></div><p>Here, the <code>tell_a_joke</code> query will use ChatGPT by default, but can still be configured to use a different model by passing it as an argument to the query function on invocation.</p><h3 id="option-b-queries-with-from-clause" tabindex="-1">Option B: Queries with <code>from</code> Clause <a class="header-anchor" href="#option-b-queries-with-from-clause" aria-label="Permalink to &quot;Option B: Queries with \`from\` Clause&quot;">​</a></h3><p>You can specify the model as part of the query itself. For this, you can use <code>from</code> in combination with the indented syntax. This can be particularly useful, if your choice of model is intentional and should be part of your program.</p><div class="language-lmql vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">lmql</span><pre class="hljs"><code><span class="line"><span class="hljs-keyword">argmax</span>
    <span class="hljs-string">&quot;This is a query with a specified &#39;from&#39;-clause: <span class="hljs-placeholder">[RESPONSE]</span>&quot;</span>
<span class="hljs-keyword">from</span>
    <span class="hljs-string">&quot;openai/text-ada-001&quot;</span>
</span></code></pre></div><p>Here, we specify <code>&quot;openai/text-ada-001&quot;</code> directly, but the shown snippet is equivalent to the use of <code>lmql.model(...)</code>, i.e. <code>lmql.model(&quot;openai/text-ada-001&quot;)</code>.</p><p>Note, that the <code>from</code> keyword is only available with the indented standalone syntax as shown here, where the decoder keywords has to be provided explicitly.</p><h2 id="playground" tabindex="-1">Playground <a class="header-anchor" href="#playground" aria-label="Permalink to &quot;Playground&quot;">​</a></h2><p>To specify the model when running in the playground, you can use the model dropdown available in the top right of the program editor, to set and override the <code>model</code> parameter of your query program:</p><figure align="center" style="width:70%;margin:auto;" alt="Screenshot of the model dropdown in the playground"><img src="https://github.com/eth-sri/lmql/assets/17903049/5ba2ffdd-e64d-465c-85be-5d9dc2ab6c14"><figcaption>Model selection dropdown in the LMQL Playground.</figcaption></figure><h2 id="adding-new-model-backends" tabindex="-1">Adding New Model Backends <a class="header-anchor" href="#adding-new-model-backends" aria-label="Permalink to &quot;Adding New Model Backends&quot;">​</a></h2><p>Due to the modular design of LMQL, it is easy to add support for new models and backends. If you would like to propose or add support for a new model API or inference engine, please reach out to us via our <a href="https://discord.com/invite/7eJP4fcyNT" target="_blank" rel="noreferrer">Community Discord</a> or via <a href="mailto:hello@lmql.ai" target="_blank" rel="noreferrer">hello@lmql.ai</a>.</p>`,23),t=[n];function i(r,d,p,c,h,m){return a(),s("div",null,t)}const g=e(o,[["render",i]]);export{f as __pageData,g as default};
