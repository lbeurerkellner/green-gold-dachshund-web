import{_ as e,o,c as a,Q as n}from"./chunks/framework.c2adf1ba.js";const m=JSON.parse('{"title":"Installation Instructions","description":"","frontmatter":{},"headers":[],"relativePath":"docs/installation.md","filePath":"docs/installation.md"}'),t={name:"docs/installation.md"},l=n(`<h1 id="installation-instructions" tabindex="-1">Installation Instructions <a class="header-anchor" href="#installation-instructions" aria-label="Permalink to &quot;Installation Instructions&quot;">â€‹</a></h1><p>To install LMQL locally, run the following command in a Python &gt;=3.10 environment.</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="hljs"><code><span class="line">pip install lmql
</span></code></pre></div><p><strong>Local GPU Support:</strong> If you want to run models on a local GPU, make sure to install LMQL in an environment with a GPU-enabled installation of <a href="https://pytorch.org/get-started/locally/" target="_blank" rel="noreferrer">PyTorch &gt;= 1.11</a>.</p><p>To install LMQL with GPU dependencies via pip, run <code>pip install lmql[hf]</code>.</p><div class="tip custom-block"><p class="custom-block-title">Playground</p><p>For quick experimentation, you can also use the web-based <a href="https://lmql.ai/playground" target="_blank" rel="noreferrer">Playground IDE</a></p></div><h2 id="running-lmql-programs" tabindex="-1">Running LMQL Programs <a class="header-anchor" href="#running-lmql-programs" aria-label="Permalink to &quot;Running LMQL Programs&quot;">â€‹</a></h2><p><strong>Playground</strong></p><p>After installation, you can launch a local instance of the Playground IDE using the following command:</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="hljs"><code><span class="line">lmql playground
</span></code></pre></div><div class="info custom-block"><p class="custom-block-title">INFO</p><p>Using <strong>the LMQL playground requires an installation of Node.js</strong>. If you are in a conda-managed environment you can install node.js via <code>conda install nodejs=14.20 -c conda-forge</code>. Otherwise, please see the official <a href="https://nodejs.org/en/download/" target="_blank" rel="noreferrer">Node.js website</a> for instructions on how to install it on your system.</p></div><p>This launches a browser-based Playground IDE, including a showcase of many exemplary LMQL programs. If the IDE does not launch automatically, go to <code>http://localhost:3000</code>.</p><p><strong>Command-Line Interface</strong></p><p>As an alternative to the playground, the command-line tool <code>lmql run</code> can be used to execute local <code>.lmql</code> files.</p><p><strong>Python Integration</strong></p><p>LMQL can also be used directly from within Python. To use LMQL in Python, you can import the <code>lmql</code> package, run query code via <code>lmql.run</code> or use a decorator <code>@lmql.query</code> for LMQL query functions.</p><p>For more details, please see the <a href="./lib/python.html">Python Integration</a> chapter.</p><h2 id="self-hosted-models" tabindex="-1">Self-Hosted Models <a class="header-anchor" href="#self-hosted-models" aria-label="Permalink to &quot;Self-Hosted Models&quot;">â€‹</a></h2><p>Note that when using local <a href="https://huggingface.co/transformers" target="_blank" rel="noreferrer">ðŸ¤— Transformers</a> models in the Playground IDE or via <code>lmql run</code>, you have to first launch an instance of the LMQL Inference API for the corresponding model via the command <code>lmql serve-model</code>.</p><p>For more details, please see <a href="./models/hf.html">ðŸ¤— Models</a> chapter.</p><h2 id="configuring-openai-api-credentials" tabindex="-1">Configuring OpenAI API Credentials <a class="header-anchor" href="#configuring-openai-api-credentials" aria-label="Permalink to &quot;Configuring OpenAI API Credentials&quot;">â€‹</a></h2><p>If you want to use OpenAI models, you have to configure your API credentials. To do so you can either define the <code>OPENAI_API_KEY</code> environment variable or create a file <code>api.env</code> in the active working directory, with the following contents.</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="hljs"><code><span class="line">openai-org: &lt;org identifier&gt;
openai-secret: &lt;api secret&gt;
</span></code></pre></div><p>For system-wide configuration, you can also create an <code>api.env</code> file at <code>$HOME/.lmql/api.env</code> or at the project root of your LMQL distribution (e.g. <code>src/</code> in a development copy).</p>`,24),s=[l];function r(i,c,d,p,h,u){return o(),a("div",null,s)}const f=e(t,[["render",r]]);export{m as __pageData,f as default};
