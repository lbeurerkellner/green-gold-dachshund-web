import{_ as e,o,c as s,Q as n}from"./chunks/framework.980cae92.js";const g=JSON.parse('{"title":"Releasing LMQL v0.0.6.4 LMTP, Azure, Synchronous API, and more","description":"","frontmatter":{"date":"2023-06-08T00:00:00.000Z","title":"Releasing LMQL v0.0.6.4 LMTP, Azure, Synchronous API, and more"},"headers":[],"relativePath":"blog/posts/release-0.0.6.4.md","filePath":"blog/posts/release-0.0.6.4.md"}'),a={name:"blog/posts/release-0.0.6.4.md"},t=n(`<h1 id="releasing-lmql-0-0-6-4-lmtp-azure-synchronous-api-and-more" tabindex="-1">Releasing LMQL 0.0.6.4: LMTP, Azure, Synchronous API, and more <a class="header-anchor" href="#releasing-lmql-0-0-6-4-lmtp-azure-synchronous-api-and-more" aria-label="Permalink to &quot;Releasing LMQL 0.0.6.4: LMTP, Azure, Synchronous API, and more&quot;">â€‹</a></h1><p><span class="date">June 8, 2023</span></p><p>Among many things, this update contains several bug fixes and improvements. The most notable changes are:</p><ul><li><p><strong>Azure OpenAI support</strong> LMQL now supports OpenAI models that are served via Azure. For more information on how to use Azure models, please see the corresponding chapter in the <a href="/docs/models/azure.html">documentation</a>. Many thanks to <a href="https://github.com/veqtor" target="_blank" rel="noreferrer">@veqtor</a> for contributing this feature.</p></li><li><p><strong>Local Models via the Language Model Transport Protocol</strong> LMQL 0.0.6.4 implements a novel protocol to stream token output from local models, vastly improving performance. In our first benchmarks, we observed a 5-6x speedup for local model inference. For more information on how to use local models, please see the corresponding chapter in the <a href="/docs/models/hf.html">documentation</a>.</p><p>To learn more about the internals of the new streaming protocol, i.e. the language model transport protocol (LMTP), you can find more details in <a href="https://github.com/eth-sri/lmql/blob/main/src/lmql/models/lmtp/README.md" target="_blank" rel="noreferrer">this README file</a>. In the future, we intend to implement more model backends using LMTP, streamlining communication between LMQL and models.</p><div style="text-align:center;"><img src="https://lmql.ai/assets/inference.f47a6f3e.svg" width="80%"><br><i>LMQL&#39;s new streaming protocol (LMTP) allows for faster local model inference.</i></div></li><li><p><strong>Synchronous Python API</strong> Next to an <code>async/await</code> based API, LMQL now also provides a synchronous API. This means you no longer need to use <code>asyncio</code> to use LMQL from Python.</p><p>To use the synchronous API, simply declare <code>@lmql.query</code> function without the <code>async</code> keyword, e.g.</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="hljs"><code><span class="line"><span class="hljs-keyword">import</span> lmql

<span class="hljs-meta">@lmql.query</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">hello</span>(<span class="hljs-params">s: <span class="hljs-built_in">str</span></span>):
    <span class="hljs-inline-lmql"><span style="opacity:0.4;">&#39;&#39;&#39;lmql</span>
    <span class="hljs-keyword">argmax</span> 
        <span class="hljs-string">&quot;Hello <span class="hljs-subst">{s}</span> <span class="hljs-placeholder">[RESPONSE]</span>&quot;</span> 
        <span class="hljs-keyword">return</span> RESPONSE
    <span class="hljs-keyword">from</span> 
        <span class="hljs-string">&quot;chatgpt&quot;</span>
    &#39;&#39;&#39;</span>

<span class="hljs-built_in">print</span>(hello(<span class="hljs-string">&quot;world&quot;</span>)) <span class="hljs-comment"># [&#39;Hello! How can I assist you today?&#39;]</span>
</span></code></pre></div><p>If you instead want to use <code>lmql.run</code> in a synchronous context, you can now use <code>lmql.run_sync</code> instead. To learn more about how LMQL can be used from Python, check out our <a href="/docs/lib/python.html">documentation</a>.</p></li><li><p><strong>Improved Tokenizer Backends</strong> LMQL can now use the excellent <a href="https://github.com/openai/tiktoken" target="_blank" rel="noreferrer"><code>tiktoken</code> tokenizer</a> as tokenization backend (for OpenAI models). Furthermore, all tokenization backends have been ported to operate on a byte-level, which improves support for multibyte characters and emojis. This is especially relevant for non-English languages and special characters.</p></li><li><p><strong>Docker Image</strong> LMQL now provides a Docker image that can be used to run the LMQL playground in a containerized environment. For more information, please see the <a href="/docs/development/docker-setup.html">documentation</a>. Many thanks to <a href="https://github.com/SilacciA" target="_blank" rel="noreferrer">@SilacciA</a> for contributing this feature.</p></li><li><p><strong>Faster Startup Time</strong> We optimized LMQL&#39;s import hierarchy, which results in faster module loading time.</p></li></ul>`,4),r=[t];function l(i,c,p,h,d,m){return o(),s("div",null,r)}const f=e(a,[["render",l]]);export{g as __pageData,f as default};
